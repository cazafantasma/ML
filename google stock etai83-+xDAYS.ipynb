{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(stock_name,shift_window, normalized=0):\n",
    "    from pandas_datareader import data\n",
    "\n",
    "    # Only get the adjusted close.\n",
    "    df = data.DataReader(stock_name,\n",
    "                       start='2017-1-1',\n",
    "                       end='2018-08-20',\n",
    "                       data_source='yahoo')\n",
    "\n",
    "    df['Adj Close future'] = df['Adj Close'].shift(-shift_window)\n",
    "    #df['Difference'] = ( df['Adj Close'].shift(-shift_window) / df['Adj Close'] ) \n",
    "    #df['Difference'] = ( df['Difference'] -1 )\n",
    "    #print(df.head())\n",
    "    df['Volume'] /= 100\n",
    "    return df[:-shift_window]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  High         Low        Open       Close   Volume  \\\n",
      "Date                                                                  \n",
      "2017-01-02  285.000000  281.000000  281.000000  284.000000  12161.0   \n",
      "2017-01-03  289.000000  283.000000  284.000000  285.000000  40930.0   \n",
      "2017-01-04  289.500000  283.049988  284.899994  288.500000  71984.0   \n",
      "2017-01-05  290.200012  288.000000  288.649994  288.000000  32706.0   \n",
      "2017-01-06  288.000000  283.600006  287.950012  284.299988  24804.0   \n",
      "\n",
      "             Adj Close  Adj Close future  \n",
      "Date                                      \n",
      "2017-01-02  282.917358        283.913574  \n",
      "2017-01-03  283.913574        281.921204  \n",
      "2017-01-04  287.400208        285.407837  \n",
      "2017-01-05  286.902130        284.909760  \n",
      "2017-01-06  283.216217        288.894501  \n"
     ]
    }
   ],
   "source": [
    "stock_name = 'TS.BA'\n",
    "df = get_stock_data(stock_name,10,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  High         Low        Open       Close   Volume  \\\n",
      "Date                                                                  \n",
      "2018-07-02  532.000000  510.049988  521.750000  516.099976   685.75   \n",
      "2018-07-03  525.000000  507.000000  521.000000  508.500000   350.18   \n",
      "2018-07-04  518.950012  505.000000  510.000000  505.450012    99.16   \n",
      "2018-07-05  529.500000  505.450012  505.450012  523.950012   907.48   \n",
      "2018-07-06  534.650024  520.000000  524.000000  528.200012   210.74   \n",
      "2018-07-10  550.000000  529.000000  549.950012  529.950012   162.03   \n",
      "2018-07-11  525.000000  505.000000  513.000000  510.000000   288.14   \n",
      "2018-07-12  517.900024  505.000000  515.000000  511.149994   170.46   \n",
      "2018-07-13  519.950012  508.000000  510.000000  512.849976   307.76   \n",
      "2018-07-16  514.000000  505.149994  513.000000  509.100006   297.30   \n",
      "2018-07-17  515.000000  506.000000  509.000000  510.649994   169.34   \n",
      "2018-07-18  524.950012  508.100006  524.950012  514.200012   142.81   \n",
      "2018-07-19  516.000000  509.149994  514.000000  514.950012   180.44   \n",
      "2018-07-20  517.950012  512.000000  512.000000  514.200012   121.71   \n",
      "2018-07-23  514.000000  505.000000  510.000000  509.600006   282.70   \n",
      "2018-07-24  512.150024  499.500000  510.000000  507.299988   511.63   \n",
      "2018-07-25  504.000000  494.000000  500.000000  501.950012   784.38   \n",
      "2018-07-26  504.500000  491.049988  499.950012  497.700012   577.08   \n",
      "2018-07-27  501.000000  481.000000  486.000000  491.100006   459.39   \n",
      "2018-07-30  503.500000  493.049988  493.049988  496.049988   186.05   \n",
      "2018-07-31  507.049988  496.149994  496.149994  504.899994   196.30   \n",
      "2018-08-01  512.950012  500.000000  504.000000  510.149994   167.66   \n",
      "2018-08-02  484.649994  467.000000  484.649994  469.100006  1532.00   \n",
      "2018-08-03  479.000000  467.000000  475.049988  467.149994   266.90   \n",
      "2018-08-06  474.950012  467.000000  467.350006  468.350006   471.69   \n",
      "\n",
      "             Adj Close  Adj Close future  \n",
      "Date                                      \n",
      "2018-07-02  516.099976        510.649994  \n",
      "2018-07-03  508.500000        514.200012  \n",
      "2018-07-04  505.450012        514.950012  \n",
      "2018-07-05  523.950012        514.200012  \n",
      "2018-07-06  528.200012        509.600006  \n",
      "2018-07-10  529.950012        507.299988  \n",
      "2018-07-11  510.000000        501.950012  \n",
      "2018-07-12  511.149994        497.700012  \n",
      "2018-07-13  512.849976        491.100006  \n",
      "2018-07-16  509.100006        496.049988  \n",
      "2018-07-17  510.649994        504.899994  \n",
      "2018-07-18  514.200012        510.149994  \n",
      "2018-07-19  514.950012        469.100006  \n",
      "2018-07-20  514.200012        467.149994  \n",
      "2018-07-23  509.600006        468.350006  \n",
      "2018-07-24  507.299988        472.250000  \n",
      "2018-07-25  501.950012        487.049988  \n",
      "2018-07-26  497.700012        490.899994  \n",
      "2018-07-27  491.100006        502.549988  \n",
      "2018-07-30  496.049988        507.700012  \n",
      "2018-07-31  504.899994        500.450012  \n",
      "2018-08-01  510.149994        488.899994  \n",
      "2018-08-02  469.100006        495.049988  \n",
      "2018-08-03  467.149994        499.049988  \n",
      "2018-08-06  468.350006        502.350006  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([290.20001221, 288.        , 288.6499939 , 288.        ,\n",
       "       327.06      , 286.90213013, 284.90975952])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.tail(25))\n",
    "df_val = df.values\n",
    "df_val[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock, seq_len):\n",
    "\n",
    "    data = stock\n",
    "    amount_of_features = len(data[::-1][0]) - 1\n",
    "    \n",
    "    \n",
    "    sequence_length = seq_len \n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length + 1])\n",
    "\n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0])\n",
    "    train = result[:int(row), :]\n",
    "    x_train = train[:, :-1]\n",
    "    y_train = train[:, -1][:,-1]\n",
    "    x_test = result[int(row):, :-1]\n",
    "    print (\"x_test:\",x_test)\n",
    "    y_test = result[int(row):, -1][:,-1]\n",
    "    print (\"y_test:\",y_test)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_v2(stock, seq_len):\n",
    "\n",
    "    data_x = stock\n",
    "    data_y = stock\n",
    "    #print(\"DATA Y 0 :\",data_y)\n",
    "    print (\"Amount of features TOT :\",len(data_x[0]) )\n",
    "    data_x = np.delete(data_x,np.s_[len(stock[0])-1],axis=1)\n",
    "    data_y = np.delete(data_y,np.s_[0:len(stock[0])-1],axis=1)\n",
    "    #print(\"DATA Y :\",data_y)\n",
    "    amount_of_features = len(data_x[0]) \n",
    "    \n",
    "    print (\"Amount of features found:\",amount_of_features)\n",
    "    \n",
    "    sequence_length = seq_len \n",
    "    result_x = []\n",
    "    result_y = []\n",
    "    for index in range(len(data_x) - sequence_length ):\n",
    "        result_x.append(data_x[index: index + sequence_length + 1])\n",
    "        result_y.append(data_y[index: index + sequence_length + 1])\n",
    "\n",
    "    result_x = np.array(result_x)\n",
    "    result_y = np.array(result_y)\n",
    "    \n",
    "    row = round(0.92 * result_x.shape[0])\n",
    "    train_x = result_x[:int(row), :]\n",
    "    train_y = result_y[:int(row), :]\n",
    "    \n",
    "    x_train = train_x[:, :-1]\n",
    "    x_test = result_x[int(row):, :-1]\n",
    "   \n",
    "    y_train = train_y[:, -1]\n",
    "    y_test = result_y[int(row):, -1]\n",
    "    \n",
    "    #print (\"x_test before:\",x_test)\n",
    "    #print (\"y_test before:\",y_test)\n",
    "    \n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "    print (\"x_test:\",x_test)\n",
    "    print (\"y_test:\",y_test)\n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features TOT : 7\n",
      "Amount of features found: 6\n",
      "x_test: [[[ 476.          462.3999939   462.3999939   467.45001221  179.15\n",
      "    467.45001221]\n",
      "  [ 488.8999939   470.3999939   479.45001221  484.20001221  617.12\n",
      "    484.20001221]\n",
      "  [ 484.20001221  470.          484.20001221  472.          292.58\n",
      "    472.        ]\n",
      "  ...\n",
      "  [ 488.          480.          481.          487.1499939   504.05\n",
      "    487.1499939 ]\n",
      "  [ 500.          476.04998779  499.95001221  480.29998779 1010.13\n",
      "    480.29998779]\n",
      "  [ 502.5         489.04998779  489.04998779  493.8999939   523.05\n",
      "    493.8999939 ]]\n",
      "\n",
      " [[ 488.8999939   470.3999939   479.45001221  484.20001221  617.12\n",
      "    484.20001221]\n",
      "  [ 484.20001221  470.          484.20001221  472.          292.58\n",
      "    472.        ]\n",
      "  [ 483.          464.          472.          479.          302.41\n",
      "    479.        ]\n",
      "  ...\n",
      "  [ 500.          476.04998779  499.95001221  480.29998779 1010.13\n",
      "    480.29998779]\n",
      "  [ 502.5         489.04998779  489.04998779  493.8999939   523.05\n",
      "    493.8999939 ]\n",
      "  [ 494.5         471.          494.5         476.04998779  215.12\n",
      "    476.04998779]]\n",
      "\n",
      " [[ 484.20001221  470.          484.20001221  472.          292.58\n",
      "    472.        ]\n",
      "  [ 483.          464.          472.          479.          302.41\n",
      "    479.        ]\n",
      "  [ 527.95001221  481.          481.          516.45001221  878.\n",
      "    516.45001221]\n",
      "  ...\n",
      "  [ 502.5         489.04998779  489.04998779  493.8999939   523.05\n",
      "    493.8999939 ]\n",
      "  [ 494.5         471.          494.5         476.04998779  215.12\n",
      "    476.04998779]\n",
      "  [ 488.95001221  470.          476.          481.          291.71\n",
      "    481.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 516.          509.1499939   514.          514.95001221  180.44\n",
      "    514.95001221]\n",
      "  [ 517.95001221  512.          512.          514.20001221  121.71\n",
      "    514.20001221]\n",
      "  [ 514.          505.          510.          509.6000061   282.7\n",
      "    509.6000061 ]\n",
      "  ...\n",
      "  [ 503.5         493.04998779  493.04998779  496.04998779  186.05\n",
      "    496.04998779]\n",
      "  [ 507.04998779  496.1499939   496.1499939   504.8999939   196.3\n",
      "    504.8999939 ]\n",
      "  [ 512.95001221  500.          504.          510.1499939   167.66\n",
      "    510.1499939 ]]\n",
      "\n",
      " [[ 517.95001221  512.          512.          514.20001221  121.71\n",
      "    514.20001221]\n",
      "  [ 514.          505.          510.          509.6000061   282.7\n",
      "    509.6000061 ]\n",
      "  [ 512.15002441  499.5         510.          507.29998779  511.63\n",
      "    507.29998779]\n",
      "  ...\n",
      "  [ 507.04998779  496.1499939   496.1499939   504.8999939   196.3\n",
      "    504.8999939 ]\n",
      "  [ 512.95001221  500.          504.          510.1499939   167.66\n",
      "    510.1499939 ]\n",
      "  [ 484.6499939   467.          484.6499939   469.1000061  1532.\n",
      "    469.1000061 ]]\n",
      "\n",
      " [[ 514.          505.          510.          509.6000061   282.7\n",
      "    509.6000061 ]\n",
      "  [ 512.15002441  499.5         510.          507.29998779  511.63\n",
      "    507.29998779]\n",
      "  [ 504.          494.          500.          501.95001221  784.38\n",
      "    501.95001221]\n",
      "  ...\n",
      "  [ 512.95001221  500.          504.          510.1499939   167.66\n",
      "    510.1499939 ]\n",
      "  [ 484.6499939   467.          484.6499939   469.1000061  1532.\n",
      "    469.1000061 ]\n",
      "  [ 479.          467.          475.04998779  467.1499939   266.9\n",
      "    467.1499939 ]]]\n",
      "y_test: [[529.95001221]\n",
      " [510.        ]\n",
      " [511.1499939 ]\n",
      " [512.84997559]\n",
      " [509.1000061 ]\n",
      " [510.6499939 ]\n",
      " [514.20001221]\n",
      " [514.95001221]\n",
      " [514.20001221]\n",
      " [509.6000061 ]\n",
      " [507.29998779]\n",
      " [501.95001221]\n",
      " [497.70001221]\n",
      " [491.1000061 ]\n",
      " [496.04998779]\n",
      " [504.8999939 ]\n",
      " [510.1499939 ]\n",
      " [469.1000061 ]\n",
      " [467.1499939 ]\n",
      " [468.3500061 ]\n",
      " [472.25      ]\n",
      " [487.04998779]\n",
      " [490.8999939 ]\n",
      " [502.54998779]\n",
      " [507.70001221]\n",
      " [500.45001221]\n",
      " [488.8999939 ]\n",
      " [495.04998779]\n",
      " [499.04998779]\n",
      " [502.3500061 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[[ 285.        ,  281.        ,  281.        ,  284.        ,\n",
       "           121.61      ,  282.9173584 ],\n",
       "         [ 289.        ,  283.        ,  284.        ,  285.        ,\n",
       "           409.3       ,  283.91357422],\n",
       "         [ 289.5       ,  283.04998779,  284.8999939 ,  288.5       ,\n",
       "           719.84      ,  287.40020752],\n",
       "         ...,\n",
       "         [ 286.        ,  277.        ,  281.        ,  283.8999939 ,\n",
       "           328.42      ,  282.81774902],\n",
       "         [ 286.        ,  280.5       ,  283.8999939 ,  283.75      ,\n",
       "           188.45      ,  282.66833496],\n",
       "         [ 285.        ,  283.        ,  284.        ,  284.        ,\n",
       "           176.06      ,  282.9173584 ]],\n",
       " \n",
       "        [[ 289.        ,  283.        ,  284.        ,  285.        ,\n",
       "           409.3       ,  283.91357422],\n",
       "         [ 289.5       ,  283.04998779,  284.8999939 ,  288.5       ,\n",
       "           719.84      ,  287.40020752],\n",
       "         [ 290.20001221,  288.        ,  288.6499939 ,  288.        ,\n",
       "           327.06      ,  286.90213013],\n",
       "         ...,\n",
       "         [ 286.        ,  280.5       ,  283.8999939 ,  283.75      ,\n",
       "           188.45      ,  282.66833496],\n",
       "         [ 285.        ,  283.        ,  284.        ,  284.        ,\n",
       "           176.06      ,  282.9173584 ],\n",
       "         [ 285.5       ,  283.25      ,  284.        ,  285.        ,\n",
       "           115.88      ,  283.91357422]],\n",
       " \n",
       "        [[ 289.5       ,  283.04998779,  284.8999939 ,  288.5       ,\n",
       "           719.84      ,  287.40020752],\n",
       "         [ 290.20001221,  288.        ,  288.6499939 ,  288.        ,\n",
       "           327.06      ,  286.90213013],\n",
       "         [ 288.        ,  283.6000061 ,  287.95001221,  284.29998779,\n",
       "           248.04      ,  283.21621704],\n",
       "         ...,\n",
       "         [ 285.        ,  283.        ,  284.        ,  284.        ,\n",
       "           176.06      ,  282.9173584 ],\n",
       "         [ 285.5       ,  283.25      ,  284.        ,  285.        ,\n",
       "           115.88      ,  283.91357422],\n",
       "         [ 284.6000061 ,  280.        ,  284.5       ,  283.        ,\n",
       "           157.36      ,  281.92120361]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 449.5       ,  442.        ,  442.6000061 ,  447.95001221,\n",
       "           693.5       ,  447.95001221],\n",
       "         [ 454.        ,  443.        ,  450.        ,  453.20001221,\n",
       "           302.45      ,  453.20001221],\n",
       "         [ 462.        ,  453.25      ,  455.        ,  459.        ,\n",
       "           270.84      ,  459.        ],\n",
       "         ...,\n",
       "         [ 527.95001221,  481.        ,  481.        ,  516.45001221,\n",
       "           878.        ,  516.45001221],\n",
       "         [ 507.8500061 ,  485.        ,  507.8500061 ,  503.29998779,\n",
       "           387.53      ,  503.29998779],\n",
       "         [ 497.95001221,  476.04998779,  497.95001221,  481.1000061 ,\n",
       "           591.36      ,  481.1000061 ]],\n",
       " \n",
       "        [[ 454.        ,  443.        ,  450.        ,  453.20001221,\n",
       "           302.45      ,  453.20001221],\n",
       "         [ 462.        ,  453.25      ,  455.        ,  459.        ,\n",
       "           270.84      ,  459.        ],\n",
       "         [ 476.        ,  462.3999939 ,  462.3999939 ,  467.45001221,\n",
       "           179.15      ,  467.45001221],\n",
       "         ...,\n",
       "         [ 507.8500061 ,  485.        ,  507.8500061 ,  503.29998779,\n",
       "           387.53      ,  503.29998779],\n",
       "         [ 497.95001221,  476.04998779,  497.95001221,  481.1000061 ,\n",
       "           591.36      ,  481.1000061 ],\n",
       "         [ 488.        ,  480.        ,  481.        ,  487.1499939 ,\n",
       "           504.05      ,  487.1499939 ]],\n",
       " \n",
       "        [[ 462.        ,  453.25      ,  455.        ,  459.        ,\n",
       "           270.84      ,  459.        ],\n",
       "         [ 476.        ,  462.3999939 ,  462.3999939 ,  467.45001221,\n",
       "           179.15      ,  467.45001221],\n",
       "         [ 488.8999939 ,  470.3999939 ,  479.45001221,  484.20001221,\n",
       "           617.12      ,  484.20001221],\n",
       "         ...,\n",
       "         [ 497.95001221,  476.04998779,  497.95001221,  481.1000061 ,\n",
       "           591.36      ,  481.1000061 ],\n",
       "         [ 488.        ,  480.        ,  481.        ,  487.1499939 ,\n",
       "           504.05      ,  487.1499939 ],\n",
       "         [ 500.        ,  476.04998779,  499.95001221,  480.29998779,\n",
       "          1010.13      ,  480.29998779]]]), array([[275.69500732],\n",
       "        [275.74478149],\n",
       "        [276.94021606],\n",
       "        [274.94781494],\n",
       "        [273.95166016],\n",
       "        [269.96694946],\n",
       "        [269.96694946],\n",
       "        [264.43807983],\n",
       "        [264.73693848],\n",
       "        [265.48403931],\n",
       "        [266.1315918 ],\n",
       "        [264.98599243],\n",
       "        [263.98980713],\n",
       "        [261.0012207 ],\n",
       "        [262.99362183],\n",
       "        [260.00506592],\n",
       "        [265.98217773],\n",
       "        [265.98217773],\n",
       "        [256.71765137],\n",
       "        [254.4263916 ],\n",
       "        [257.51461792],\n",
       "        [252.03553772],\n",
       "        [253.03172302],\n",
       "        [249.94355774],\n",
       "        [251.03935242],\n",
       "        [247.00479126],\n",
       "        [238.08891296],\n",
       "        [243.56793213],\n",
       "        [246.5565033 ],\n",
       "        [241.37632751],\n",
       "        [251.03935242],\n",
       "        [252.53363037],\n",
       "        [253.33059692],\n",
       "        [252.03553772],\n",
       "        [246.8553772 ],\n",
       "        [250.04316711],\n",
       "        [247.95117188],\n",
       "        [250.04316711],\n",
       "        [261.0012207 ],\n",
       "        [258.11230469],\n",
       "        [264.23886108],\n",
       "        [263.98980713],\n",
       "        [263.98980713],\n",
       "        [263.49169922],\n",
       "        [255.02410889],\n",
       "        [261.0012207 ],\n",
       "        [258.80963135],\n",
       "        [260.40353394],\n",
       "        [265.48403931],\n",
       "        [253.03172302],\n",
       "        [247.95117188],\n",
       "        [246.65614319],\n",
       "        [244.56413269],\n",
       "        [249.54507446],\n",
       "        [246.5565033 ],\n",
       "        [248.94735718],\n",
       "        [252.53363037],\n",
       "        [255.5222168 ],\n",
       "        [245.36109924],\n",
       "        [241.57556152],\n",
       "        [239.08511353],\n",
       "        [238.08891296],\n",
       "        [231.61369324],\n",
       "        [237.93948364],\n",
       "        [238.48738098],\n",
       "        [237.59082031],\n",
       "        [242.07365417],\n",
       "        [239.38395691],\n",
       "        [238.38778687],\n",
       "        [245.51049805],\n",
       "        [248.05079651],\n",
       "        [245.16184998],\n",
       "        [254.77505493],\n",
       "        [261.38699341],\n",
       "        [257.67318726],\n",
       "        [256.32540894],\n",
       "        [254.67814636],\n",
       "        [248.78788757],\n",
       "        [244.84443665],\n",
       "        [246.09236145],\n",
       "        [244.59483337],\n",
       "        [247.58988953],\n",
       "        [244.9941864 ],\n",
       "        [244.59483337],\n",
       "        [244.09565735],\n",
       "        [238.10559082],\n",
       "        [241.59979248],\n",
       "        [247.58988953],\n",
       "        [244.84443665],\n",
       "        [248.58822632],\n",
       "        [243.59649658],\n",
       "        [243.0973053 ],\n",
       "        [248.58822632],\n",
       "        [249.58657837],\n",
       "        [244.09565735],\n",
       "        [242.09896851],\n",
       "        [242.09896851],\n",
       "        [242.59814453],\n",
       "        [244.09565735],\n",
       "        [250.83450317],\n",
       "        [256.47512817],\n",
       "        [258.07254028],\n",
       "        [262.36535645],\n",
       "        [261.56671143],\n",
       "        [265.56011963],\n",
       "        [265.56011963],\n",
       "        [260.9677124 ],\n",
       "        [262.51516724],\n",
       "        [266.55847168],\n",
       "        [269.80307007],\n",
       "        [274.54522705],\n",
       "        [274.7947998 ],\n",
       "        [275.54354858],\n",
       "        [277.34060669],\n",
       "        [282.0328064 ],\n",
       "        [277.04107666],\n",
       "        [274.54522705],\n",
       "        [274.54522705],\n",
       "        [278.03945923],\n",
       "        [277.54025269],\n",
       "        [279.03778076],\n",
       "        [282.0328064 ],\n",
       "        [279.03778076],\n",
       "        [270.90127563],\n",
       "        [269.75317383],\n",
       "        [253.57995605],\n",
       "        [253.57995605],\n",
       "        [249.93600464],\n",
       "        [248.73797607],\n",
       "        [248.98754883],\n",
       "        [247.58988953],\n",
       "        [247.58988953],\n",
       "        [237.20709229],\n",
       "        [233.61303711],\n",
       "        [234.11219788],\n",
       "        [235.60972595],\n",
       "        [236.608078  ],\n",
       "        [233.81269836],\n",
       "        [234.51153564],\n",
       "        [233.11384583],\n",
       "        [234.11219788],\n",
       "        [232.5148468 ],\n",
       "        [233.91253662],\n",
       "        [233.96244812],\n",
       "        [230.91751099],\n",
       "        [230.46824646],\n",
       "        [231.76609802],\n",
       "        [231.76609802],\n",
       "        [231.76609802],\n",
       "        [231.76609802],\n",
       "        [231.76609802],\n",
       "        [231.76609802],\n",
       "        [237.70626831],\n",
       "        [237.70626831],\n",
       "        [237.70626831],\n",
       "        [237.70626831],\n",
       "        [237.70626831],\n",
       "        [248.88772583],\n",
       "        [246.59153748],\n",
       "        [247.09069824],\n",
       "        [248.08905029],\n",
       "        [249.38690186],\n",
       "        [249.3369751 ],\n",
       "        [247.58988953],\n",
       "        [248.28872681],\n",
       "        [246.59153748],\n",
       "        [245.74293518],\n",
       "        [246.29202271],\n",
       "        [243.14724731],\n",
       "        [244.44508362],\n",
       "        [241.40013123],\n",
       "        [239.25369263],\n",
       "        [237.10723877],\n",
       "        [236.90759277],\n",
       "        [239.05400085],\n",
       "        [234.6113739 ],\n",
       "        [230.66792297],\n",
       "        [231.16708374],\n",
       "        [230.11882019],\n",
       "        [228.37171936],\n",
       "        [232.76441956],\n",
       "        [227.87254333],\n",
       "        [231.96577454],\n",
       "        [234.96080017],\n",
       "        [240.65138245],\n",
       "        [242.24871826],\n",
       "        [241.35021973],\n",
       "        [268.80471802],\n",
       "        [267.4569397 ],\n",
       "        [267.10751343],\n",
       "        [262.61499023],\n",
       "        [261.46688843],\n",
       "        [258.27218628],\n",
       "        [255.6265564 ],\n",
       "        [250.63484192],\n",
       "        [247.98921204],\n",
       "        [246.3419342 ],\n",
       "        [249.03747559],\n",
       "        [254.2615509 ],\n",
       "        [256.20935059],\n",
       "        [255.4102478 ],\n",
       "        [253.91194153],\n",
       "        [249.66676331],\n",
       "        [249.76664734],\n",
       "        [251.66448975],\n",
       "        [254.61114502],\n",
       "        [260.6043396 ],\n",
       "        [255.46018982],\n",
       "        [250.21612549],\n",
       "        [253.06291199],\n",
       "        [263.35125732],\n",
       "        [267.6463623 ],\n",
       "        [266.99710083],\n",
       "        [263.85064697],\n",
       "        [263.00161743],\n",
       "        [269.59414673],\n",
       "        [270.69293213],\n",
       "        [275.43753052],\n",
       "        [283.0788269 ],\n",
       "        [284.47723389],\n",
       "        [291.61917114],\n",
       "        [288.22299194],\n",
       "        [297.21282959],\n",
       "        [294.66571045],\n",
       "        [296.2638855 ],\n",
       "        [300.80874634],\n",
       "        [307.65097046],\n",
       "        [313.59423828],\n",
       "        [314.59310913],\n",
       "        [312.34564209],\n",
       "        [306.25256348],\n",
       "        [319.63735962],\n",
       "        [323.98245239],\n",
       "        [331.12435913],\n",
       "        [319.33773804],\n",
       "        [327.22879028],\n",
       "        [328.37747192],\n",
       "        [330.77471924],\n",
       "        [341.46261597],\n",
       "        [349.25378418],\n",
       "        [352.55004883],\n",
       "        [352.10055542],\n",
       "        [356.19592285],\n",
       "        [348.95413208],\n",
       "        [342.81109619],\n",
       "        [339.61471558],\n",
       "        [345.40811157],\n",
       "        [327.87802124],\n",
       "        [321.73501587],\n",
       "        [324.88140869],\n",
       "        [319.63735962],\n",
       "        [316.79058838],\n",
       "        [311.64642334],\n",
       "        [326.08007812],\n",
       "        [320.33660889],\n",
       "        [333.9211731 ],\n",
       "        [345.35818481],\n",
       "        [334.62039185],\n",
       "        [332.2230835 ],\n",
       "        [347.95526123],\n",
       "        [349.95300293],\n",
       "        [366.13464355],\n",
       "        [358.79299927],\n",
       "        [350.4524231 ],\n",
       "        [358.84292603],\n",
       "        [358.64309692],\n",
       "        [357.54440308],\n",
       "        [359.84176636],\n",
       "        [357.99383545],\n",
       "        [365.38549805],\n",
       "        [376.42297363],\n",
       "        [371.17889404],\n",
       "        [360.89056396],\n",
       "        [360.04153442],\n",
       "        [354.74752808],\n",
       "        [357.59432983],\n",
       "        [356.89511108],\n",
       "        [361.88943481],\n",
       "        [376.47290039],\n",
       "        [340.11413574],\n",
       "        [335.21969604],\n",
       "        [347.8553772 ],\n",
       "        [341.86218262],\n",
       "        [337.51708984],\n",
       "        [349.70327759],\n",
       "        [350.00292969],\n",
       "        [359.7918396 ],\n",
       "        [351.65106201],\n",
       "        [355.59658813],\n",
       "        [367.08355713],\n",
       "        [373.27651978],\n",
       "        [376.0234375 ],\n",
       "        [368.33215332],\n",
       "        [369.68060303],\n",
       "        [371.02908325],\n",
       "        [378.52056885],\n",
       "        [379.51947021],\n",
       "        [380.01889038],\n",
       "        [381.26748657],\n",
       "        [380.5682373 ],\n",
       "        [377.2220459 ],\n",
       "        [386.2618103 ],\n",
       "        [384.01437378],\n",
       "        [408.63641357],\n",
       "        [439.60131836],\n",
       "        [422.42077637],\n",
       "        [427.21536255],\n",
       "        [434.50708008],\n",
       "        [441.14953613],\n",
       "        [445.39468384],\n",
       "        [469.56729126],\n",
       "        [484.70013428],\n",
       "        [467.96908569],\n",
       "        [479.85559082],\n",
       "        [491.39254761],\n",
       "        [495.86999512],\n",
       "        [488.04998779],\n",
       "        [475.54998779],\n",
       "        [476.5       ],\n",
       "        [471.        ],\n",
       "        [449.3999939 ],\n",
       "        [453.        ],\n",
       "        [465.79998779],\n",
       "        [451.1000061 ],\n",
       "        [455.        ],\n",
       "        [442.6000061 ],\n",
       "        [447.95001221],\n",
       "        [453.20001221],\n",
       "        [459.        ],\n",
       "        [467.45001221],\n",
       "        [484.20001221],\n",
       "        [472.        ],\n",
       "        [479.        ],\n",
       "        [516.45001221],\n",
       "        [503.29998779],\n",
       "        [481.1000061 ],\n",
       "        [487.1499939 ],\n",
       "        [480.29998779],\n",
       "        [493.8999939 ],\n",
       "        [476.04998779],\n",
       "        [481.        ],\n",
       "        [500.5       ],\n",
       "        [516.34997559],\n",
       "        [521.95001221],\n",
       "        [516.09997559],\n",
       "        [508.5       ],\n",
       "        [505.45001221],\n",
       "        [523.95001221],\n",
       "        [528.20001221]]), array([[[ 476.        ,  462.3999939 ,  462.3999939 ,  467.45001221,\n",
       "           179.15      ,  467.45001221],\n",
       "         [ 488.8999939 ,  470.3999939 ,  479.45001221,  484.20001221,\n",
       "           617.12      ,  484.20001221],\n",
       "         [ 484.20001221,  470.        ,  484.20001221,  472.        ,\n",
       "           292.58      ,  472.        ],\n",
       "         ...,\n",
       "         [ 488.        ,  480.        ,  481.        ,  487.1499939 ,\n",
       "           504.05      ,  487.1499939 ],\n",
       "         [ 500.        ,  476.04998779,  499.95001221,  480.29998779,\n",
       "          1010.13      ,  480.29998779],\n",
       "         [ 502.5       ,  489.04998779,  489.04998779,  493.8999939 ,\n",
       "           523.05      ,  493.8999939 ]],\n",
       " \n",
       "        [[ 488.8999939 ,  470.3999939 ,  479.45001221,  484.20001221,\n",
       "           617.12      ,  484.20001221],\n",
       "         [ 484.20001221,  470.        ,  484.20001221,  472.        ,\n",
       "           292.58      ,  472.        ],\n",
       "         [ 483.        ,  464.        ,  472.        ,  479.        ,\n",
       "           302.41      ,  479.        ],\n",
       "         ...,\n",
       "         [ 500.        ,  476.04998779,  499.95001221,  480.29998779,\n",
       "          1010.13      ,  480.29998779],\n",
       "         [ 502.5       ,  489.04998779,  489.04998779,  493.8999939 ,\n",
       "           523.05      ,  493.8999939 ],\n",
       "         [ 494.5       ,  471.        ,  494.5       ,  476.04998779,\n",
       "           215.12      ,  476.04998779]],\n",
       " \n",
       "        [[ 484.20001221,  470.        ,  484.20001221,  472.        ,\n",
       "           292.58      ,  472.        ],\n",
       "         [ 483.        ,  464.        ,  472.        ,  479.        ,\n",
       "           302.41      ,  479.        ],\n",
       "         [ 527.95001221,  481.        ,  481.        ,  516.45001221,\n",
       "           878.        ,  516.45001221],\n",
       "         ...,\n",
       "         [ 502.5       ,  489.04998779,  489.04998779,  493.8999939 ,\n",
       "           523.05      ,  493.8999939 ],\n",
       "         [ 494.5       ,  471.        ,  494.5       ,  476.04998779,\n",
       "           215.12      ,  476.04998779],\n",
       "         [ 488.95001221,  470.        ,  476.        ,  481.        ,\n",
       "           291.71      ,  481.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 516.        ,  509.1499939 ,  514.        ,  514.95001221,\n",
       "           180.44      ,  514.95001221],\n",
       "         [ 517.95001221,  512.        ,  512.        ,  514.20001221,\n",
       "           121.71      ,  514.20001221],\n",
       "         [ 514.        ,  505.        ,  510.        ,  509.6000061 ,\n",
       "           282.7       ,  509.6000061 ],\n",
       "         ...,\n",
       "         [ 503.5       ,  493.04998779,  493.04998779,  496.04998779,\n",
       "           186.05      ,  496.04998779],\n",
       "         [ 507.04998779,  496.1499939 ,  496.1499939 ,  504.8999939 ,\n",
       "           196.3       ,  504.8999939 ],\n",
       "         [ 512.95001221,  500.        ,  504.        ,  510.1499939 ,\n",
       "           167.66      ,  510.1499939 ]],\n",
       " \n",
       "        [[ 517.95001221,  512.        ,  512.        ,  514.20001221,\n",
       "           121.71      ,  514.20001221],\n",
       "         [ 514.        ,  505.        ,  510.        ,  509.6000061 ,\n",
       "           282.7       ,  509.6000061 ],\n",
       "         [ 512.15002441,  499.5       ,  510.        ,  507.29998779,\n",
       "           511.63      ,  507.29998779],\n",
       "         ...,\n",
       "         [ 507.04998779,  496.1499939 ,  496.1499939 ,  504.8999939 ,\n",
       "           196.3       ,  504.8999939 ],\n",
       "         [ 512.95001221,  500.        ,  504.        ,  510.1499939 ,\n",
       "           167.66      ,  510.1499939 ],\n",
       "         [ 484.6499939 ,  467.        ,  484.6499939 ,  469.1000061 ,\n",
       "          1532.        ,  469.1000061 ]],\n",
       " \n",
       "        [[ 514.        ,  505.        ,  510.        ,  509.6000061 ,\n",
       "           282.7       ,  509.6000061 ],\n",
       "         [ 512.15002441,  499.5       ,  510.        ,  507.29998779,\n",
       "           511.63      ,  507.29998779],\n",
       "         [ 504.        ,  494.        ,  500.        ,  501.95001221,\n",
       "           784.38      ,  501.95001221],\n",
       "         ...,\n",
       "         [ 512.95001221,  500.        ,  504.        ,  510.1499939 ,\n",
       "           167.66      ,  510.1499939 ],\n",
       "         [ 484.6499939 ,  467.        ,  484.6499939 ,  469.1000061 ,\n",
       "          1532.        ,  469.1000061 ],\n",
       "         [ 479.        ,  467.        ,  475.04998779,  467.1499939 ,\n",
       "           266.9       ,  467.1499939 ]]]), array([[529.95001221],\n",
       "        [510.        ],\n",
       "        [511.1499939 ],\n",
       "        [512.84997559],\n",
       "        [509.1000061 ],\n",
       "        [510.6499939 ],\n",
       "        [514.20001221],\n",
       "        [514.95001221],\n",
       "        [514.20001221],\n",
       "        [509.6000061 ],\n",
       "        [507.29998779],\n",
       "        [501.95001221],\n",
       "        [497.70001221],\n",
       "        [491.1000061 ],\n",
       "        [496.04998779],\n",
       "        [504.8999939 ],\n",
       "        [510.1499939 ],\n",
       "        [469.1000061 ],\n",
       "        [467.1499939 ],\n",
       "        [468.3500061 ],\n",
       "        [472.25      ],\n",
       "        [487.04998779],\n",
       "        [490.8999939 ],\n",
       "        [502.54998779],\n",
       "        [507.70001221],\n",
       "        [500.45001221],\n",
       "        [488.8999939 ],\n",
       "        [495.04998779],\n",
       "        [499.04998779],\n",
       "        [502.3500061 ]])]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data_v2(df_val, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        input_dim=layers[0],\n",
    "        output_dim=layers[1],\n",
    "        return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "        layers[2],\n",
    "        return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "        output_dim=layers[2]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\",metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "def build_model2(layers):\n",
    "        d = 0.2\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "        model.add(Dropout(d))\n",
    "        model.add(LSTM(64, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "        model.add(Dropout(d))\n",
    "        model.add(Dense(16,kernel_initializer='uniform',activation='relu'))        \n",
    "        model.add(Dense(1,kernel_initializer='uniform',activation='relu'))\n",
    "        model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features TOT : 7\n",
      "Amount of features found: 6\n",
      "x_test: [[[0.265      0.26       0.2645     0.261      0.17853    0.26000507]\n",
      "  [0.26525    0.259      0.2615     0.264      0.28015    0.26299362]\n",
      "  [0.26695001 0.26114999 0.266      0.262      0.15064    0.26100122]\n",
      "  [0.266      0.26310001 0.26425    0.265      0.17396    0.26398981]\n",
      "  [0.26720001 0.264      0.26720001 0.266      0.14868    0.26498599]]\n",
      "\n",
      " [[0.26525    0.259      0.2615     0.264      0.28015    0.26299362]\n",
      "  [0.26695001 0.26114999 0.266      0.262      0.15064    0.26100122]\n",
      "  [0.266      0.26310001 0.26425    0.265      0.17396    0.26398981]\n",
      "  [0.26720001 0.264      0.26720001 0.266      0.14868    0.26498599]\n",
      "  [0.26889999 0.265      0.2665     0.26714999 0.12141    0.26613159]]\n",
      "\n",
      " [[0.26695001 0.26114999 0.266      0.262      0.15064    0.26100122]\n",
      "  [0.266      0.26310001 0.26425    0.265      0.17396    0.26398981]\n",
      "  [0.26720001 0.264      0.26720001 0.266      0.14868    0.26498599]\n",
      "  [0.26889999 0.265      0.2665     0.26714999 0.12141    0.26613159]\n",
      "  [0.26904999 0.26575    0.26579999 0.2665     0.55597    0.26548404]]\n",
      "\n",
      " [[0.266      0.26310001 0.26425    0.265      0.17396    0.26398981]\n",
      "  [0.26720001 0.264      0.26720001 0.266      0.14868    0.26498599]\n",
      "  [0.26889999 0.265      0.2665     0.26714999 0.12141    0.26613159]\n",
      "  [0.26904999 0.26575    0.26579999 0.2665     0.55597    0.26548404]\n",
      "  [0.26860001 0.265      0.26604999 0.26575    0.33121    0.26473694]]\n",
      "\n",
      " [[0.26720001 0.264      0.26720001 0.266      0.14868    0.26498599]\n",
      "  [0.26889999 0.265      0.2665     0.26714999 0.12141    0.26613159]\n",
      "  [0.26904999 0.26575    0.26579999 0.2665     0.55597    0.26548404]\n",
      "  [0.26860001 0.265      0.26604999 0.26575    0.33121    0.26473694]\n",
      "  [0.268      0.26054999 0.268      0.26545001 0.41839    0.26443808]]\n",
      "\n",
      " [[0.26889999 0.265      0.2665     0.26714999 0.12141    0.26613159]\n",
      "  [0.26904999 0.26575    0.26579999 0.2665     0.55597    0.26548404]\n",
      "  [0.26860001 0.265      0.26604999 0.26575    0.33121    0.26473694]\n",
      "  [0.268      0.26054999 0.268      0.26545001 0.41839    0.26443808]\n",
      "  [0.272      0.268      0.2715     0.271      0.29736    0.26996695]]\n",
      "\n",
      " [[0.26904999 0.26575    0.26579999 0.2665     0.55597    0.26548404]\n",
      "  [0.26860001 0.265      0.26604999 0.26575    0.33121    0.26473694]\n",
      "  [0.268      0.26054999 0.268      0.26545001 0.41839    0.26443808]\n",
      "  [0.272      0.268      0.2715     0.271      0.29736    0.26996695]\n",
      "  [0.274      0.27       0.274      0.271      0.43966    0.26996695]]\n",
      "\n",
      " [[0.26860001 0.265      0.26604999 0.26575    0.33121    0.26473694]\n",
      "  [0.268      0.26054999 0.268      0.26545001 0.41839    0.26443808]\n",
      "  [0.272      0.268      0.2715     0.271      0.29736    0.26996695]\n",
      "  [0.274      0.27       0.274      0.271      0.43966    0.26996695]\n",
      "  [0.27629999 0.27139999 0.276      0.275      0.42158    0.27395166]]\n",
      "\n",
      " [[0.268      0.26054999 0.268      0.26545001 0.41839    0.26443808]\n",
      "  [0.272      0.268      0.2715     0.271      0.29736    0.26996695]\n",
      "  [0.274      0.27       0.274      0.271      0.43966    0.26996695]\n",
      "  [0.27629999 0.27139999 0.276      0.275      0.42158    0.27395166]\n",
      "  [0.28       0.273      0.27704999 0.276      0.45931    0.27494781]]\n",
      "\n",
      " [[0.272      0.268      0.2715     0.271      0.29736    0.26996695]\n",
      "  [0.274      0.27       0.274      0.271      0.43966    0.26996695]\n",
      "  [0.27629999 0.27139999 0.276      0.275      0.42158    0.27395166]\n",
      "  [0.28       0.273      0.27704999 0.276      0.45931    0.27494781]\n",
      "  [0.28       0.273      0.27845001 0.278      0.22846    0.27694022]]\n",
      "\n",
      " [[0.274      0.27       0.274      0.271      0.43966    0.26996695]\n",
      "  [0.27629999 0.27139999 0.276      0.275      0.42158    0.27395166]\n",
      "  [0.28       0.273      0.27704999 0.276      0.45931    0.27494781]\n",
      "  [0.28       0.273      0.27845001 0.278      0.22846    0.27694022]\n",
      "  [0.279      0.274      0.276      0.27679999 0.59641    0.27574478]]\n",
      "\n",
      " [[0.27629999 0.27139999 0.276      0.275      0.42158    0.27395166]\n",
      "  [0.28       0.273      0.27704999 0.276      0.45931    0.27494781]\n",
      "  [0.28       0.273      0.27845001 0.278      0.22846    0.27694022]\n",
      "  [0.279      0.274      0.276      0.27679999 0.59641    0.27574478]\n",
      "  [0.287      0.27504999 0.287      0.27675    0.59924    0.27569501]]\n",
      "\n",
      " [[0.28       0.273      0.27704999 0.276      0.45931    0.27494781]\n",
      "  [0.28       0.273      0.27845001 0.278      0.22846    0.27694022]\n",
      "  [0.279      0.274      0.276      0.27679999 0.59641    0.27574478]\n",
      "  [0.287      0.27504999 0.287      0.27675    0.59924    0.27569501]\n",
      "  [0.29239999 0.2875     0.29239999 0.28845001 0.1232     0.2873504 ]]\n",
      "\n",
      " [[0.28       0.273      0.27845001 0.278      0.22846    0.27694022]\n",
      "  [0.279      0.274      0.276      0.27679999 0.59641    0.27574478]\n",
      "  [0.287      0.27504999 0.287      0.27675    0.59924    0.27569501]\n",
      "  [0.29239999 0.2875     0.29239999 0.28845001 0.1232     0.2873504 ]\n",
      "  [0.297      0.292      0.296      0.29239999 0.19862    0.29128537]]\n",
      "\n",
      " [[0.279      0.274      0.276      0.27679999 0.59641    0.27574478]\n",
      "  [0.287      0.27504999 0.287      0.27675    0.59924    0.27569501]\n",
      "  [0.29239999 0.2875     0.29239999 0.28845001 0.1232     0.2873504 ]\n",
      "  [0.297      0.292      0.296      0.29239999 0.19862    0.29128537]\n",
      "  [0.298      0.291      0.293      0.295      0.73167    0.29387543]]\n",
      "\n",
      " [[0.287      0.27504999 0.287      0.27675    0.59924    0.27569501]\n",
      "  [0.29239999 0.2875     0.29239999 0.28845001 0.1232     0.2873504 ]\n",
      "  [0.297      0.292      0.296      0.29239999 0.19862    0.29128537]\n",
      "  [0.298      0.291      0.293      0.295      0.73167    0.29387543]\n",
      "  [0.292      0.28510001 0.2885     0.292      0.4943     0.29088684]]\n",
      "\n",
      " [[0.29239999 0.2875     0.29239999 0.28845001 0.1232     0.2873504 ]\n",
      "  [0.297      0.292      0.296      0.29239999 0.19862    0.29128537]\n",
      "  [0.298      0.291      0.293      0.295      0.73167    0.29387543]\n",
      "  [0.292      0.28510001 0.2885     0.292      0.4943     0.29088684]\n",
      "  [0.29395001 0.286      0.291      0.287      0.38158    0.28590594]]\n",
      "\n",
      " [[0.297      0.292      0.296      0.29239999 0.19862    0.29128537]\n",
      "  [0.298      0.291      0.293      0.295      0.73167    0.29387543]\n",
      "  [0.292      0.28510001 0.2885     0.292      0.4943     0.29088684]\n",
      "  [0.29395001 0.286      0.291      0.287      0.38158    0.28590594]\n",
      "  [0.2905     0.28639999 0.28639999 0.29       0.35837    0.2888945 ]]\n",
      "\n",
      " [[0.298      0.291      0.293      0.295      0.73167    0.29387543]\n",
      "  [0.292      0.28510001 0.2885     0.292      0.4943     0.29088684]\n",
      "  [0.29395001 0.286      0.291      0.287      0.38158    0.28590594]\n",
      "  [0.2905     0.28639999 0.28639999 0.29       0.35837    0.2888945 ]\n",
      "  [0.28895001 0.2855     0.28639999 0.286      0.22531    0.28490976]]\n",
      "\n",
      " [[0.292      0.28510001 0.2885     0.292      0.4943     0.29088684]\n",
      "  [0.29395001 0.286      0.291      0.287      0.38158    0.28590594]\n",
      "  [0.2905     0.28639999 0.28639999 0.29       0.35837    0.2888945 ]\n",
      "  [0.28895001 0.2855     0.28639999 0.286      0.22531    0.28490976]\n",
      "  [0.28895001 0.28       0.28320001 0.2865     0.23484    0.28540784]]\n",
      "\n",
      " [[0.29395001 0.286      0.291      0.287      0.38158    0.28590594]\n",
      "  [0.2905     0.28639999 0.28639999 0.29       0.35837    0.2888945 ]\n",
      "  [0.28895001 0.2855     0.28639999 0.286      0.22531    0.28490976]\n",
      "  [0.28895001 0.28       0.28320001 0.2865     0.23484    0.28540784]\n",
      "  [0.28460001 0.28       0.2845     0.283      0.15736    0.2819212 ]]\n",
      "\n",
      " [[0.2905     0.28639999 0.28639999 0.29       0.35837    0.2888945 ]\n",
      "  [0.28895001 0.2855     0.28639999 0.286      0.22531    0.28490976]\n",
      "  [0.28895001 0.28       0.28320001 0.2865     0.23484    0.28540784]\n",
      "  [0.28460001 0.28       0.2845     0.283      0.15736    0.2819212 ]\n",
      "  [0.2855     0.28325    0.284      0.285      0.11588    0.28391357]]\n",
      "\n",
      " [[0.28895001 0.2855     0.28639999 0.286      0.22531    0.28490976]\n",
      "  [0.28895001 0.28       0.28320001 0.2865     0.23484    0.28540784]\n",
      "  [0.28460001 0.28       0.2845     0.283      0.15736    0.2819212 ]\n",
      "  [0.2855     0.28325    0.284      0.285      0.11588    0.28391357]\n",
      "  [0.285      0.283      0.284      0.284      0.17606    0.28291736]]\n",
      "\n",
      " [[0.28895001 0.28       0.28320001 0.2865     0.23484    0.28540784]\n",
      "  [0.28460001 0.28       0.2845     0.283      0.15736    0.2819212 ]\n",
      "  [0.2855     0.28325    0.284      0.285      0.11588    0.28391357]\n",
      "  [0.285      0.283      0.284      0.284      0.17606    0.28291736]\n",
      "  [0.286      0.2805     0.28389999 0.28375    0.18845    0.28266833]]\n",
      "\n",
      " [[0.28460001 0.28       0.2845     0.283      0.15736    0.2819212 ]\n",
      "  [0.2855     0.28325    0.284      0.285      0.11588    0.28391357]\n",
      "  [0.285      0.283      0.284      0.284      0.17606    0.28291736]\n",
      "  [0.286      0.2805     0.28389999 0.28375    0.18845    0.28266833]\n",
      "  [0.286      0.277      0.281      0.28389999 0.32842    0.28281775]]\n",
      "\n",
      " [[0.2855     0.28325    0.284      0.285      0.11588    0.28391357]\n",
      "  [0.285      0.283      0.284      0.284      0.17606    0.28291736]\n",
      "  [0.286      0.2805     0.28389999 0.28375    0.18845    0.28266833]\n",
      "  [0.286      0.277      0.281      0.28389999 0.32842    0.28281775]\n",
      "  [0.2845     0.278      0.283      0.28       0.41967    0.27893262]]\n",
      "\n",
      " [[0.285      0.283      0.284      0.284      0.17606    0.28291736]\n",
      "  [0.286      0.2805     0.28389999 0.28375    0.18845    0.28266833]\n",
      "  [0.286      0.277      0.281      0.28389999 0.32842    0.28281775]\n",
      "  [0.2845     0.278      0.283      0.28       0.41967    0.27893262]\n",
      "  [0.28385001 0.27       0.28375    0.282      0.48905    0.28092502]]\n",
      "\n",
      " [[0.286      0.2805     0.28389999 0.28375    0.18845    0.28266833]\n",
      "  [0.286      0.277      0.281      0.28389999 0.32842    0.28281775]\n",
      "  [0.2845     0.278      0.283      0.28       0.41967    0.27893262]\n",
      "  [0.28385001 0.27       0.28375    0.282      0.48905    0.28092502]\n",
      "  [0.288      0.28360001 0.28795001 0.28429999 0.24804    0.28321622]]\n",
      "\n",
      " [[0.286      0.277      0.281      0.28389999 0.32842    0.28281775]\n",
      "  [0.2845     0.278      0.283      0.28       0.41967    0.27893262]\n",
      "  [0.28385001 0.27       0.28375    0.282      0.48905    0.28092502]\n",
      "  [0.288      0.28360001 0.28795001 0.28429999 0.24804    0.28321622]\n",
      "  [0.29020001 0.288      0.28864999 0.288      0.32706    0.28690213]]\n",
      "\n",
      " [[0.2845     0.278      0.283      0.28       0.41967    0.27893262]\n",
      "  [0.28385001 0.27       0.28375    0.282      0.48905    0.28092502]\n",
      "  [0.288      0.28360001 0.28795001 0.28429999 0.24804    0.28321622]\n",
      "  [0.29020001 0.288      0.28864999 0.288      0.32706    0.28690213]\n",
      "  [0.2895     0.28304999 0.28489999 0.2885     0.71984    0.28740021]]\n",
      "\n",
      " [[0.28385001 0.27       0.28375    0.282      0.48905    0.28092502]\n",
      "  [0.288      0.28360001 0.28795001 0.28429999 0.24804    0.28321622]\n",
      "  [0.29020001 0.288      0.28864999 0.288      0.32706    0.28690213]\n",
      "  [0.2895     0.28304999 0.28489999 0.2885     0.71984    0.28740021]\n",
      "  [0.289      0.283      0.284      0.285      0.4093     0.28391357]]]\n",
      "y_test: [[0.25751462]\n",
      " [0.25442639]\n",
      " [0.25671765]\n",
      " [0.26598218]\n",
      " [0.26598218]\n",
      " [0.26000507]\n",
      " [0.26299362]\n",
      " [0.26100122]\n",
      " [0.26398981]\n",
      " [0.26498599]\n",
      " [0.26613159]\n",
      " [0.26548404]\n",
      " [0.26473694]\n",
      " [0.26443808]\n",
      " [0.26996695]\n",
      " [0.26996695]\n",
      " [0.27395166]\n",
      " [0.27494781]\n",
      " [0.27694022]\n",
      " [0.27574478]\n",
      " [0.27569501]\n",
      " [0.2873504 ]\n",
      " [0.29128537]\n",
      " [0.29387543]\n",
      " [0.29088684]\n",
      " [0.28590594]\n",
      " [0.2888945 ]\n",
      " [0.28490976]\n",
      " [0.28540784]\n",
      " [0.2819212 ]\n",
      " [0.28391357]]\n",
      "X_train (353, 5, 6)\n",
      "y_train (353, 1)\n",
      "X_test (31, 5, 6)\n",
      "y_test (31, 1)\n"
     ]
    }
   ],
   "source": [
    "df_val = df.values / 1000\n",
    "#for a in range(len(df_val[0])-2):\n",
    "#    df_val[:,a] /= 1000\n",
    "window  =  5\n",
    "X_train, y_train, X_test, y_test = load_data_v2(df_val[::-1], window)\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 317 samples, validate on 36 samples\n",
      "Epoch 1/250\n",
      "317/317 [==============================] - 11s 35ms/step - loss: 0.1158 - acc: 0.0000e+00 - val_loss: 0.0628 - val_acc: 0.0000e+00\n",
      "Epoch 2/250\n",
      "317/317 [==============================] - 0s 636us/step - loss: 0.1145 - acc: 0.0000e+00 - val_loss: 0.0618 - val_acc: 0.0000e+00\n",
      "Epoch 3/250\n",
      "317/317 [==============================] - 0s 492us/step - loss: 0.1130 - acc: 0.0000e+00 - val_loss: 0.0608 - val_acc: 0.0000e+00\n",
      "Epoch 4/250\n",
      "317/317 [==============================] - 0s 624us/step - loss: 0.1115 - acc: 0.0000e+00 - val_loss: 0.0596 - val_acc: 0.0000e+00\n",
      "Epoch 5/250\n",
      "317/317 [==============================] - 0s 668us/step - loss: 0.1096 - acc: 0.0000e+00 - val_loss: 0.0582 - val_acc: 0.0000e+00\n",
      "Epoch 6/250\n",
      "317/317 [==============================] - 0s 553us/step - loss: 0.1076 - acc: 0.0000e+00 - val_loss: 0.0567 - val_acc: 0.0000e+00\n",
      "Epoch 7/250\n",
      "317/317 [==============================] - 0s 617us/step - loss: 0.1053 - acc: 0.0000e+00 - val_loss: 0.0549 - val_acc: 0.0000e+00\n",
      "Epoch 8/250\n",
      "317/317 [==============================] - 0s 761us/step - loss: 0.1026 - acc: 0.0000e+00 - val_loss: 0.0529 - val_acc: 0.0000e+00\n",
      "Epoch 9/250\n",
      "317/317 [==============================] - 0s 523us/step - loss: 0.0994 - acc: 0.0000e+00 - val_loss: 0.0506 - val_acc: 0.0000e+00\n",
      "Epoch 10/250\n",
      "317/317 [==============================] - 0s 599us/step - loss: 0.0956 - acc: 0.0000e+00 - val_loss: 0.0479 - val_acc: 0.0000e+00\n",
      "Epoch 11/250\n",
      "317/317 [==============================] - 0s 550us/step - loss: 0.0911 - acc: 0.0000e+00 - val_loss: 0.0447 - val_acc: 0.0000e+00\n",
      "Epoch 12/250\n",
      "317/317 [==============================] - 0s 623us/step - loss: 0.0859 - acc: 0.0000e+00 - val_loss: 0.0409 - val_acc: 0.0000e+00\n",
      "Epoch 13/250\n",
      "317/317 [==============================] - 0s 583us/step - loss: 0.0796 - acc: 0.0000e+00 - val_loss: 0.0366 - val_acc: 0.0000e+00\n",
      "Epoch 14/250\n",
      "317/317 [==============================] - 0s 628us/step - loss: 0.0722 - acc: 0.0000e+00 - val_loss: 0.0315 - val_acc: 0.0000e+00\n",
      "Epoch 15/250\n",
      "317/317 [==============================] - 0s 658us/step - loss: 0.0639 - acc: 0.0000e+00 - val_loss: 0.0258 - val_acc: 0.0000e+00\n",
      "Epoch 16/250\n",
      "317/317 [==============================] - 0s 589us/step - loss: 0.0538 - acc: 0.0000e+00 - val_loss: 0.0196 - val_acc: 0.0000e+00\n",
      "Epoch 17/250\n",
      "317/317 [==============================] - 0s 595us/step - loss: 0.0432 - acc: 0.0000e+00 - val_loss: 0.0130 - val_acc: 0.0000e+00\n",
      "Epoch 18/250\n",
      "317/317 [==============================] - 0s 559us/step - loss: 0.0321 - acc: 0.0000e+00 - val_loss: 0.0068 - val_acc: 0.0000e+00\n",
      "Epoch 19/250\n",
      "317/317 [==============================] - 0s 595us/step - loss: 0.0214 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 20/250\n",
      "317/317 [==============================] - 0s 625us/step - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 7.4561e-05 - val_acc: 0.0000e+00\n",
      "Epoch 21/250\n",
      "317/317 [==============================] - 0s 528us/step - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 22/250\n",
      "317/317 [==============================] - 0s 614us/step - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0104 - val_acc: 0.0000e+00\n",
      "Epoch 23/250\n",
      "317/317 [==============================] - 0s 623us/step - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.0181 - val_acc: 0.0000e+00\n",
      "Epoch 24/250\n",
      "317/317 [==============================] - 0s 520us/step - loss: 0.0183 - acc: 0.0000e+00 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 25/250\n",
      "317/317 [==============================] - 0s 604us/step - loss: 0.0209 - acc: 0.0000e+00 - val_loss: 0.0200 - val_acc: 0.0000e+00\n",
      "Epoch 26/250\n",
      "317/317 [==============================] - 0s 529us/step - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.0159 - val_acc: 0.0000e+00\n",
      "Epoch 27/250\n",
      "317/317 [==============================] - 0s 599us/step - loss: 0.0161 - acc: 0.0000e+00 - val_loss: 0.0109 - val_acc: 0.0000e+00\n",
      "Epoch 28/250\n",
      "317/317 [==============================] - 0s 532us/step - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.0065 - val_acc: 0.0000e+00\n",
      "Epoch 29/250\n",
      "317/317 [==============================] - 0s 604us/step - loss: 0.0083 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 30/250\n",
      "317/317 [==============================] - 0s 519us/step - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 31/250\n",
      "317/317 [==============================] - 0s 573us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 3.9737e-04 - val_acc: 0.0000e+00\n",
      "Epoch 32/250\n",
      "317/317 [==============================] - 0s 541us/step - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 6.9052e-05 - val_acc: 0.0000e+00\n",
      "Epoch 33/250\n",
      "317/317 [==============================] - 0s 615us/step - loss: 0.0070 - acc: 0.0000e+00 - val_loss: 7.4539e-05 - val_acc: 0.0000e+00\n",
      "Epoch 34/250\n",
      "317/317 [==============================] - 0s 543us/step - loss: 0.0083 - acc: 0.0000e+00 - val_loss: 1.9386e-04 - val_acc: 0.0000e+00\n",
      "Epoch 35/250\n",
      "317/317 [==============================] - 0s 587us/step - loss: 0.0092 - acc: 0.0000e+00 - val_loss: 2.9525e-04 - val_acc: 0.0000e+00\n",
      "Epoch 36/250\n",
      "317/317 [==============================] - 0s 606us/step - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 3.1835e-04 - val_acc: 0.0000e+00\n",
      "Epoch 37/250\n",
      "317/317 [==============================] - 0s 644us/step - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 2.6036e-04 - val_acc: 0.0000e+00\n",
      "Epoch 38/250\n",
      "317/317 [==============================] - 0s 711us/step - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 1.5603e-04 - val_acc: 0.0000e+00\n",
      "Epoch 39/250\n",
      "317/317 [==============================] - 0s 540us/step - loss: 0.0088 - acc: 0.0000e+00 - val_loss: 6.3046e-05 - val_acc: 0.0000e+00\n",
      "Epoch 40/250\n",
      "317/317 [==============================] - 0s 517us/step - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 5.1372e-05 - val_acc: 0.0000e+00\n",
      "Epoch 41/250\n",
      "317/317 [==============================] - 0s 515us/step - loss: 0.0071 - acc: 0.0000e+00 - val_loss: 1.9286e-04 - val_acc: 0.0000e+00\n",
      "Epoch 42/250\n",
      "317/317 [==============================] - 0s 518us/step - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 5.4232e-04 - val_acc: 0.0000e+00\n",
      "Epoch 43/250\n",
      "317/317 [==============================] - 0s 521us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 44/250\n",
      "317/317 [==============================] - 0s 578us/step - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 45/250\n",
      "317/317 [==============================] - 0s 478us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 46/250\n",
      "317/317 [==============================] - 0s 504us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 47/250\n",
      "317/317 [==============================] - 0s 522us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 48/250\n",
      "317/317 [==============================] - 0s 501us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0051 - val_acc: 0.0000e+00\n",
      "Epoch 49/250\n",
      "317/317 [==============================] - 0s 504us/step - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0053 - val_acc: 0.0000e+00\n",
      "Epoch 50/250\n",
      "317/317 [==============================] - 0s 506us/step - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
      "Epoch 51/250\n",
      "317/317 [==============================] - 0s 493us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 52/250\n",
      "317/317 [==============================] - 0s 502us/step - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 53/250\n",
      "317/317 [==============================] - 0s 528us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 54/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 55/250\n",
      "317/317 [==============================] - 0s 497us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 56/250\n",
      "317/317 [==============================] - 0s 513us/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 57/250\n",
      "317/317 [==============================] - 0s 505us/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 8.3835e-04 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/250\n",
      "317/317 [==============================] - 0s 495us/step - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 6.1056e-04 - val_acc: 0.0000e+00\n",
      "Epoch 59/250\n",
      "317/317 [==============================] - 0s 483us/step - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 4.7890e-04 - val_acc: 0.0000e+00\n",
      "Epoch 60/250\n",
      "317/317 [==============================] - 0s 459us/step - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 4.2649e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/250\n",
      "317/317 [==============================] - 0s 496us/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 4.3167e-04 - val_acc: 0.0000e+00\n",
      "Epoch 62/250\n",
      "317/317 [==============================] - 0s 495us/step - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 4.9126e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/250\n",
      "317/317 [==============================] - 0s 492us/step - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 6.1176e-04 - val_acc: 0.0000e+00\n",
      "Epoch 64/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 7.9040e-04 - val_acc: 0.0000e+00\n",
      "Epoch 65/250\n",
      "317/317 [==============================] - 0s 470us/step - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 66/250\n",
      "317/317 [==============================] - 0s 501us/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 67/250\n",
      "317/317 [==============================] - 0s 466us/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 68/250\n",
      "317/317 [==============================] - 0s 470us/step - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 69/250\n",
      "317/317 [==============================] - 0s 483us/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 70/250\n",
      "317/317 [==============================] - 0s 487us/step - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 71/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 72/250\n",
      "317/317 [==============================] - 0s 478us/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 73/250\n",
      "317/317 [==============================] - 0s 476us/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 74/250\n",
      "317/317 [==============================] - 0s 484us/step - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 75/250\n",
      "317/317 [==============================] - 0s 484us/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 76/250\n",
      "317/317 [==============================] - 0s 549us/step - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 9.4241e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/250\n",
      "317/317 [==============================] - 0s 501us/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 7.9646e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/250\n",
      "317/317 [==============================] - 0s 489us/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 7.1439e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/250\n",
      "317/317 [==============================] - 0s 483us/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 6.9780e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/250\n",
      "317/317 [==============================] - 0s 486us/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 7.2889e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/250\n",
      "317/317 [==============================] - 0s 470us/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 8.0170e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/250\n",
      "317/317 [==============================] - 0s 486us/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 9.0759e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/250\n",
      "317/317 [==============================] - 0s 457us/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 84/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 0.0017 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 85/250\n",
      "317/317 [==============================] - 0s 478us/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 86/250\n",
      "317/317 [==============================] - 0s 477us/step - loss: 0.0016 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 87/250\n",
      "317/317 [==============================] - 0s 478us/step - loss: 0.0014 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 88/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 89/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 0.0014 - acc: 0.0000e+00 - val_loss: 9.8891e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 8.4927e-04 - val_acc: 0.0000e+00\n",
      "Epoch 91/250\n",
      "317/317 [==============================] - 0s 484us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 7.1820e-04 - val_acc: 0.0000e+00\n",
      "Epoch 92/250\n",
      "317/317 [==============================] - 0s 472us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 6.3560e-04 - val_acc: 0.0000e+00\n",
      "Epoch 93/250\n",
      "317/317 [==============================] - 0s 471us/step - loss: 0.0014 - acc: 0.0000e+00 - val_loss: 6.1418e-04 - val_acc: 0.0000e+00\n",
      "Epoch 94/250\n",
      "317/317 [==============================] - 0s 493us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 6.1144e-04 - val_acc: 0.0000e+00\n",
      "Epoch 95/250\n",
      "317/317 [==============================] - 0s 470us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 6.2893e-04 - val_acc: 0.0000e+00\n",
      "Epoch 96/250\n",
      "317/317 [==============================] - 0s 476us/step - loss: 9.7858e-04 - acc: 0.0000e+00 - val_loss: 6.4444e-04 - val_acc: 0.0000e+00\n",
      "Epoch 97/250\n",
      "317/317 [==============================] - 0s 475us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 6.5388e-04 - val_acc: 0.0000e+00\n",
      "Epoch 98/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 6.6268e-04 - val_acc: 0.0000e+00\n",
      "Epoch 99/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 6.3415e-04 - val_acc: 0.0000e+00\n",
      "Epoch 100/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 6.0081e-04 - val_acc: 0.0000e+00\n",
      "Epoch 101/250\n",
      "317/317 [==============================] - 0s 517us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 5.3502e-04 - val_acc: 0.0000e+00\n",
      "Epoch 102/250\n",
      "317/317 [==============================] - 0s 477us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 4.5365e-04 - val_acc: 0.0000e+00\n",
      "Epoch 103/250\n",
      "317/317 [==============================] - 0s 511us/step - loss: 9.5037e-04 - acc: 0.0000e+00 - val_loss: 3.8483e-04 - val_acc: 0.0000e+00\n",
      "Epoch 104/250\n",
      "317/317 [==============================] - 0s 485us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 3.4399e-04 - val_acc: 0.0000e+00\n",
      "Epoch 105/250\n",
      "317/317 [==============================] - 0s 483us/step - loss: 9.8339e-04 - acc: 0.0000e+00 - val_loss: 3.0933e-04 - val_acc: 0.0000e+00\n",
      "Epoch 106/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 8.7968e-04 - acc: 0.0000e+00 - val_loss: 2.8496e-04 - val_acc: 0.0000e+00\n",
      "Epoch 107/250\n",
      "317/317 [==============================] - 0s 485us/step - loss: 9.7694e-04 - acc: 0.0000e+00 - val_loss: 2.7666e-04 - val_acc: 0.0000e+00\n",
      "Epoch 108/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 7.6765e-04 - acc: 0.0000e+00 - val_loss: 2.6751e-04 - val_acc: 0.0000e+00\n",
      "Epoch 109/250\n",
      "317/317 [==============================] - 0s 470us/step - loss: 8.8684e-04 - acc: 0.0000e+00 - val_loss: 2.5571e-04 - val_acc: 0.0000e+00\n",
      "Epoch 110/250\n",
      "317/317 [==============================] - 0s 502us/step - loss: 7.4353e-04 - acc: 0.0000e+00 - val_loss: 2.4356e-04 - val_acc: 0.0000e+00\n",
      "Epoch 111/250\n",
      "317/317 [==============================] - 0s 488us/step - loss: 8.8582e-04 - acc: 0.0000e+00 - val_loss: 2.2970e-04 - val_acc: 0.0000e+00\n",
      "Epoch 112/250\n",
      "317/317 [==============================] - 0s 483us/step - loss: 9.3048e-04 - acc: 0.0000e+00 - val_loss: 2.0819e-04 - val_acc: 0.0000e+00\n",
      "Epoch 113/250\n",
      "317/317 [==============================] - 0s 469us/step - loss: 9.2017e-04 - acc: 0.0000e+00 - val_loss: 1.8949e-04 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 7.6929e-04 - acc: 0.0000e+00 - val_loss: 1.7943e-04 - val_acc: 0.0000e+00\n",
      "Epoch 115/250\n",
      "317/317 [==============================] - 0s 479us/step - loss: 8.1617e-04 - acc: 0.0000e+00 - val_loss: 1.7340e-04 - val_acc: 0.0000e+00\n",
      "Epoch 116/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 8.5496e-04 - acc: 0.0000e+00 - val_loss: 1.7363e-04 - val_acc: 0.0000e+00\n",
      "Epoch 117/250\n",
      "317/317 [==============================] - 0s 489us/step - loss: 7.1706e-04 - acc: 0.0000e+00 - val_loss: 1.7411e-04 - val_acc: 0.0000e+00\n",
      "Epoch 118/250\n",
      "317/317 [==============================] - 0s 505us/step - loss: 8.6064e-04 - acc: 0.0000e+00 - val_loss: 1.6092e-04 - val_acc: 0.0000e+00\n",
      "Epoch 119/250\n",
      "317/317 [==============================] - 0s 470us/step - loss: 7.2140e-04 - acc: 0.0000e+00 - val_loss: 1.4906e-04 - val_acc: 0.0000e+00\n",
      "Epoch 120/250\n",
      "317/317 [==============================] - 0s 488us/step - loss: 8.6326e-04 - acc: 0.0000e+00 - val_loss: 1.3492e-04 - val_acc: 0.0000e+00\n",
      "Epoch 121/250\n",
      "317/317 [==============================] - 0s 476us/step - loss: 8.7308e-04 - acc: 0.0000e+00 - val_loss: 1.2225e-04 - val_acc: 0.0000e+00\n",
      "Epoch 122/250\n",
      "317/317 [==============================] - 0s 477us/step - loss: 8.2036e-04 - acc: 0.0000e+00 - val_loss: 1.1501e-04 - val_acc: 0.0000e+00\n",
      "Epoch 123/250\n",
      "317/317 [==============================] - 0s 472us/step - loss: 8.5619e-04 - acc: 0.0000e+00 - val_loss: 1.1221e-04 - val_acc: 0.0000e+00\n",
      "Epoch 124/250\n",
      "317/317 [==============================] - 0s 492us/step - loss: 6.8038e-04 - acc: 0.0000e+00 - val_loss: 1.0723e-04 - val_acc: 0.0000e+00\n",
      "Epoch 125/250\n",
      "317/317 [==============================] - 0s 472us/step - loss: 8.1140e-04 - acc: 0.0000e+00 - val_loss: 1.0416e-04 - val_acc: 0.0000e+00\n",
      "Epoch 126/250\n",
      "317/317 [==============================] - 0s 524us/step - loss: 8.8865e-04 - acc: 0.0000e+00 - val_loss: 1.0467e-04 - val_acc: 0.0000e+00\n",
      "Epoch 127/250\n",
      "317/317 [==============================] - 0s 555us/step - loss: 6.6924e-04 - acc: 0.0000e+00 - val_loss: 1.0816e-04 - val_acc: 0.0000e+00\n",
      "Epoch 128/250\n",
      "317/317 [==============================] - 0s 535us/step - loss: 8.6271e-04 - acc: 0.0000e+00 - val_loss: 1.1237e-04 - val_acc: 0.0000e+00\n",
      "Epoch 129/250\n",
      "317/317 [==============================] - 0s 547us/step - loss: 7.8153e-04 - acc: 0.0000e+00 - val_loss: 1.0801e-04 - val_acc: 0.0000e+00\n",
      "Epoch 130/250\n",
      "317/317 [==============================] - 0s 498us/step - loss: 7.4165e-04 - acc: 0.0000e+00 - val_loss: 9.9257e-05 - val_acc: 0.0000e+00\n",
      "Epoch 131/250\n",
      "317/317 [==============================] - 0s 537us/step - loss: 7.5663e-04 - acc: 0.0000e+00 - val_loss: 9.0564e-05 - val_acc: 0.0000e+00\n",
      "Epoch 132/250\n",
      "317/317 [==============================] - 0s 566us/step - loss: 7.4521e-04 - acc: 0.0000e+00 - val_loss: 8.5411e-05 - val_acc: 0.0000e+00\n",
      "Epoch 133/250\n",
      "317/317 [==============================] - 0s 584us/step - loss: 7.4472e-04 - acc: 0.0000e+00 - val_loss: 8.0774e-05 - val_acc: 0.0000e+00\n",
      "Epoch 134/250\n",
      "317/317 [==============================] - 0s 492us/step - loss: 8.6735e-04 - acc: 0.0000e+00 - val_loss: 7.7280e-05 - val_acc: 0.0000e+00\n",
      "Epoch 135/250\n",
      "317/317 [==============================] - 0s 496us/step - loss: 7.8006e-04 - acc: 0.0000e+00 - val_loss: 7.6286e-05 - val_acc: 0.0000e+00\n",
      "Epoch 136/250\n",
      "317/317 [==============================] - 0s 536us/step - loss: 7.4089e-04 - acc: 0.0000e+00 - val_loss: 7.8978e-05 - val_acc: 0.0000e+00\n",
      "Epoch 137/250\n",
      "317/317 [==============================] - 0s 493us/step - loss: 8.3215e-04 - acc: 0.0000e+00 - val_loss: 8.5943e-05 - val_acc: 0.0000e+00\n",
      "Epoch 138/250\n",
      "317/317 [==============================] - 0s 465us/step - loss: 8.2699e-04 - acc: 0.0000e+00 - val_loss: 9.3168e-05 - val_acc: 0.0000e+00\n",
      "Epoch 139/250\n",
      "317/317 [==============================] - 0s 471us/step - loss: 9.2068e-04 - acc: 0.0000e+00 - val_loss: 9.7073e-05 - val_acc: 0.0000e+00\n",
      "Epoch 140/250\n",
      "317/317 [==============================] - 0s 485us/step - loss: 8.7365e-04 - acc: 0.0000e+00 - val_loss: 9.5222e-05 - val_acc: 0.0000e+00\n",
      "Epoch 141/250\n",
      "317/317 [==============================] - 0s 485us/step - loss: 9.0928e-04 - acc: 0.0000e+00 - val_loss: 9.3069e-05 - val_acc: 0.0000e+00\n",
      "Epoch 142/250\n",
      "317/317 [==============================] - 0s 475us/step - loss: 8.5286e-04 - acc: 0.0000e+00 - val_loss: 9.2515e-05 - val_acc: 0.0000e+00\n",
      "Epoch 143/250\n",
      "317/317 [==============================] - 0s 503us/step - loss: 6.7056e-04 - acc: 0.0000e+00 - val_loss: 8.9775e-05 - val_acc: 0.0000e+00\n",
      "Epoch 144/250\n",
      "317/317 [==============================] - 0s 465us/step - loss: 7.6285e-04 - acc: 0.0000e+00 - val_loss: 8.2454e-05 - val_acc: 0.0000e+00\n",
      "Epoch 145/250\n",
      "317/317 [==============================] - 0s 501us/step - loss: 9.7296e-04 - acc: 0.0000e+00 - val_loss: 7.8871e-05 - val_acc: 0.0000e+00\n",
      "Epoch 146/250\n",
      "317/317 [==============================] - 0s 488us/step - loss: 7.5560e-04 - acc: 0.0000e+00 - val_loss: 7.6374e-05 - val_acc: 0.0000e+00\n",
      "Epoch 147/250\n",
      "317/317 [==============================] - 0s 485us/step - loss: 7.7920e-04 - acc: 0.0000e+00 - val_loss: 7.6221e-05 - val_acc: 0.0000e+00\n",
      "Epoch 148/250\n",
      "317/317 [==============================] - 0s 480us/step - loss: 7.6169e-04 - acc: 0.0000e+00 - val_loss: 7.8272e-05 - val_acc: 0.0000e+00\n",
      "Epoch 149/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 7.0091e-04 - acc: 0.0000e+00 - val_loss: 8.0152e-05 - val_acc: 0.0000e+00\n",
      "Epoch 150/250\n",
      "317/317 [==============================] - 0s 501us/step - loss: 6.9767e-04 - acc: 0.0000e+00 - val_loss: 8.3710e-05 - val_acc: 0.0000e+00\n",
      "Epoch 151/250\n",
      "317/317 [==============================] - 0s 475us/step - loss: 8.4767e-04 - acc: 0.0000e+00 - val_loss: 8.8312e-05 - val_acc: 0.0000e+00\n",
      "Epoch 152/250\n",
      "317/317 [==============================] - 0s 531us/step - loss: 6.9392e-04 - acc: 0.0000e+00 - val_loss: 9.2535e-05 - val_acc: 0.0000e+00\n",
      "Epoch 153/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 8.0300e-04 - acc: 0.0000e+00 - val_loss: 9.0715e-05 - val_acc: 0.0000e+00\n",
      "Epoch 154/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 7.2587e-04 - acc: 0.0000e+00 - val_loss: 8.4182e-05 - val_acc: 0.0000e+00\n",
      "Epoch 155/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 7.7109e-04 - acc: 0.0000e+00 - val_loss: 8.0203e-05 - val_acc: 0.0000e+00\n",
      "Epoch 156/250\n",
      "317/317 [==============================] - 0s 485us/step - loss: 8.3407e-04 - acc: 0.0000e+00 - val_loss: 7.7173e-05 - val_acc: 0.0000e+00\n",
      "Epoch 157/250\n",
      "317/317 [==============================] - 0s 481us/step - loss: 8.6052e-04 - acc: 0.0000e+00 - val_loss: 7.6649e-05 - val_acc: 0.0000e+00\n",
      "Epoch 158/250\n",
      "317/317 [==============================] - 0s 483us/step - loss: 7.3589e-04 - acc: 0.0000e+00 - val_loss: 7.8004e-05 - val_acc: 0.0000e+00\n",
      "Epoch 159/250\n",
      "317/317 [==============================] - 0s 483us/step - loss: 7.4197e-04 - acc: 0.0000e+00 - val_loss: 7.9932e-05 - val_acc: 0.0000e+00\n",
      "Epoch 160/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 7.8011e-04 - acc: 0.0000e+00 - val_loss: 8.2937e-05 - val_acc: 0.0000e+00\n",
      "Epoch 161/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 8.6178e-04 - acc: 0.0000e+00 - val_loss: 8.4641e-05 - val_acc: 0.0000e+00\n",
      "Epoch 162/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 7.9761e-04 - acc: 0.0000e+00 - val_loss: 8.4686e-05 - val_acc: 0.0000e+00\n",
      "Epoch 163/250\n",
      "317/317 [==============================] - 0s 488us/step - loss: 7.2889e-04 - acc: 0.0000e+00 - val_loss: 8.4062e-05 - val_acc: 0.0000e+00\n",
      "Epoch 164/250\n",
      "317/317 [==============================] - 0s 484us/step - loss: 9.3725e-04 - acc: 0.0000e+00 - val_loss: 8.4838e-05 - val_acc: 0.0000e+00\n",
      "Epoch 165/250\n",
      "317/317 [==============================] - 0s 477us/step - loss: 8.0332e-04 - acc: 0.0000e+00 - val_loss: 8.6538e-05 - val_acc: 0.0000e+00\n",
      "Epoch 166/250\n",
      "317/317 [==============================] - 0s 487us/step - loss: 7.6182e-04 - acc: 0.0000e+00 - val_loss: 8.6488e-05 - val_acc: 0.0000e+00\n",
      "Epoch 167/250\n",
      "317/317 [==============================] - 0s 487us/step - loss: 8.0040e-04 - acc: 0.0000e+00 - val_loss: 8.6796e-05 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 8.4259e-04 - acc: 0.0000e+00 - val_loss: 8.7758e-05 - val_acc: 0.0000e+00\n",
      "Epoch 169/250\n",
      "317/317 [==============================] - 0s 478us/step - loss: 8.7219e-04 - acc: 0.0000e+00 - val_loss: 8.6475e-05 - val_acc: 0.0000e+00\n",
      "Epoch 170/250\n",
      "317/317 [==============================] - 0s 473us/step - loss: 7.7412e-04 - acc: 0.0000e+00 - val_loss: 8.3567e-05 - val_acc: 0.0000e+00\n",
      "Epoch 171/250\n",
      "317/317 [==============================] - 0s 468us/step - loss: 7.5247e-04 - acc: 0.0000e+00 - val_loss: 8.4215e-05 - val_acc: 0.0000e+00\n",
      "Epoch 172/250\n",
      "317/317 [==============================] - 0s 467us/step - loss: 7.6449e-04 - acc: 0.0000e+00 - val_loss: 8.3043e-05 - val_acc: 0.0000e+00\n",
      "Epoch 173/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 7.3372e-04 - acc: 0.0000e+00 - val_loss: 8.2343e-05 - val_acc: 0.0000e+00\n",
      "Epoch 174/250\n",
      "317/317 [==============================] - 0s 474us/step - loss: 8.0748e-04 - acc: 0.0000e+00 - val_loss: 8.0804e-05 - val_acc: 0.0000e+00\n",
      "Epoch 175/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 7.7303e-04 - acc: 0.0000e+00 - val_loss: 8.0316e-05 - val_acc: 0.0000e+00\n",
      "Epoch 176/250\n",
      "317/317 [==============================] - 0s 479us/step - loss: 6.8954e-04 - acc: 0.0000e+00 - val_loss: 8.1713e-05 - val_acc: 0.0000e+00\n",
      "Epoch 177/250\n",
      "317/317 [==============================] - 0s 502us/step - loss: 7.1558e-04 - acc: 0.0000e+00 - val_loss: 8.2888e-05 - val_acc: 0.0000e+00\n",
      "Epoch 178/250\n",
      "317/317 [==============================] - 0s 493us/step - loss: 8.8836e-04 - acc: 0.0000e+00 - val_loss: 8.7290e-05 - val_acc: 0.0000e+00\n",
      "Epoch 179/250\n",
      "317/317 [==============================] - 0s 487us/step - loss: 6.7645e-04 - acc: 0.0000e+00 - val_loss: 9.4820e-05 - val_acc: 0.0000e+00\n",
      "Epoch 180/250\n",
      "317/317 [==============================] - 0s 493us/step - loss: 7.8524e-04 - acc: 0.0000e+00 - val_loss: 1.0154e-04 - val_acc: 0.0000e+00\n",
      "Epoch 181/250\n",
      "317/317 [==============================] - 0s 469us/step - loss: 7.1553e-04 - acc: 0.0000e+00 - val_loss: 9.6925e-05 - val_acc: 0.0000e+00\n",
      "Epoch 182/250\n",
      "317/317 [==============================] - 0s 474us/step - loss: 6.8225e-04 - acc: 0.0000e+00 - val_loss: 8.9423e-05 - val_acc: 0.0000e+00\n",
      "Epoch 183/250\n",
      "317/317 [==============================] - 0s 497us/step - loss: 9.5002e-04 - acc: 0.0000e+00 - val_loss: 8.4798e-05 - val_acc: 0.0000e+00\n",
      "Epoch 184/250\n",
      "317/317 [==============================] - 0s 491us/step - loss: 8.6334e-04 - acc: 0.0000e+00 - val_loss: 8.3003e-05 - val_acc: 0.0000e+00\n",
      "Epoch 185/250\n",
      "317/317 [==============================] - 0s 472us/step - loss: 7.2612e-04 - acc: 0.0000e+00 - val_loss: 8.1154e-05 - val_acc: 0.0000e+00\n",
      "Epoch 186/250\n",
      "317/317 [==============================] - 0s 487us/step - loss: 8.4296e-04 - acc: 0.0000e+00 - val_loss: 8.8275e-05 - val_acc: 0.0000e+00\n",
      "Epoch 187/250\n",
      "317/317 [==============================] - 0s 497us/step - loss: 7.9933e-04 - acc: 0.0000e+00 - val_loss: 9.5094e-05 - val_acc: 0.0000e+00\n",
      "Epoch 188/250\n",
      "317/317 [==============================] - 0s 493us/step - loss: 7.5947e-04 - acc: 0.0000e+00 - val_loss: 1.0139e-04 - val_acc: 0.0000e+00\n",
      "Epoch 189/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 7.8254e-04 - acc: 0.0000e+00 - val_loss: 9.7764e-05 - val_acc: 0.0000e+00\n",
      "Epoch 190/250\n",
      "317/317 [==============================] - 0s 474us/step - loss: 8.3884e-04 - acc: 0.0000e+00 - val_loss: 9.4709e-05 - val_acc: 0.0000e+00\n",
      "Epoch 191/250\n",
      "317/317 [==============================] - 0s 567us/step - loss: 7.8289e-04 - acc: 0.0000e+00 - val_loss: 9.3308e-05 - val_acc: 0.0000e+00\n",
      "Epoch 192/250\n",
      "317/317 [==============================] - 0s 451us/step - loss: 7.9295e-04 - acc: 0.0000e+00 - val_loss: 9.2087e-05 - val_acc: 0.0000e+00\n",
      "Epoch 193/250\n",
      "317/317 [==============================] - 0s 473us/step - loss: 7.2004e-04 - acc: 0.0000e+00 - val_loss: 8.7545e-05 - val_acc: 0.0000e+00\n",
      "Epoch 194/250\n",
      "317/317 [==============================] - 0s 467us/step - loss: 7.8127e-04 - acc: 0.0000e+00 - val_loss: 8.7913e-05 - val_acc: 0.0000e+00\n",
      "Epoch 195/250\n",
      "317/317 [==============================] - 0s 497us/step - loss: 6.7824e-04 - acc: 0.0000e+00 - val_loss: 8.4954e-05 - val_acc: 0.0000e+00\n",
      "Epoch 196/250\n",
      "317/317 [==============================] - 0s 537us/step - loss: 7.5424e-04 - acc: 0.0000e+00 - val_loss: 7.9812e-05 - val_acc: 0.0000e+00\n",
      "Epoch 197/250\n",
      "317/317 [==============================] - 0s 481us/step - loss: 8.3195e-04 - acc: 0.0000e+00 - val_loss: 7.7830e-05 - val_acc: 0.0000e+00\n",
      "Epoch 198/250\n",
      "317/317 [==============================] - 0s 463us/step - loss: 8.4956e-04 - acc: 0.0000e+00 - val_loss: 8.0355e-05 - val_acc: 0.0000e+00\n",
      "Epoch 199/250\n",
      "317/317 [==============================] - 0s 489us/step - loss: 6.8053e-04 - acc: 0.0000e+00 - val_loss: 8.6576e-05 - val_acc: 0.0000e+00\n",
      "Epoch 200/250\n",
      "317/317 [==============================] - 0s 514us/step - loss: 5.9322e-04 - acc: 0.0000e+00 - val_loss: 1.0354e-04 - val_acc: 0.0000e+00\n",
      "Epoch 201/250\n",
      "317/317 [==============================] - 0s 497us/step - loss: 7.9472e-04 - acc: 0.0000e+00 - val_loss: 1.1538e-04 - val_acc: 0.0000e+00\n",
      "Epoch 202/250\n",
      "317/317 [==============================] - 0s 493us/step - loss: 8.8511e-04 - acc: 0.0000e+00 - val_loss: 1.0505e-04 - val_acc: 0.0000e+00\n",
      "Epoch 203/250\n",
      "317/317 [==============================] - 0s 483us/step - loss: 7.9315e-04 - acc: 0.0000e+00 - val_loss: 9.1666e-05 - val_acc: 0.0000e+00\n",
      "Epoch 204/250\n",
      "317/317 [==============================] - 0s 465us/step - loss: 7.5593e-04 - acc: 0.0000e+00 - val_loss: 8.2973e-05 - val_acc: 0.0000e+00\n",
      "Epoch 205/250\n",
      "317/317 [==============================] - 0s 484us/step - loss: 8.2895e-04 - acc: 0.0000e+00 - val_loss: 7.9100e-05 - val_acc: 0.0000e+00\n",
      "Epoch 206/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 7.6447e-04 - acc: 0.0000e+00 - val_loss: 8.2786e-05 - val_acc: 0.0000e+00\n",
      "Epoch 207/250\n",
      "317/317 [==============================] - 0s 470us/step - loss: 6.3797e-04 - acc: 0.0000e+00 - val_loss: 8.9780e-05 - val_acc: 0.0000e+00\n",
      "Epoch 208/250\n",
      "317/317 [==============================] - 0s 461us/step - loss: 7.8394e-04 - acc: 0.0000e+00 - val_loss: 9.4480e-05 - val_acc: 0.0000e+00\n",
      "Epoch 209/250\n",
      "317/317 [==============================] - 0s 501us/step - loss: 7.6581e-04 - acc: 0.0000e+00 - val_loss: 1.0124e-04 - val_acc: 0.0000e+00\n",
      "Epoch 210/250\n",
      "317/317 [==============================] - 0s 477us/step - loss: 8.0130e-04 - acc: 0.0000e+00 - val_loss: 1.0573e-04 - val_acc: 0.0000e+00\n",
      "Epoch 211/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 8.3932e-04 - acc: 0.0000e+00 - val_loss: 1.0326e-04 - val_acc: 0.0000e+00\n",
      "Epoch 212/250\n",
      "317/317 [==============================] - 0s 487us/step - loss: 7.0509e-04 - acc: 0.0000e+00 - val_loss: 9.4851e-05 - val_acc: 0.0000e+00\n",
      "Epoch 213/250\n",
      "317/317 [==============================] - 0s 467us/step - loss: 6.6557e-04 - acc: 0.0000e+00 - val_loss: 9.1946e-05 - val_acc: 0.0000e+00\n",
      "Epoch 214/250\n",
      "317/317 [==============================] - 0s 502us/step - loss: 7.0650e-04 - acc: 0.0000e+00 - val_loss: 9.2218e-05 - val_acc: 0.0000e+00\n",
      "Epoch 215/250\n",
      "317/317 [==============================] - 0s 497us/step - loss: 8.0946e-04 - acc: 0.0000e+00 - val_loss: 8.9389e-05 - val_acc: 0.0000e+00\n",
      "Epoch 216/250\n",
      "317/317 [==============================] - 0s 498us/step - loss: 7.7888e-04 - acc: 0.0000e+00 - val_loss: 8.6422e-05 - val_acc: 0.0000e+00\n",
      "Epoch 217/250\n",
      "317/317 [==============================] - 0s 472us/step - loss: 7.8024e-04 - acc: 0.0000e+00 - val_loss: 8.3758e-05 - val_acc: 0.0000e+00\n",
      "Epoch 218/250\n",
      "317/317 [==============================] - 0s 500us/step - loss: 7.1766e-04 - acc: 0.0000e+00 - val_loss: 8.1099e-05 - val_acc: 0.0000e+00\n",
      "Epoch 219/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 7.3095e-04 - acc: 0.0000e+00 - val_loss: 8.1377e-05 - val_acc: 0.0000e+00\n",
      "Epoch 220/250\n",
      "317/317 [==============================] - 0s 487us/step - loss: 8.2998e-04 - acc: 0.0000e+00 - val_loss: 8.3801e-05 - val_acc: 0.0000e+00\n",
      "Epoch 221/250\n",
      "317/317 [==============================] - 0s 464us/step - loss: 7.1263e-04 - acc: 0.0000e+00 - val_loss: 8.5319e-05 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/250\n",
      "317/317 [==============================] - 0s 484us/step - loss: 7.6999e-04 - acc: 0.0000e+00 - val_loss: 9.2265e-05 - val_acc: 0.0000e+00\n",
      "Epoch 223/250\n",
      "317/317 [==============================] - 0s 478us/step - loss: 8.0937e-04 - acc: 0.0000e+00 - val_loss: 9.6252e-05 - val_acc: 0.0000e+00\n",
      "Epoch 224/250\n",
      "317/317 [==============================] - 0s 470us/step - loss: 8.7223e-04 - acc: 0.0000e+00 - val_loss: 9.4536e-05 - val_acc: 0.0000e+00\n",
      "Epoch 225/250\n",
      "317/317 [==============================] - 0s 504us/step - loss: 8.4174e-04 - acc: 0.0000e+00 - val_loss: 9.0419e-05 - val_acc: 0.0000e+00\n",
      "Epoch 226/250\n",
      "317/317 [==============================] - 0s 474us/step - loss: 8.9885e-04 - acc: 0.0000e+00 - val_loss: 8.6883e-05 - val_acc: 0.0000e+00\n",
      "Epoch 227/250\n",
      "317/317 [==============================] - 0s 466us/step - loss: 7.2975e-04 - acc: 0.0000e+00 - val_loss: 8.6257e-05 - val_acc: 0.0000e+00\n",
      "Epoch 228/250\n",
      "317/317 [==============================] - 0s 476us/step - loss: 8.3359e-04 - acc: 0.0000e+00 - val_loss: 8.9737e-05 - val_acc: 0.0000e+00\n",
      "Epoch 229/250\n",
      "317/317 [==============================] - 0s 480us/step - loss: 7.8154e-04 - acc: 0.0000e+00 - val_loss: 9.3145e-05 - val_acc: 0.0000e+00\n",
      "Epoch 230/250\n",
      "317/317 [==============================] - 0s 463us/step - loss: 7.8465e-04 - acc: 0.0000e+00 - val_loss: 9.8117e-05 - val_acc: 0.0000e+00\n",
      "Epoch 231/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 8.9906e-04 - acc: 0.0000e+00 - val_loss: 1.0183e-04 - val_acc: 0.0000e+00\n",
      "Epoch 232/250\n",
      "317/317 [==============================] - 0s 479us/step - loss: 7.3386e-04 - acc: 0.0000e+00 - val_loss: 1.0406e-04 - val_acc: 0.0000e+00\n",
      "Epoch 233/250\n",
      "317/317 [==============================] - 0s 479us/step - loss: 7.6906e-04 - acc: 0.0000e+00 - val_loss: 9.4910e-05 - val_acc: 0.0000e+00\n",
      "Epoch 234/250\n",
      "317/317 [==============================] - 0s 477us/step - loss: 7.8727e-04 - acc: 0.0000e+00 - val_loss: 8.5394e-05 - val_acc: 0.0000e+00\n",
      "Epoch 235/250\n",
      "317/317 [==============================] - 0s 460us/step - loss: 7.2825e-04 - acc: 0.0000e+00 - val_loss: 7.8363e-05 - val_acc: 0.0000e+00\n",
      "Epoch 236/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 6.8179e-04 - acc: 0.0000e+00 - val_loss: 7.7123e-05 - val_acc: 0.0000e+00\n",
      "Epoch 237/250\n",
      "317/317 [==============================] - 0s 474us/step - loss: 7.8579e-04 - acc: 0.0000e+00 - val_loss: 7.7863e-05 - val_acc: 0.0000e+00\n",
      "Epoch 238/250\n",
      "317/317 [==============================] - 0s 467us/step - loss: 7.6181e-04 - acc: 0.0000e+00 - val_loss: 8.6822e-05 - val_acc: 0.0000e+00\n",
      "Epoch 239/250\n",
      "317/317 [==============================] - 0s 480us/step - loss: 8.3407e-04 - acc: 0.0000e+00 - val_loss: 9.8368e-05 - val_acc: 0.0000e+00\n",
      "Epoch 240/250\n",
      "317/317 [==============================] - 0s 487us/step - loss: 6.9612e-04 - acc: 0.0000e+00 - val_loss: 1.1256e-04 - val_acc: 0.0000e+00\n",
      "Epoch 241/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 6.2838e-04 - acc: 0.0000e+00 - val_loss: 1.1401e-04 - val_acc: 0.0000e+00\n",
      "Epoch 242/250\n",
      "317/317 [==============================] - 0s 475us/step - loss: 8.1628e-04 - acc: 0.0000e+00 - val_loss: 9.5923e-05 - val_acc: 0.0000e+00\n",
      "Epoch 243/250\n",
      "317/317 [==============================] - 0s 480us/step - loss: 6.8221e-04 - acc: 0.0000e+00 - val_loss: 8.4942e-05 - val_acc: 0.0000e+00\n",
      "Epoch 244/250\n",
      "317/317 [==============================] - 0s 489us/step - loss: 7.2323e-04 - acc: 0.0000e+00 - val_loss: 7.9208e-05 - val_acc: 0.0000e+00\n",
      "Epoch 245/250\n",
      "317/317 [==============================] - 0s 462us/step - loss: 7.3275e-04 - acc: 0.0000e+00 - val_loss: 7.8862e-05 - val_acc: 0.0000e+00\n",
      "Epoch 246/250\n",
      "317/317 [==============================] - 0s 481us/step - loss: 7.5793e-04 - acc: 0.0000e+00 - val_loss: 8.4582e-05 - val_acc: 0.0000e+00\n",
      "Epoch 247/250\n",
      "317/317 [==============================] - 0s 499us/step - loss: 6.0673e-04 - acc: 0.0000e+00 - val_loss: 9.3564e-05 - val_acc: 0.0000e+00\n",
      "Epoch 248/250\n",
      "317/317 [==============================] - 0s 469us/step - loss: 8.1997e-04 - acc: 0.0000e+00 - val_loss: 9.8676e-05 - val_acc: 0.0000e+00\n",
      "Epoch 249/250\n",
      "317/317 [==============================] - 0s 500us/step - loss: 7.4788e-04 - acc: 0.0000e+00 - val_loss: 9.7868e-05 - val_acc: 0.0000e+00\n",
      "Epoch 250/250\n",
      "317/317 [==============================] - 0s 491us/step - loss: 6.5171e-04 - acc: 0.0000e+00 - val_loss: 9.2013e-05 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f88ad6a6668>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = build_model([3,lag,1])\n",
    "#model = build_model2([3,window,1])\n",
    "model = build_model2([len(df_val[0])-1,5,1])\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=576,\n",
    "    epochs=250,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 0s 432us/step\n",
      "Train Score: 0.00 MSE (0.02 RMSE)\n",
      "[[[0.265      0.26       0.2645     0.261      0.17853    0.26000507]\n",
      "  [0.26525    0.259      0.2615     0.264      0.28015    0.26299362]\n",
      "  [0.26695001 0.26114999 0.266      0.262      0.15064    0.26100122]\n",
      "  [0.266      0.26310001 0.26425    0.265      0.17396    0.26398981]\n",
      "  [0.26720001 0.264      0.26720001 0.266      0.14868    0.26498599]]\n",
      "\n",
      " [[0.26525    0.259      0.2615     0.264      0.28015    0.26299362]\n",
      "  [0.26695001 0.26114999 0.266      0.262      0.15064    0.26100122]\n",
      "  [0.266      0.26310001 0.26425    0.265      0.17396    0.26398981]\n",
      "  [0.26720001 0.264      0.26720001 0.266      0.14868    0.26498599]\n",
      "  [0.26889999 0.265      0.2665     0.26714999 0.12141    0.26613159]]\n",
      "\n",
      " [[0.26695001 0.26114999 0.266      0.262      0.15064    0.26100122]\n",
      "  [0.266      0.26310001 0.26425    0.265      0.17396    0.26398981]\n",
      "  [0.26720001 0.264      0.26720001 0.266      0.14868    0.26498599]\n",
      "  [0.26889999 0.265      0.2665     0.26714999 0.12141    0.26613159]\n",
      "  [0.26904999 0.26575    0.26579999 0.2665     0.55597    0.26548404]]\n",
      "\n",
      " [[0.266      0.26310001 0.26425    0.265      0.17396    0.26398981]\n",
      "  [0.26720001 0.264      0.26720001 0.266      0.14868    0.26498599]\n",
      "  [0.26889999 0.265      0.2665     0.26714999 0.12141    0.26613159]\n",
      "  [0.26904999 0.26575    0.26579999 0.2665     0.55597    0.26548404]\n",
      "  [0.26860001 0.265      0.26604999 0.26575    0.33121    0.26473694]]\n",
      "\n",
      " [[0.26720001 0.264      0.26720001 0.266      0.14868    0.26498599]\n",
      "  [0.26889999 0.265      0.2665     0.26714999 0.12141    0.26613159]\n",
      "  [0.26904999 0.26575    0.26579999 0.2665     0.55597    0.26548404]\n",
      "  [0.26860001 0.265      0.26604999 0.26575    0.33121    0.26473694]\n",
      "  [0.268      0.26054999 0.268      0.26545001 0.41839    0.26443808]]\n",
      "\n",
      " [[0.26889999 0.265      0.2665     0.26714999 0.12141    0.26613159]\n",
      "  [0.26904999 0.26575    0.26579999 0.2665     0.55597    0.26548404]\n",
      "  [0.26860001 0.265      0.26604999 0.26575    0.33121    0.26473694]\n",
      "  [0.268      0.26054999 0.268      0.26545001 0.41839    0.26443808]\n",
      "  [0.272      0.268      0.2715     0.271      0.29736    0.26996695]]\n",
      "\n",
      " [[0.26904999 0.26575    0.26579999 0.2665     0.55597    0.26548404]\n",
      "  [0.26860001 0.265      0.26604999 0.26575    0.33121    0.26473694]\n",
      "  [0.268      0.26054999 0.268      0.26545001 0.41839    0.26443808]\n",
      "  [0.272      0.268      0.2715     0.271      0.29736    0.26996695]\n",
      "  [0.274      0.27       0.274      0.271      0.43966    0.26996695]]\n",
      "\n",
      " [[0.26860001 0.265      0.26604999 0.26575    0.33121    0.26473694]\n",
      "  [0.268      0.26054999 0.268      0.26545001 0.41839    0.26443808]\n",
      "  [0.272      0.268      0.2715     0.271      0.29736    0.26996695]\n",
      "  [0.274      0.27       0.274      0.271      0.43966    0.26996695]\n",
      "  [0.27629999 0.27139999 0.276      0.275      0.42158    0.27395166]]\n",
      "\n",
      " [[0.268      0.26054999 0.268      0.26545001 0.41839    0.26443808]\n",
      "  [0.272      0.268      0.2715     0.271      0.29736    0.26996695]\n",
      "  [0.274      0.27       0.274      0.271      0.43966    0.26996695]\n",
      "  [0.27629999 0.27139999 0.276      0.275      0.42158    0.27395166]\n",
      "  [0.28       0.273      0.27704999 0.276      0.45931    0.27494781]]\n",
      "\n",
      " [[0.272      0.268      0.2715     0.271      0.29736    0.26996695]\n",
      "  [0.274      0.27       0.274      0.271      0.43966    0.26996695]\n",
      "  [0.27629999 0.27139999 0.276      0.275      0.42158    0.27395166]\n",
      "  [0.28       0.273      0.27704999 0.276      0.45931    0.27494781]\n",
      "  [0.28       0.273      0.27845001 0.278      0.22846    0.27694022]]\n",
      "\n",
      " [[0.274      0.27       0.274      0.271      0.43966    0.26996695]\n",
      "  [0.27629999 0.27139999 0.276      0.275      0.42158    0.27395166]\n",
      "  [0.28       0.273      0.27704999 0.276      0.45931    0.27494781]\n",
      "  [0.28       0.273      0.27845001 0.278      0.22846    0.27694022]\n",
      "  [0.279      0.274      0.276      0.27679999 0.59641    0.27574478]]\n",
      "\n",
      " [[0.27629999 0.27139999 0.276      0.275      0.42158    0.27395166]\n",
      "  [0.28       0.273      0.27704999 0.276      0.45931    0.27494781]\n",
      "  [0.28       0.273      0.27845001 0.278      0.22846    0.27694022]\n",
      "  [0.279      0.274      0.276      0.27679999 0.59641    0.27574478]\n",
      "  [0.287      0.27504999 0.287      0.27675    0.59924    0.27569501]]\n",
      "\n",
      " [[0.28       0.273      0.27704999 0.276      0.45931    0.27494781]\n",
      "  [0.28       0.273      0.27845001 0.278      0.22846    0.27694022]\n",
      "  [0.279      0.274      0.276      0.27679999 0.59641    0.27574478]\n",
      "  [0.287      0.27504999 0.287      0.27675    0.59924    0.27569501]\n",
      "  [0.29239999 0.2875     0.29239999 0.28845001 0.1232     0.2873504 ]]\n",
      "\n",
      " [[0.28       0.273      0.27845001 0.278      0.22846    0.27694022]\n",
      "  [0.279      0.274      0.276      0.27679999 0.59641    0.27574478]\n",
      "  [0.287      0.27504999 0.287      0.27675    0.59924    0.27569501]\n",
      "  [0.29239999 0.2875     0.29239999 0.28845001 0.1232     0.2873504 ]\n",
      "  [0.297      0.292      0.296      0.29239999 0.19862    0.29128537]]\n",
      "\n",
      " [[0.279      0.274      0.276      0.27679999 0.59641    0.27574478]\n",
      "  [0.287      0.27504999 0.287      0.27675    0.59924    0.27569501]\n",
      "  [0.29239999 0.2875     0.29239999 0.28845001 0.1232     0.2873504 ]\n",
      "  [0.297      0.292      0.296      0.29239999 0.19862    0.29128537]\n",
      "  [0.298      0.291      0.293      0.295      0.73167    0.29387543]]\n",
      "\n",
      " [[0.287      0.27504999 0.287      0.27675    0.59924    0.27569501]\n",
      "  [0.29239999 0.2875     0.29239999 0.28845001 0.1232     0.2873504 ]\n",
      "  [0.297      0.292      0.296      0.29239999 0.19862    0.29128537]\n",
      "  [0.298      0.291      0.293      0.295      0.73167    0.29387543]\n",
      "  [0.292      0.28510001 0.2885     0.292      0.4943     0.29088684]]\n",
      "\n",
      " [[0.29239999 0.2875     0.29239999 0.28845001 0.1232     0.2873504 ]\n",
      "  [0.297      0.292      0.296      0.29239999 0.19862    0.29128537]\n",
      "  [0.298      0.291      0.293      0.295      0.73167    0.29387543]\n",
      "  [0.292      0.28510001 0.2885     0.292      0.4943     0.29088684]\n",
      "  [0.29395001 0.286      0.291      0.287      0.38158    0.28590594]]\n",
      "\n",
      " [[0.297      0.292      0.296      0.29239999 0.19862    0.29128537]\n",
      "  [0.298      0.291      0.293      0.295      0.73167    0.29387543]\n",
      "  [0.292      0.28510001 0.2885     0.292      0.4943     0.29088684]\n",
      "  [0.29395001 0.286      0.291      0.287      0.38158    0.28590594]\n",
      "  [0.2905     0.28639999 0.28639999 0.29       0.35837    0.2888945 ]]\n",
      "\n",
      " [[0.298      0.291      0.293      0.295      0.73167    0.29387543]\n",
      "  [0.292      0.28510001 0.2885     0.292      0.4943     0.29088684]\n",
      "  [0.29395001 0.286      0.291      0.287      0.38158    0.28590594]\n",
      "  [0.2905     0.28639999 0.28639999 0.29       0.35837    0.2888945 ]\n",
      "  [0.28895001 0.2855     0.28639999 0.286      0.22531    0.28490976]]\n",
      "\n",
      " [[0.292      0.28510001 0.2885     0.292      0.4943     0.29088684]\n",
      "  [0.29395001 0.286      0.291      0.287      0.38158    0.28590594]\n",
      "  [0.2905     0.28639999 0.28639999 0.29       0.35837    0.2888945 ]\n",
      "  [0.28895001 0.2855     0.28639999 0.286      0.22531    0.28490976]\n",
      "  [0.28895001 0.28       0.28320001 0.2865     0.23484    0.28540784]]\n",
      "\n",
      " [[0.29395001 0.286      0.291      0.287      0.38158    0.28590594]\n",
      "  [0.2905     0.28639999 0.28639999 0.29       0.35837    0.2888945 ]\n",
      "  [0.28895001 0.2855     0.28639999 0.286      0.22531    0.28490976]\n",
      "  [0.28895001 0.28       0.28320001 0.2865     0.23484    0.28540784]\n",
      "  [0.28460001 0.28       0.2845     0.283      0.15736    0.2819212 ]]\n",
      "\n",
      " [[0.2905     0.28639999 0.28639999 0.29       0.35837    0.2888945 ]\n",
      "  [0.28895001 0.2855     0.28639999 0.286      0.22531    0.28490976]\n",
      "  [0.28895001 0.28       0.28320001 0.2865     0.23484    0.28540784]\n",
      "  [0.28460001 0.28       0.2845     0.283      0.15736    0.2819212 ]\n",
      "  [0.2855     0.28325    0.284      0.285      0.11588    0.28391357]]\n",
      "\n",
      " [[0.28895001 0.2855     0.28639999 0.286      0.22531    0.28490976]\n",
      "  [0.28895001 0.28       0.28320001 0.2865     0.23484    0.28540784]\n",
      "  [0.28460001 0.28       0.2845     0.283      0.15736    0.2819212 ]\n",
      "  [0.2855     0.28325    0.284      0.285      0.11588    0.28391357]\n",
      "  [0.285      0.283      0.284      0.284      0.17606    0.28291736]]\n",
      "\n",
      " [[0.28895001 0.28       0.28320001 0.2865     0.23484    0.28540784]\n",
      "  [0.28460001 0.28       0.2845     0.283      0.15736    0.2819212 ]\n",
      "  [0.2855     0.28325    0.284      0.285      0.11588    0.28391357]\n",
      "  [0.285      0.283      0.284      0.284      0.17606    0.28291736]\n",
      "  [0.286      0.2805     0.28389999 0.28375    0.18845    0.28266833]]\n",
      "\n",
      " [[0.28460001 0.28       0.2845     0.283      0.15736    0.2819212 ]\n",
      "  [0.2855     0.28325    0.284      0.285      0.11588    0.28391357]\n",
      "  [0.285      0.283      0.284      0.284      0.17606    0.28291736]\n",
      "  [0.286      0.2805     0.28389999 0.28375    0.18845    0.28266833]\n",
      "  [0.286      0.277      0.281      0.28389999 0.32842    0.28281775]]\n",
      "\n",
      " [[0.2855     0.28325    0.284      0.285      0.11588    0.28391357]\n",
      "  [0.285      0.283      0.284      0.284      0.17606    0.28291736]\n",
      "  [0.286      0.2805     0.28389999 0.28375    0.18845    0.28266833]\n",
      "  [0.286      0.277      0.281      0.28389999 0.32842    0.28281775]\n",
      "  [0.2845     0.278      0.283      0.28       0.41967    0.27893262]]\n",
      "\n",
      " [[0.285      0.283      0.284      0.284      0.17606    0.28291736]\n",
      "  [0.286      0.2805     0.28389999 0.28375    0.18845    0.28266833]\n",
      "  [0.286      0.277      0.281      0.28389999 0.32842    0.28281775]\n",
      "  [0.2845     0.278      0.283      0.28       0.41967    0.27893262]\n",
      "  [0.28385001 0.27       0.28375    0.282      0.48905    0.28092502]]\n",
      "\n",
      " [[0.286      0.2805     0.28389999 0.28375    0.18845    0.28266833]\n",
      "  [0.286      0.277      0.281      0.28389999 0.32842    0.28281775]\n",
      "  [0.2845     0.278      0.283      0.28       0.41967    0.27893262]\n",
      "  [0.28385001 0.27       0.28375    0.282      0.48905    0.28092502]\n",
      "  [0.288      0.28360001 0.28795001 0.28429999 0.24804    0.28321622]]\n",
      "\n",
      " [[0.286      0.277      0.281      0.28389999 0.32842    0.28281775]\n",
      "  [0.2845     0.278      0.283      0.28       0.41967    0.27893262]\n",
      "  [0.28385001 0.27       0.28375    0.282      0.48905    0.28092502]\n",
      "  [0.288      0.28360001 0.28795001 0.28429999 0.24804    0.28321622]\n",
      "  [0.29020001 0.288      0.28864999 0.288      0.32706    0.28690213]]\n",
      "\n",
      " [[0.2845     0.278      0.283      0.28       0.41967    0.27893262]\n",
      "  [0.28385001 0.27       0.28375    0.282      0.48905    0.28092502]\n",
      "  [0.288      0.28360001 0.28795001 0.28429999 0.24804    0.28321622]\n",
      "  [0.29020001 0.288      0.28864999 0.288      0.32706    0.28690213]\n",
      "  [0.2895     0.28304999 0.28489999 0.2885     0.71984    0.28740021]]\n",
      "\n",
      " [[0.28385001 0.27       0.28375    0.282      0.48905    0.28092502]\n",
      "  [0.288      0.28360001 0.28795001 0.28429999 0.24804    0.28321622]\n",
      "  [0.29020001 0.288      0.28864999 0.288      0.32706    0.28690213]\n",
      "  [0.2895     0.28304999 0.28489999 0.2885     0.71984    0.28740021]\n",
      "  [0.289      0.283      0.284      0.285      0.4093     0.28391357]]]\n"
     ]
    }
   ],
   "source": [
    "trainScore = model.evaluate(X_train, y_train, verbose=1)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "print (X_test)\n",
    "\n",
    "#testScore = model.evaluate(X_test, y_test, verbose=1)\n",
    "#print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28385001 0.27       0.28375    0.282      0.48905    0.28092502]\n",
      " [0.288      0.28360001 0.28795001 0.28429999 0.24804    0.28321622]\n",
      " [0.29020001 0.288      0.28864999 0.288      0.32706    0.28690213]\n",
      " [0.2895     0.28304999 0.28489999 0.2885     0.71984    0.28740021]\n",
      " [0.289      0.283      0.284      0.285      0.4093     0.28391357]]\n",
      "pred [[0.26166242]\n",
      " [0.26224172]\n",
      " [0.2636206 ]\n",
      " [0.26556486]\n",
      " [0.26724932]\n",
      " [0.26841867]\n",
      " [0.2693055 ]\n",
      " [0.26999938]\n",
      " [0.2722715 ]\n",
      " [0.27528596]\n",
      " [0.27732602]\n",
      " [0.27953023]\n",
      " [0.28113073]\n",
      " [0.28281024]\n",
      " [0.28632692]\n",
      " [0.29100186]\n",
      " [0.29540527]\n",
      " [0.29688656]\n",
      " [0.29512566]\n",
      " [0.2913074 ]\n",
      " [0.2890576 ]\n",
      " [0.28692198]\n",
      " [0.2846614 ]\n",
      " [0.28345513]\n",
      " [0.2830875 ]\n",
      " [0.2840867 ]\n",
      " [0.2845821 ]\n",
      " [0.28475672]\n",
      " [0.2849548 ]\n",
      " [0.28608578]\n",
      " [0.28794205]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[-1])\n",
    "diff=[]\n",
    "ratio=[]\n",
    "p = model.predict(X_test)\n",
    "print (\"pred\",p)\n",
    "for u in range(len(y_test)):\n",
    "    pr = p[u][0]\n",
    "    ratio.append((y_test[u]/pr)-1)\n",
    "    diff.append(abs(y_test[u]- pr))\n",
    "    #print(u, y_test[u], pr, (y_test[u]/pr)-1, abs(y_test[u]- pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "plt2.plot((p * 1000 ) ,color='red', label='prediction')\n",
    "plt2.plot(y_test * 1000,color='blue', label='y_test')\n",
    "plt2.legend(loc='upper left')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_last(stock_name, seq_len, window ):\n",
    "    from pandas_datareader import data\n",
    "\n",
    "    # Only get the adjusted close.\n",
    "    df = data.DataReader(stock_name,\n",
    "                       start='2018-08-01',\n",
    "                       end='2018-09-03',\n",
    "                       data_source='yahoo')\n",
    "\n",
    "    \n",
    "\n",
    "    df = df[0:-1]\n",
    "    data_x = df\n",
    "    \n",
    "    #print(\"DATA Y 0 :\",data_y)\n",
    "    data_x = data_x.values / 1000\n",
    "    print(\"data_x input:\",data_x)\n",
    "    data_x = np.delete(data_x,np.s_[len(data_x[0])-1],axis=1)\n",
    "    \n",
    "    #print(\"DATA Y :\",data_y)\n",
    "    amount_of_features = len(data_x[0]) \n",
    "    \n",
    "    sequence_length = seq_len \n",
    "    result_x = []\n",
    "\n",
    "    for index in range(len(data_x) - sequence_length ):\n",
    "        result_x.append(data_x[index: index + sequence_length + 1])\n",
    "\n",
    "    x_train = np.array(result_x)\n",
    "    x_train = x_train[:, :-1] \n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features ))\n",
    "    \n",
    "    print (\"x_train:\",x_train)\n",
    "  \n",
    "    return [x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_x input: [[ 0.516       0.505       0.515       0.51359998 29.859       0.51359998]\n",
      " [ 0.523       0.513       0.515       0.52215002 29.058       0.52215002]\n",
      " [ 0.53884998  0.52504999  0.52504999  0.53370001 84.355       0.53370001]\n",
      " [ 0.553       0.53        0.53        0.54779999 47.823       0.54779999]\n",
      " [ 0.556       0.54415002  0.55        0.55315002 34.555       0.55315002]\n",
      " [ 0.60495001  0.55        0.553       0.60029999 58.823       0.60029999]\n",
      " [ 0.681       0.595       0.595       0.64995001 70.135       0.64995001]\n",
      " [ 0.652       0.61704999  0.64995001  0.62465002 54.83        0.62465002]\n",
      " [ 0.675       0.62        0.625       0.62184998 31.015       0.62184998]]\n",
      "x_train: [[[ 0.516       0.505       0.515       0.51359998 29.859     ]\n",
      "  [ 0.523       0.513       0.515       0.52215002 29.058     ]\n",
      "  [ 0.53884998  0.52504999  0.52504999  0.53370001 84.355     ]\n",
      "  [ 0.553       0.53        0.53        0.54779999 47.823     ]\n",
      "  [ 0.556       0.54415002  0.55        0.55315002 34.555     ]]\n",
      "\n",
      " [[ 0.523       0.513       0.515       0.52215002 29.058     ]\n",
      "  [ 0.53884998  0.52504999  0.52504999  0.53370001 84.355     ]\n",
      "  [ 0.553       0.53        0.53        0.54779999 47.823     ]\n",
      "  [ 0.556       0.54415002  0.55        0.55315002 34.555     ]\n",
      "  [ 0.60495001  0.55        0.553       0.60029999 58.823     ]]\n",
      "\n",
      " [[ 0.53884998  0.52504999  0.52504999  0.53370001 84.355     ]\n",
      "  [ 0.553       0.53        0.53        0.54779999 47.823     ]\n",
      "  [ 0.556       0.54415002  0.55        0.55315002 34.555     ]\n",
      "  [ 0.60495001  0.55        0.553       0.60029999 58.823     ]\n",
      "  [ 0.681       0.595       0.595       0.64995001 70.135     ]]\n",
      "\n",
      " [[ 0.553       0.53        0.53        0.54779999 47.823     ]\n",
      "  [ 0.556       0.54415002  0.55        0.55315002 34.555     ]\n",
      "  [ 0.60495001  0.55        0.553       0.60029999 58.823     ]\n",
      "  [ 0.681       0.595       0.595       0.64995001 70.135     ]\n",
      "  [ 0.652       0.61704999  0.64995001  0.62465002 54.83      ]]]\n",
      "[array([[[ 0.516     ,  0.505     ,  0.515     ,  0.51359998,\n",
      "         29.859     ],\n",
      "        [ 0.523     ,  0.513     ,  0.515     ,  0.52215002,\n",
      "         29.058     ],\n",
      "        [ 0.53884998,  0.52504999,  0.52504999,  0.53370001,\n",
      "         84.355     ],\n",
      "        [ 0.553     ,  0.53      ,  0.53      ,  0.54779999,\n",
      "         47.823     ],\n",
      "        [ 0.556     ,  0.54415002,  0.55      ,  0.55315002,\n",
      "         34.555     ]],\n",
      "\n",
      "       [[ 0.523     ,  0.513     ,  0.515     ,  0.52215002,\n",
      "         29.058     ],\n",
      "        [ 0.53884998,  0.52504999,  0.52504999,  0.53370001,\n",
      "         84.355     ],\n",
      "        [ 0.553     ,  0.53      ,  0.53      ,  0.54779999,\n",
      "         47.823     ],\n",
      "        [ 0.556     ,  0.54415002,  0.55      ,  0.55315002,\n",
      "         34.555     ],\n",
      "        [ 0.60495001,  0.55      ,  0.553     ,  0.60029999,\n",
      "         58.823     ]],\n",
      "\n",
      "       [[ 0.53884998,  0.52504999,  0.52504999,  0.53370001,\n",
      "         84.355     ],\n",
      "        [ 0.553     ,  0.53      ,  0.53      ,  0.54779999,\n",
      "         47.823     ],\n",
      "        [ 0.556     ,  0.54415002,  0.55      ,  0.55315002,\n",
      "         34.555     ],\n",
      "        [ 0.60495001,  0.55      ,  0.553     ,  0.60029999,\n",
      "         58.823     ],\n",
      "        [ 0.681     ,  0.595     ,  0.595     ,  0.64995001,\n",
      "         70.135     ]],\n",
      "\n",
      "       [[ 0.553     ,  0.53      ,  0.53      ,  0.54779999,\n",
      "         47.823     ],\n",
      "        [ 0.556     ,  0.54415002,  0.55      ,  0.55315002,\n",
      "         34.555     ],\n",
      "        [ 0.60495001,  0.55      ,  0.553     ,  0.60029999,\n",
      "         58.823     ],\n",
      "        [ 0.681     ,  0.595     ,  0.595     ,  0.64995001,\n",
      "         70.135     ],\n",
      "        [ 0.652     ,  0.61704999,  0.64995001,  0.62465002,\n",
      "         54.83      ]]])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_17_input to have shape (5, 6) but got array with shape (5, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-ce88ec0bc76e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luch/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                              'argument.')\n\u001b[1;32m   1146\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luch/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luch/.local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_17_input to have shape (5, 6) but got array with shape (5, 5)"
     ]
    }
   ],
   "source": [
    "df_new = load_data_last ('TS.BA', 5, 10 )\n",
    "\n",
    "window  =  5\n",
    "\n",
    "print(df_new)\n",
    "pred = model.predict(df_new) \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
