{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM"
    "from stockstats import StockDataFrame as Sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(stock_name,shift_window, normalized=0):\n",
    "    from pandas_datareader import data\n",
    "\n",
    "    # Only get the adjusted close.\n",
    "    df = data.DataReader(stock_name,\n",
    "                       start='2017-1-1',\n",
    "                       end='2018-08-20',\n",
    "                       data_source='yahoo')\n",
    "\n",
    "    df['Adj Close future'] = df['Adj Close'].shift(-shift_window)\n",
    "    #df['Difference'] = ( df['Adj Close'].shift(-shift_window) / df['Adj Close'] ) \n",
    "    #df['Difference'] = ( df['Difference'] -1 )\n",
    "    #print(df.head())\n",
    "    df['Volume'] /= 100\n",
    "    return df[:-shift_window]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = 'TRAN.BA'\n",
    "df = get_stock_data(stock_name,10,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 High        Low       Open      Close   Volume  Adj Close  \\\n",
      "Date                                                                         \n",
      "2018-07-31  53.000000  52.000000  52.000000  52.400002  8606.01  52.400002   \n",
      "2018-08-01  53.000000  51.500000  52.799999  52.200001  1028.04  52.200001   \n",
      "2018-08-02  53.000000  51.099998  52.099998  51.599998  1209.03  51.599998   \n",
      "2018-08-03  53.299999  51.599998  51.599998  53.150002  1855.86  53.150002   \n",
      "2018-08-06  53.500000  51.200001  53.299999  53.000000  2561.59  53.000000   \n",
      "\n",
      "            Adj Close future  \n",
      "Date                          \n",
      "2018-07-31         45.349998  \n",
      "2018-08-01         46.200001  \n",
      "2018-08-02         47.349998  \n",
      "2018-08-03         45.900002  \n",
      "2018-08-06         44.000000  \n"
     ]
    }
   ],
   "source": [
    "print(df.tail(5))\n",
    "df_val = df.values\n",
    "#df_val[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_NOT_USED(stock, seq_len):\n",
    "\n",
    "    data = stock\n",
    "    amount_of_features = len(data[::-1][0]) - 1\n",
    "    \n",
    "    \n",
    "    sequence_length = seq_len \n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length + 1])\n",
    "\n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0])\n",
    "    train = result[:int(row), :]\n",
    "    x_train = train[:, :-1]\n",
    "    y_train = train[:, -1][:,-1]\n",
    "    x_test = result[int(row):, :-1]\n",
    "    print (\"x_test:\",x_test)\n",
    "    y_test = result[int(row):, -1][:,-1]\n",
    "    print (\"y_test:\",y_test)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_v2(stock, seq_len):\n",
    "\n",
    "    data_x = stock\n",
    "    data_y = stock\n",
    "    #print(\"DATA Y 0 :\",data_y)\n",
    "    print (\"Amount of features TOT :\",len(data_x[0]) )\n",
    "    data_x = np.delete(data_x,np.s_[len(stock[0])-1],axis=1)\n",
    "    data_y = np.delete(data_y,np.s_[0:len(stock[0])-1],axis=1)\n",
    "    #print(\"DATA Y :\",data_y)\n",
    "    amount_of_features = len(data_x[0]) \n",
    "    \n",
    "    print (\"Amount of features found:\",amount_of_features)\n",
    "    \n",
    "    sequence_length = seq_len \n",
    "    result_x = []\n",
    "    result_y = []\n",
    "    for index in range(len(data_x) - sequence_length ):\n",
    "        result_x.append(data_x[index: index + sequence_length + 1])\n",
    "        result_y.append(data_y[index: index + sequence_length + 1])\n",
    "\n",
    "    result_x = np.array(result_x)\n",
    "    result_y = np.array(result_y)\n",
    "    \n",
    "    row = round(0.92 * result_x.shape[0])\n",
    "    train_x = result_x[:int(row), :]\n",
    "    train_y = result_y[:int(row), :]\n",
    "    \n",
    "    x_train = train_x[:, :-1]\n",
    "    x_test = result_x[int(row):, :-1]\n",
    "   \n",
    "    y_train = train_y[:, -1]\n",
    "    y_test = result_y[int(row):, -1]\n",
    "    \n",
    "    #print (\"x_test before:\",x_test)\n",
    "    #print (\"y_test before:\",y_test)\n",
    "    \n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "    print (\"x_test:\",x_test)\n",
    "    print (\"y_test:\",y_test)\n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window is 10 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features TOT : 7\n",
      "Amount of features found: 6\n",
      "x_test: [[[ 0.05995  0.05725  0.05925  0.05855  6.24781  0.05855]\n",
      "  [ 0.0607   0.05765  0.0587   0.0579   5.16477  0.0579 ]\n",
      "  [ 0.0585   0.05515  0.0575   0.05545  3.30152  0.05545]\n",
      "  ...\n",
      "  [ 0.0479   0.04545  0.0455   0.0471  13.49893  0.0471 ]\n",
      "  [ 0.05375  0.0501   0.051    0.05345 12.65525  0.05345]\n",
      "  [ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]]\n",
      "\n",
      " [[ 0.0607   0.05765  0.0587   0.0579   5.16477  0.0579 ]\n",
      "  [ 0.0585   0.05515  0.0575   0.05545  3.30152  0.05545]\n",
      "  [ 0.05525  0.0525   0.05525  0.05305  2.64494  0.05305]\n",
      "  ...\n",
      "  [ 0.05375  0.0501   0.051    0.05345 12.65525  0.05345]\n",
      "  [ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]\n",
      "  [ 0.052    0.049    0.051    0.05185  1.99467  0.05185]]\n",
      "\n",
      " [[ 0.0585   0.05515  0.0575   0.05545  3.30152  0.05545]\n",
      "  [ 0.05525  0.0525   0.05525  0.05305  2.64494  0.05305]\n",
      "  [ 0.05495  0.05     0.053    0.0509   4.80171  0.0509 ]\n",
      "  ...\n",
      "  [ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]\n",
      "  [ 0.052    0.049    0.051    0.05185  1.99467  0.05185]\n",
      "  [ 0.052    0.04975  0.052    0.05015  2.36211  0.05015]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.0448   0.04375  0.0447   0.04455  0.6867   0.04455]\n",
      "  [ 0.0458   0.04455  0.04455  0.04555  2.34566  0.04555]\n",
      "  [ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]\n",
      "  ...\n",
      "  [ 0.05335  0.0496   0.05175  0.05295  3.48324  0.05295]\n",
      "  [ 0.053    0.052    0.052    0.0524   8.60601  0.0524 ]\n",
      "  [ 0.053    0.0515   0.0528   0.0522   1.02804  0.0522 ]]\n",
      "\n",
      " [[ 0.0458   0.04455  0.04455  0.04555  2.34566  0.04555]\n",
      "  [ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]\n",
      "  [ 0.049    0.0477   0.04795  0.0487   8.7053   0.0487 ]\n",
      "  ...\n",
      "  [ 0.053    0.052    0.052    0.0524   8.60601  0.0524 ]\n",
      "  [ 0.053    0.0515   0.0528   0.0522   1.02804  0.0522 ]\n",
      "  [ 0.053    0.0511   0.0521   0.0516   1.20903  0.0516 ]]\n",
      "\n",
      " [[ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]\n",
      "  [ 0.049    0.0477   0.04795  0.0487   8.7053   0.0487 ]\n",
      "  [ 0.052    0.0485   0.0485   0.0519   2.06345  0.0519 ]\n",
      "  ...\n",
      "  [ 0.053    0.0515   0.0528   0.0522   1.02804  0.0522 ]\n",
      "  [ 0.053    0.0511   0.0521   0.0516   1.20903  0.0516 ]\n",
      "  [ 0.0533   0.0516   0.0516   0.05315  1.85586  0.05315]]]\n",
      "y_test: [[0.04855]\n",
      " [0.04655]\n",
      " [0.0442 ]\n",
      " [0.04415]\n",
      " [0.04315]\n",
      " [0.04515]\n",
      " [0.04485]\n",
      " [0.04455]\n",
      " [0.04555]\n",
      " [0.0467 ]\n",
      " [0.0487 ]\n",
      " [0.0519 ]\n",
      " [0.05035]\n",
      " [0.0509 ]\n",
      " [0.05295]\n",
      " [0.0524 ]\n",
      " [0.0522 ]\n",
      " [0.0516 ]\n",
      " [0.05315]\n",
      " [0.053  ]\n",
      " [0.05015]\n",
      " [0.04635]\n",
      " [0.0467 ]\n",
      " [0.0466 ]\n",
      " [0.0437 ]\n",
      " [0.04535]\n",
      " [0.0462 ]\n",
      " [0.04735]\n",
      " [0.0459 ]\n",
      " [0.044  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[[ 0.0164 ,  0.0147 ,  0.01515,  0.0162 ,  4.58286,  0.0162 ],\n",
       "         [ 0.01695,  0.0162 ,  0.0162 ,  0.01665,  7.30317,  0.01665],\n",
       "         [ 0.01715,  0.0165 ,  0.0166 ,  0.01705,  2.6815 ,  0.01705],\n",
       "         ...,\n",
       "         [ 0.0169 ,  0.016  ,  0.0169 ,  0.0163 , 10.65948,  0.0163 ],\n",
       "         [ 0.0163 ,  0.0155 ,  0.016  ,  0.016  ,  3.14829,  0.016  ],\n",
       "         [ 0.0174 ,  0.0157 ,  0.0157 ,  0.01735,  5.1975 ,  0.01735]],\n",
       " \n",
       "        [[ 0.01695,  0.0162 ,  0.0162 ,  0.01665,  7.30317,  0.01665],\n",
       "         [ 0.01715,  0.0165 ,  0.0166 ,  0.01705,  2.6815 ,  0.01705],\n",
       "         [ 0.0178 ,  0.0166 ,  0.01705,  0.01755,  4.63835,  0.01755],\n",
       "         ...,\n",
       "         [ 0.0163 ,  0.0155 ,  0.016  ,  0.016  ,  3.14829,  0.016  ],\n",
       "         [ 0.0174 ,  0.0157 ,  0.0157 ,  0.01735,  5.1975 ,  0.01735],\n",
       "         [ 0.01785,  0.0174 ,  0.0174 ,  0.0177 ,  2.53231,  0.0177 ]],\n",
       " \n",
       "        [[ 0.01715,  0.0165 ,  0.0166 ,  0.01705,  2.6815 ,  0.01705],\n",
       "         [ 0.0178 ,  0.0166 ,  0.01705,  0.01755,  4.63835,  0.01755],\n",
       "         [ 0.018  ,  0.0169 ,  0.018  ,  0.01745,  6.2795 ,  0.01745],\n",
       "         ...,\n",
       "         [ 0.0174 ,  0.0157 ,  0.0157 ,  0.01735,  5.1975 ,  0.01735],\n",
       "         [ 0.01785,  0.0174 ,  0.0174 ,  0.0177 ,  2.53231,  0.0177 ],\n",
       "         [ 0.01795,  0.0176 ,  0.01795,  0.0177 ,  4.36214,  0.0177 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.056  ,  0.051  ,  0.051  ,  0.05455,  5.87103,  0.05455],\n",
       "         [ 0.05725,  0.055  ,  0.05595,  0.0571 ,  3.83362,  0.0571 ],\n",
       "         [ 0.0575 ,  0.05475,  0.057  ,  0.05695,  2.76289,  0.05695],\n",
       "         ...,\n",
       "         [ 0.05495,  0.05   ,  0.053  ,  0.0509 ,  4.80171,  0.0509 ],\n",
       "         [ 0.0535 ,  0.0495 ,  0.052  ,  0.0521 ,  2.92069,  0.0521 ],\n",
       "         [ 0.0495 ,  0.0455 ,  0.0495 ,  0.0457 ,  9.65772,  0.0457 ]],\n",
       " \n",
       "        [[ 0.05725,  0.055  ,  0.05595,  0.0571 ,  3.83362,  0.0571 ],\n",
       "         [ 0.0575 ,  0.05475,  0.057  ,  0.05695,  2.76289,  0.05695],\n",
       "         [ 0.05995,  0.05725,  0.05925,  0.05855,  6.24781,  0.05855],\n",
       "         ...,\n",
       "         [ 0.0535 ,  0.0495 ,  0.052  ,  0.0521 ,  2.92069,  0.0521 ],\n",
       "         [ 0.0495 ,  0.0455 ,  0.0495 ,  0.0457 ,  9.65772,  0.0457 ],\n",
       "         [ 0.0479 ,  0.04545,  0.0455 ,  0.0471 , 13.49893,  0.0471 ]],\n",
       " \n",
       "        [[ 0.0575 ,  0.05475,  0.057  ,  0.05695,  2.76289,  0.05695],\n",
       "         [ 0.05995,  0.05725,  0.05925,  0.05855,  6.24781,  0.05855],\n",
       "         [ 0.0607 ,  0.05765,  0.0587 ,  0.0579 ,  5.16477,  0.0579 ],\n",
       "         ...,\n",
       "         [ 0.0495 ,  0.0455 ,  0.0495 ,  0.0457 ,  9.65772,  0.0457 ],\n",
       "         [ 0.0479 ,  0.04545,  0.0455 ,  0.0471 , 13.49893,  0.0471 ],\n",
       "         [ 0.05375,  0.0501 ,  0.051  ,  0.05345, 12.65525,  0.05345]]]),\n",
       " array([[0.0173 ],\n",
       "        [0.0186 ],\n",
       "        [0.0184 ],\n",
       "        [0.0181 ],\n",
       "        [0.01755],\n",
       "        [0.01725],\n",
       "        [0.0171 ],\n",
       "        [0.017  ],\n",
       "        [0.0172 ],\n",
       "        [0.018  ],\n",
       "        [0.01855],\n",
       "        [0.0185 ],\n",
       "        [0.01835],\n",
       "        [0.0188 ],\n",
       "        [0.01995],\n",
       "        [0.0209 ],\n",
       "        [0.02125],\n",
       "        [0.0221 ],\n",
       "        [0.0214 ],\n",
       "        [0.0209 ],\n",
       "        [0.0216 ],\n",
       "        [0.0211 ],\n",
       "        [0.021  ],\n",
       "        [0.02065],\n",
       "        [0.02065],\n",
       "        [0.021  ],\n",
       "        [0.0205 ],\n",
       "        [0.02015],\n",
       "        [0.0208 ],\n",
       "        [0.02095],\n",
       "        [0.0208 ],\n",
       "        [0.0207 ],\n",
       "        [0.0207 ],\n",
       "        [0.02145],\n",
       "        [0.0212 ],\n",
       "        [0.02165],\n",
       "        [0.0214 ],\n",
       "        [0.02135],\n",
       "        [0.0222 ],\n",
       "        [0.0233 ],\n",
       "        [0.0243 ],\n",
       "        [0.02365],\n",
       "        [0.0234 ],\n",
       "        [0.02315],\n",
       "        [0.02375],\n",
       "        [0.02405],\n",
       "        [0.02365],\n",
       "        [0.0241 ],\n",
       "        [0.0243 ],\n",
       "        [0.0247 ],\n",
       "        [0.02425],\n",
       "        [0.02415],\n",
       "        [0.02405],\n",
       "        [0.0241 ],\n",
       "        [0.024  ],\n",
       "        [0.0255 ],\n",
       "        [0.0262 ],\n",
       "        [0.0256 ],\n",
       "        [0.0255 ],\n",
       "        [0.0255 ],\n",
       "        [0.0254 ],\n",
       "        [0.0248 ],\n",
       "        [0.0244 ],\n",
       "        [0.024  ],\n",
       "        [0.0238 ],\n",
       "        [0.0238 ],\n",
       "        [0.0245 ],\n",
       "        [0.0251 ],\n",
       "        [0.02525],\n",
       "        [0.0257 ],\n",
       "        [0.02595],\n",
       "        [0.0261 ],\n",
       "        [0.0262 ],\n",
       "        [0.02675],\n",
       "        [0.026  ],\n",
       "        [0.0259 ],\n",
       "        [0.0259 ],\n",
       "        [0.0265 ],\n",
       "        [0.0266 ],\n",
       "        [0.0273 ],\n",
       "        [0.0276 ],\n",
       "        [0.02775],\n",
       "        [0.0284 ],\n",
       "        [0.0291 ],\n",
       "        [0.02835],\n",
       "        [0.0287 ],\n",
       "        [0.02845],\n",
       "        [0.0277 ],\n",
       "        [0.026  ],\n",
       "        [0.0268 ],\n",
       "        [0.02685],\n",
       "        [0.027  ],\n",
       "        [0.0282 ],\n",
       "        [0.02895],\n",
       "        [0.02715],\n",
       "        [0.0283 ],\n",
       "        [0.0283 ],\n",
       "        [0.02855],\n",
       "        [0.02835],\n",
       "        [0.0284 ],\n",
       "        [0.02855],\n",
       "        [0.02865],\n",
       "        [0.0292 ],\n",
       "        [0.03045],\n",
       "        [0.0307 ],\n",
       "        [0.03   ],\n",
       "        [0.0299 ],\n",
       "        [0.03   ],\n",
       "        [0.0295 ],\n",
       "        [0.02905],\n",
       "        [0.0283 ],\n",
       "        [0.0276 ],\n",
       "        [0.0265 ],\n",
       "        [0.0262 ],\n",
       "        [0.0271 ],\n",
       "        [0.02745],\n",
       "        [0.0267 ],\n",
       "        [0.0254 ],\n",
       "        [0.02475],\n",
       "        [0.024  ],\n",
       "        [0.0246 ],\n",
       "        [0.02595],\n",
       "        [0.0261 ],\n",
       "        [0.0257 ],\n",
       "        [0.0257 ],\n",
       "        [0.0258 ],\n",
       "        [0.0255 ],\n",
       "        [0.0246 ],\n",
       "        [0.0247 ],\n",
       "        [0.02445],\n",
       "        [0.02585],\n",
       "        [0.0267 ],\n",
       "        [0.0306 ],\n",
       "        [0.03225],\n",
       "        [0.03315],\n",
       "        [0.0322 ],\n",
       "        [0.0325 ],\n",
       "        [0.0327 ],\n",
       "        [0.0335 ],\n",
       "        [0.0338 ],\n",
       "        [0.0349 ],\n",
       "        [0.0365 ],\n",
       "        [0.0373 ],\n",
       "        [0.03615],\n",
       "        [0.0365 ],\n",
       "        [0.0365 ],\n",
       "        [0.0368 ],\n",
       "        [0.0368 ],\n",
       "        [0.0368 ],\n",
       "        [0.0368 ],\n",
       "        [0.0368 ],\n",
       "        [0.0368 ],\n",
       "        [0.034  ],\n",
       "        [0.0359 ],\n",
       "        [0.0359 ],\n",
       "        [0.0368 ],\n",
       "        [0.0377 ],\n",
       "        [0.0399 ],\n",
       "        [0.03915],\n",
       "        [0.0405 ],\n",
       "        [0.04125],\n",
       "        [0.0406 ],\n",
       "        [0.03955],\n",
       "        [0.04075],\n",
       "        [0.0409 ],\n",
       "        [0.04085],\n",
       "        [0.0415 ],\n",
       "        [0.0414 ],\n",
       "        [0.04125],\n",
       "        [0.04285],\n",
       "        [0.0429 ],\n",
       "        [0.04205],\n",
       "        [0.0408 ],\n",
       "        [0.04005],\n",
       "        [0.0393 ],\n",
       "        [0.0384 ],\n",
       "        [0.03795],\n",
       "        [0.04   ],\n",
       "        [0.0409 ],\n",
       "        [0.0425 ],\n",
       "        [0.0412 ],\n",
       "        [0.0417 ],\n",
       "        [0.041  ],\n",
       "        [0.0398 ],\n",
       "        [0.03985],\n",
       "        [0.0417 ],\n",
       "        [0.04255],\n",
       "        [0.04165],\n",
       "        [0.04025],\n",
       "        [0.04   ],\n",
       "        [0.04295],\n",
       "        [0.0441 ],\n",
       "        [0.04305],\n",
       "        [0.0421 ],\n",
       "        [0.04135],\n",
       "        [0.04195],\n",
       "        [0.04435],\n",
       "        [0.04385],\n",
       "        [0.0455 ],\n",
       "        [0.045  ],\n",
       "        [0.04475],\n",
       "        [0.046  ],\n",
       "        [0.04385],\n",
       "        [0.0443 ],\n",
       "        [0.04485],\n",
       "        [0.04585],\n",
       "        [0.0456 ],\n",
       "        [0.044  ],\n",
       "        [0.0433 ],\n",
       "        [0.0448 ],\n",
       "        [0.04495],\n",
       "        [0.04425],\n",
       "        [0.04275],\n",
       "        [0.04195],\n",
       "        [0.04155],\n",
       "        [0.04235],\n",
       "        [0.0429 ],\n",
       "        [0.0435 ],\n",
       "        [0.0451 ],\n",
       "        [0.0479 ],\n",
       "        [0.04695],\n",
       "        [0.04705],\n",
       "        [0.0477 ],\n",
       "        [0.04895],\n",
       "        [0.05185],\n",
       "        [0.0539 ],\n",
       "        [0.05485],\n",
       "        [0.0579 ],\n",
       "        [0.0604 ],\n",
       "        [0.061  ],\n",
       "        [0.05965],\n",
       "        [0.05975],\n",
       "        [0.05985],\n",
       "        [0.0598 ],\n",
       "        [0.0599 ],\n",
       "        [0.0604 ],\n",
       "        [0.061  ],\n",
       "        [0.06   ],\n",
       "        [0.0599 ],\n",
       "        [0.05995],\n",
       "        [0.06355],\n",
       "        [0.06325],\n",
       "        [0.06365],\n",
       "        [0.06395],\n",
       "        [0.06355],\n",
       "        [0.0644 ],\n",
       "        [0.06505],\n",
       "        [0.06115],\n",
       "        [0.0561 ],\n",
       "        [0.0549 ],\n",
       "        [0.05785],\n",
       "        [0.05545],\n",
       "        [0.05175],\n",
       "        [0.05445],\n",
       "        [0.05505],\n",
       "        [0.0571 ],\n",
       "        [0.05865],\n",
       "        [0.05805],\n",
       "        [0.0574 ],\n",
       "        [0.05645],\n",
       "        [0.0556 ],\n",
       "        [0.05445],\n",
       "        [0.0528 ],\n",
       "        [0.05345],\n",
       "        [0.0513 ],\n",
       "        [0.05655],\n",
       "        [0.05875],\n",
       "        [0.05865],\n",
       "        [0.0596 ],\n",
       "        [0.05945],\n",
       "        [0.0613 ],\n",
       "        [0.0623 ],\n",
       "        [0.0615 ],\n",
       "        [0.06145],\n",
       "        [0.0614 ],\n",
       "        [0.06115],\n",
       "        [0.05965],\n",
       "        [0.0586 ],\n",
       "        [0.0597 ],\n",
       "        [0.05815],\n",
       "        [0.0567 ],\n",
       "        [0.0574 ],\n",
       "        [0.05655],\n",
       "        [0.0567 ],\n",
       "        [0.0579 ],\n",
       "        [0.0576 ],\n",
       "        [0.0582 ],\n",
       "        [0.0579 ],\n",
       "        [0.0601 ],\n",
       "        [0.062  ],\n",
       "        [0.0617 ],\n",
       "        [0.06145],\n",
       "        [0.0613 ],\n",
       "        [0.06015],\n",
       "        [0.0597 ],\n",
       "        [0.0596 ],\n",
       "        [0.0579 ],\n",
       "        [0.0575 ],\n",
       "        [0.05625],\n",
       "        [0.054  ],\n",
       "        [0.05265],\n",
       "        [0.0499 ],\n",
       "        [0.05265],\n",
       "        [0.0512 ],\n",
       "        [0.04925],\n",
       "        [0.04895],\n",
       "        [0.0464 ],\n",
       "        [0.04225],\n",
       "        [0.0469 ],\n",
       "        [0.05   ],\n",
       "        [0.05085],\n",
       "        [0.05375],\n",
       "        [0.0565 ],\n",
       "        [0.0592 ],\n",
       "        [0.05955],\n",
       "        [0.05765],\n",
       "        [0.0563 ],\n",
       "        [0.05465],\n",
       "        [0.0535 ],\n",
       "        [0.05295],\n",
       "        [0.0523 ],\n",
       "        [0.0504 ],\n",
       "        [0.05045],\n",
       "        [0.05055],\n",
       "        [0.0506 ],\n",
       "        [0.05095],\n",
       "        [0.05455],\n",
       "        [0.0571 ],\n",
       "        [0.05695],\n",
       "        [0.05855],\n",
       "        [0.0579 ],\n",
       "        [0.05545],\n",
       "        [0.05305],\n",
       "        [0.0509 ],\n",
       "        [0.0521 ],\n",
       "        [0.0457 ],\n",
       "        [0.0471 ],\n",
       "        [0.05345],\n",
       "        [0.0522 ],\n",
       "        [0.05185],\n",
       "        [0.05015],\n",
       "        [0.0445 ],\n",
       "        [0.045  ],\n",
       "        [0.0409 ],\n",
       "        [0.03845],\n",
       "        [0.04335],\n",
       "        [0.04585],\n",
       "        [0.0469 ],\n",
       "        [0.0469 ]]),\n",
       " array([[[ 0.05995,  0.05725,  0.05925,  0.05855,  6.24781,  0.05855],\n",
       "         [ 0.0607 ,  0.05765,  0.0587 ,  0.0579 ,  5.16477,  0.0579 ],\n",
       "         [ 0.0585 ,  0.05515,  0.0575 ,  0.05545,  3.30152,  0.05545],\n",
       "         ...,\n",
       "         [ 0.0479 ,  0.04545,  0.0455 ,  0.0471 , 13.49893,  0.0471 ],\n",
       "         [ 0.05375,  0.0501 ,  0.051  ,  0.05345, 12.65525,  0.05345],\n",
       "         [ 0.054  ,  0.05175,  0.054  ,  0.0522 ,  1.97621,  0.0522 ]],\n",
       " \n",
       "        [[ 0.0607 ,  0.05765,  0.0587 ,  0.0579 ,  5.16477,  0.0579 ],\n",
       "         [ 0.0585 ,  0.05515,  0.0575 ,  0.05545,  3.30152,  0.05545],\n",
       "         [ 0.05525,  0.0525 ,  0.05525,  0.05305,  2.64494,  0.05305],\n",
       "         ...,\n",
       "         [ 0.05375,  0.0501 ,  0.051  ,  0.05345, 12.65525,  0.05345],\n",
       "         [ 0.054  ,  0.05175,  0.054  ,  0.0522 ,  1.97621,  0.0522 ],\n",
       "         [ 0.052  ,  0.049  ,  0.051  ,  0.05185,  1.99467,  0.05185]],\n",
       " \n",
       "        [[ 0.0585 ,  0.05515,  0.0575 ,  0.05545,  3.30152,  0.05545],\n",
       "         [ 0.05525,  0.0525 ,  0.05525,  0.05305,  2.64494,  0.05305],\n",
       "         [ 0.05495,  0.05   ,  0.053  ,  0.0509 ,  4.80171,  0.0509 ],\n",
       "         ...,\n",
       "         [ 0.054  ,  0.05175,  0.054  ,  0.0522 ,  1.97621,  0.0522 ],\n",
       "         [ 0.052  ,  0.049  ,  0.051  ,  0.05185,  1.99467,  0.05185],\n",
       "         [ 0.052  ,  0.04975,  0.052  ,  0.05015,  2.36211,  0.05015]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.0448 ,  0.04375,  0.0447 ,  0.04455,  0.6867 ,  0.04455],\n",
       "         [ 0.0458 ,  0.04455,  0.04455,  0.04555,  2.34566,  0.04555],\n",
       "         [ 0.0475 ,  0.045  ,  0.045  ,  0.0467 ,  4.94062,  0.0467 ],\n",
       "         ...,\n",
       "         [ 0.05335,  0.0496 ,  0.05175,  0.05295,  3.48324,  0.05295],\n",
       "         [ 0.053  ,  0.052  ,  0.052  ,  0.0524 ,  8.60601,  0.0524 ],\n",
       "         [ 0.053  ,  0.0515 ,  0.0528 ,  0.0522 ,  1.02804,  0.0522 ]],\n",
       " \n",
       "        [[ 0.0458 ,  0.04455,  0.04455,  0.04555,  2.34566,  0.04555],\n",
       "         [ 0.0475 ,  0.045  ,  0.045  ,  0.0467 ,  4.94062,  0.0467 ],\n",
       "         [ 0.049  ,  0.0477 ,  0.04795,  0.0487 ,  8.7053 ,  0.0487 ],\n",
       "         ...,\n",
       "         [ 0.053  ,  0.052  ,  0.052  ,  0.0524 ,  8.60601,  0.0524 ],\n",
       "         [ 0.053  ,  0.0515 ,  0.0528 ,  0.0522 ,  1.02804,  0.0522 ],\n",
       "         [ 0.053  ,  0.0511 ,  0.0521 ,  0.0516 ,  1.20903,  0.0516 ]],\n",
       " \n",
       "        [[ 0.0475 ,  0.045  ,  0.045  ,  0.0467 ,  4.94062,  0.0467 ],\n",
       "         [ 0.049  ,  0.0477 ,  0.04795,  0.0487 ,  8.7053 ,  0.0487 ],\n",
       "         [ 0.052  ,  0.0485 ,  0.0485 ,  0.0519 ,  2.06345,  0.0519 ],\n",
       "         ...,\n",
       "         [ 0.053  ,  0.0515 ,  0.0528 ,  0.0522 ,  1.02804,  0.0522 ],\n",
       "         [ 0.053  ,  0.0511 ,  0.0521 ,  0.0516 ,  1.20903,  0.0516 ],\n",
       "         [ 0.0533 ,  0.0516 ,  0.0516 ,  0.05315,  1.85586,  0.05315]]]),\n",
       " array([[0.04855],\n",
       "        [0.04655],\n",
       "        [0.0442 ],\n",
       "        [0.04415],\n",
       "        [0.04315],\n",
       "        [0.04515],\n",
       "        [0.04485],\n",
       "        [0.04455],\n",
       "        [0.04555],\n",
       "        [0.0467 ],\n",
       "        [0.0487 ],\n",
       "        [0.0519 ],\n",
       "        [0.05035],\n",
       "        [0.0509 ],\n",
       "        [0.05295],\n",
       "        [0.0524 ],\n",
       "        [0.0522 ],\n",
       "        [0.0516 ],\n",
       "        [0.05315],\n",
       "        [0.053  ],\n",
       "        [0.05015],\n",
       "        [0.04635],\n",
       "        [0.0467 ],\n",
       "        [0.0466 ],\n",
       "        [0.0437 ],\n",
       "        [0.04535],\n",
       "        [0.0462 ],\n",
       "        [0.04735],\n",
       "        [0.0459 ],\n",
       "        [0.044  ]])]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data_v2(df_val, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        input_dim=layers[0],\n",
    "        output_dim=layers[1],\n",
    "        return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "        layers[2],\n",
    "        return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "        output_dim=layers[2]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\",metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "def build_model2(layers):\n",
    "        d = 0.2\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "        model.add(Dropout(d))\n",
    "        model.add(LSTM(64, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "        model.add(Dropout(d))\n",
    "        model.add(Dense(16,kernel_initializer='uniform',activation='relu'))        \n",
    "        model.add(Dense(1,kernel_initializer='uniform',activation='relu'))\n",
    "        model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features TOT : 7\n",
      "Amount of features found: 6\n",
      "x_test: [[[ 0.05495  0.05     0.053    0.0509   4.80171  0.0509 ]\n",
      "  [ 0.0535   0.0495   0.052    0.0521   2.92069  0.0521 ]\n",
      "  [ 0.0495   0.0455   0.0495   0.0457   9.65772  0.0457 ]\n",
      "  [ 0.0479   0.04545  0.0455   0.0471  13.49893  0.0471 ]\n",
      "  [ 0.05375  0.0501   0.051    0.05345 12.65525  0.05345]]\n",
      "\n",
      " [[ 0.0535   0.0495   0.052    0.0521   2.92069  0.0521 ]\n",
      "  [ 0.0495   0.0455   0.0495   0.0457   9.65772  0.0457 ]\n",
      "  [ 0.0479   0.04545  0.0455   0.0471  13.49893  0.0471 ]\n",
      "  [ 0.05375  0.0501   0.051    0.05345 12.65525  0.05345]\n",
      "  [ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]]\n",
      "\n",
      " [[ 0.0495   0.0455   0.0495   0.0457   9.65772  0.0457 ]\n",
      "  [ 0.0479   0.04545  0.0455   0.0471  13.49893  0.0471 ]\n",
      "  [ 0.05375  0.0501   0.051    0.05345 12.65525  0.05345]\n",
      "  [ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]\n",
      "  [ 0.052    0.049    0.051    0.05185  1.99467  0.05185]]\n",
      "\n",
      " [[ 0.0479   0.04545  0.0455   0.0471  13.49893  0.0471 ]\n",
      "  [ 0.05375  0.0501   0.051    0.05345 12.65525  0.05345]\n",
      "  [ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]\n",
      "  [ 0.052    0.049    0.051    0.05185  1.99467  0.05185]\n",
      "  [ 0.052    0.04975  0.052    0.05015  2.36211  0.05015]]\n",
      "\n",
      " [[ 0.05375  0.0501   0.051    0.05345 12.65525  0.05345]\n",
      "  [ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]\n",
      "  [ 0.052    0.049    0.051    0.05185  1.99467  0.05185]\n",
      "  [ 0.052    0.04975  0.052    0.05015  2.36211  0.05015]\n",
      "  [ 0.05     0.043    0.05     0.0445   7.65132  0.0445 ]]\n",
      "\n",
      " [[ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]\n",
      "  [ 0.052    0.049    0.051    0.05185  1.99467  0.05185]\n",
      "  [ 0.052    0.04975  0.052    0.05015  2.36211  0.05015]\n",
      "  [ 0.05     0.043    0.05     0.0445   7.65132  0.0445 ]\n",
      "  [ 0.046    0.04125  0.046    0.045   10.98163  0.045  ]]\n",
      "\n",
      " [[ 0.052    0.049    0.051    0.05185  1.99467  0.05185]\n",
      "  [ 0.052    0.04975  0.052    0.05015  2.36211  0.05015]\n",
      "  [ 0.05     0.043    0.05     0.0445   7.65132  0.0445 ]\n",
      "  [ 0.046    0.04125  0.046    0.045   10.98163  0.045  ]\n",
      "  [ 0.045    0.04     0.045    0.0409   8.67518  0.0409 ]]\n",
      "\n",
      " [[ 0.052    0.04975  0.052    0.05015  2.36211  0.05015]\n",
      "  [ 0.05     0.043    0.05     0.0445   7.65132  0.0445 ]\n",
      "  [ 0.046    0.04125  0.046    0.045   10.98163  0.045  ]\n",
      "  [ 0.045    0.04     0.045    0.0409   8.67518  0.0409 ]\n",
      "  [ 0.0409   0.0367   0.0409   0.03845 13.00883  0.03845]]\n",
      "\n",
      " [[ 0.05     0.043    0.05     0.0445   7.65132  0.0445 ]\n",
      "  [ 0.046    0.04125  0.046    0.045   10.98163  0.045  ]\n",
      "  [ 0.045    0.04     0.045    0.0409   8.67518  0.0409 ]\n",
      "  [ 0.0409   0.0367   0.0409   0.03845 13.00883  0.03845]\n",
      "  [ 0.044    0.0395   0.0395   0.04335 11.21623  0.04335]]\n",
      "\n",
      " [[ 0.046    0.04125  0.046    0.045   10.98163  0.045  ]\n",
      "  [ 0.045    0.04     0.045    0.0409   8.67518  0.0409 ]\n",
      "  [ 0.0409   0.0367   0.0409   0.03845 13.00883  0.03845]\n",
      "  [ 0.044    0.0395   0.0395   0.04335 11.21623  0.04335]\n",
      "  [ 0.0463   0.04275  0.044    0.04585  3.61627  0.04585]]\n",
      "\n",
      " [[ 0.045    0.04     0.045    0.0409   8.67518  0.0409 ]\n",
      "  [ 0.0409   0.0367   0.0409   0.03845 13.00883  0.03845]\n",
      "  [ 0.044    0.0395   0.0395   0.04335 11.21623  0.04335]\n",
      "  [ 0.0463   0.04275  0.044    0.04585  3.61627  0.04585]\n",
      "  [ 0.048    0.0444   0.0459   0.0469   3.15284  0.0469 ]]\n",
      "\n",
      " [[ 0.0409   0.0367   0.0409   0.03845 13.00883  0.03845]\n",
      "  [ 0.044    0.0395   0.0395   0.04335 11.21623  0.04335]\n",
      "  [ 0.0463   0.04275  0.044    0.04585  3.61627  0.04585]\n",
      "  [ 0.048    0.0444   0.0459   0.0469   3.15284  0.0469 ]\n",
      "  [ 0.0474   0.04405  0.0465   0.0469   4.35053  0.0469 ]]\n",
      "\n",
      " [[ 0.044    0.0395   0.0395   0.04335 11.21623  0.04335]\n",
      "  [ 0.0463   0.04275  0.044    0.04585  3.61627  0.04585]\n",
      "  [ 0.048    0.0444   0.0459   0.0469   3.15284  0.0469 ]\n",
      "  [ 0.0474   0.04405  0.0465   0.0469   4.35053  0.0469 ]\n",
      "  [ 0.04905  0.0475   0.048    0.04855  4.0111   0.04855]]\n",
      "\n",
      " [[ 0.0463   0.04275  0.044    0.04585  3.61627  0.04585]\n",
      "  [ 0.048    0.0444   0.0459   0.0469   3.15284  0.0469 ]\n",
      "  [ 0.0474   0.04405  0.0465   0.0469   4.35053  0.0469 ]\n",
      "  [ 0.04905  0.0475   0.048    0.04855  4.0111   0.04855]\n",
      "  [ 0.0486   0.0453   0.0486   0.04655  2.14289  0.04655]]\n",
      "\n",
      " [[ 0.048    0.0444   0.0459   0.0469   3.15284  0.0469 ]\n",
      "  [ 0.0474   0.04405  0.0465   0.0469   4.35053  0.0469 ]\n",
      "  [ 0.04905  0.0475   0.048    0.04855  4.0111   0.04855]\n",
      "  [ 0.0486   0.0453   0.0486   0.04655  2.14289  0.04655]\n",
      "  [ 0.048    0.0435   0.0465   0.0442   2.66491  0.0442 ]]\n",
      "\n",
      " [[ 0.0474   0.04405  0.0465   0.0469   4.35053  0.0469 ]\n",
      "  [ 0.04905  0.0475   0.048    0.04855  4.0111   0.04855]\n",
      "  [ 0.0486   0.0453   0.0486   0.04655  2.14289  0.04655]\n",
      "  [ 0.048    0.0435   0.0465   0.0442   2.66491  0.0442 ]\n",
      "  [ 0.0447   0.04225  0.0442   0.04415  3.42594  0.04415]]\n",
      "\n",
      " [[ 0.04905  0.0475   0.048    0.04855  4.0111   0.04855]\n",
      "  [ 0.0486   0.0453   0.0486   0.04655  2.14289  0.04655]\n",
      "  [ 0.048    0.0435   0.0465   0.0442   2.66491  0.0442 ]\n",
      "  [ 0.0447   0.04225  0.0442   0.04415  3.42594  0.04415]\n",
      "  [ 0.04415  0.04225  0.04415  0.04315  2.15404  0.04315]]\n",
      "\n",
      " [[ 0.0486   0.0453   0.0486   0.04655  2.14289  0.04655]\n",
      "  [ 0.048    0.0435   0.0465   0.0442   2.66491  0.0442 ]\n",
      "  [ 0.0447   0.04225  0.0442   0.04415  3.42594  0.04415]\n",
      "  [ 0.04415  0.04225  0.04415  0.04315  2.15404  0.04315]\n",
      "  [ 0.04555  0.04225  0.04385  0.04515  2.53208  0.04515]]\n",
      "\n",
      " [[ 0.048    0.0435   0.0465   0.0442   2.66491  0.0442 ]\n",
      "  [ 0.0447   0.04225  0.0442   0.04415  3.42594  0.04415]\n",
      "  [ 0.04415  0.04225  0.04415  0.04315  2.15404  0.04315]\n",
      "  [ 0.04555  0.04225  0.04385  0.04515  2.53208  0.04515]\n",
      "  [ 0.0455   0.04415  0.0451   0.04485  1.48535  0.04485]]\n",
      "\n",
      " [[ 0.0447   0.04225  0.0442   0.04415  3.42594  0.04415]\n",
      "  [ 0.04415  0.04225  0.04415  0.04315  2.15404  0.04315]\n",
      "  [ 0.04555  0.04225  0.04385  0.04515  2.53208  0.04515]\n",
      "  [ 0.0455   0.04415  0.0451   0.04485  1.48535  0.04485]\n",
      "  [ 0.0448   0.04375  0.0447   0.04455  0.6867   0.04455]]\n",
      "\n",
      " [[ 0.04415  0.04225  0.04415  0.04315  2.15404  0.04315]\n",
      "  [ 0.04555  0.04225  0.04385  0.04515  2.53208  0.04515]\n",
      "  [ 0.0455   0.04415  0.0451   0.04485  1.48535  0.04485]\n",
      "  [ 0.0448   0.04375  0.0447   0.04455  0.6867   0.04455]\n",
      "  [ 0.0458   0.04455  0.04455  0.04555  2.34566  0.04555]]\n",
      "\n",
      " [[ 0.04555  0.04225  0.04385  0.04515  2.53208  0.04515]\n",
      "  [ 0.0455   0.04415  0.0451   0.04485  1.48535  0.04485]\n",
      "  [ 0.0448   0.04375  0.0447   0.04455  0.6867   0.04455]\n",
      "  [ 0.0458   0.04455  0.04455  0.04555  2.34566  0.04555]\n",
      "  [ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]]\n",
      "\n",
      " [[ 0.0455   0.04415  0.0451   0.04485  1.48535  0.04485]\n",
      "  [ 0.0448   0.04375  0.0447   0.04455  0.6867   0.04455]\n",
      "  [ 0.0458   0.04455  0.04455  0.04555  2.34566  0.04555]\n",
      "  [ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]\n",
      "  [ 0.049    0.0477   0.04795  0.0487   8.7053   0.0487 ]]\n",
      "\n",
      " [[ 0.0448   0.04375  0.0447   0.04455  0.6867   0.04455]\n",
      "  [ 0.0458   0.04455  0.04455  0.04555  2.34566  0.04555]\n",
      "  [ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]\n",
      "  [ 0.049    0.0477   0.04795  0.0487   8.7053   0.0487 ]\n",
      "  [ 0.052    0.0485   0.0485   0.0519   2.06345  0.0519 ]]\n",
      "\n",
      " [[ 0.0458   0.04455  0.04455  0.04555  2.34566  0.04555]\n",
      "  [ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]\n",
      "  [ 0.049    0.0477   0.04795  0.0487   8.7053   0.0487 ]\n",
      "  [ 0.052    0.0485   0.0485   0.0519   2.06345  0.0519 ]\n",
      "  [ 0.0525   0.04985  0.052    0.05035  1.43449  0.05035]]\n",
      "\n",
      " [[ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]\n",
      "  [ 0.049    0.0477   0.04795  0.0487   8.7053   0.0487 ]\n",
      "  [ 0.052    0.0485   0.0485   0.0519   2.06345  0.0519 ]\n",
      "  [ 0.0525   0.04985  0.052    0.05035  1.43449  0.05035]\n",
      "  [ 0.0515   0.0493   0.05075  0.0509   3.4663   0.0509 ]]\n",
      "\n",
      " [[ 0.049    0.0477   0.04795  0.0487   8.7053   0.0487 ]\n",
      "  [ 0.052    0.0485   0.0485   0.0519   2.06345  0.0519 ]\n",
      "  [ 0.0525   0.04985  0.052    0.05035  1.43449  0.05035]\n",
      "  [ 0.0515   0.0493   0.05075  0.0509   3.4663   0.0509 ]\n",
      "  [ 0.05335  0.0496   0.05175  0.05295  3.48324  0.05295]]\n",
      "\n",
      " [[ 0.052    0.0485   0.0485   0.0519   2.06345  0.0519 ]\n",
      "  [ 0.0525   0.04985  0.052    0.05035  1.43449  0.05035]\n",
      "  [ 0.0515   0.0493   0.05075  0.0509   3.4663   0.0509 ]\n",
      "  [ 0.05335  0.0496   0.05175  0.05295  3.48324  0.05295]\n",
      "  [ 0.053    0.052    0.052    0.0524   8.60601  0.0524 ]]\n",
      "\n",
      " [[ 0.0525   0.04985  0.052    0.05035  1.43449  0.05035]\n",
      "  [ 0.0515   0.0493   0.05075  0.0509   3.4663   0.0509 ]\n",
      "  [ 0.05335  0.0496   0.05175  0.05295  3.48324  0.05295]\n",
      "  [ 0.053    0.052    0.052    0.0524   8.60601  0.0524 ]\n",
      "  [ 0.053    0.0515   0.0528   0.0522   1.02804  0.0522 ]]\n",
      "\n",
      " [[ 0.0515   0.0493   0.05075  0.0509   3.4663   0.0509 ]\n",
      "  [ 0.05335  0.0496   0.05175  0.05295  3.48324  0.05295]\n",
      "  [ 0.053    0.052    0.052    0.0524   8.60601  0.0524 ]\n",
      "  [ 0.053    0.0515   0.0528   0.0522   1.02804  0.0522 ]\n",
      "  [ 0.053    0.0511   0.0521   0.0516   1.20903  0.0516 ]]\n",
      "\n",
      " [[ 0.05335  0.0496   0.05175  0.05295  3.48324  0.05295]\n",
      "  [ 0.053    0.052    0.052    0.0524   8.60601  0.0524 ]\n",
      "  [ 0.053    0.0515   0.0528   0.0522   1.02804  0.0522 ]\n",
      "  [ 0.053    0.0511   0.0521   0.0516   1.20903  0.0516 ]\n",
      "  [ 0.0533   0.0516   0.0516   0.05315  1.85586  0.05315]]]\n",
      "y_test: [[0.0469 ]\n",
      " [0.04855]\n",
      " [0.04655]\n",
      " [0.0442 ]\n",
      " [0.04415]\n",
      " [0.04315]\n",
      " [0.04515]\n",
      " [0.04485]\n",
      " [0.04455]\n",
      " [0.04555]\n",
      " [0.0467 ]\n",
      " [0.0487 ]\n",
      " [0.0519 ]\n",
      " [0.05035]\n",
      " [0.0509 ]\n",
      " [0.05295]\n",
      " [0.0524 ]\n",
      " [0.0522 ]\n",
      " [0.0516 ]\n",
      " [0.05315]\n",
      " [0.053  ]\n",
      " [0.05015]\n",
      " [0.04635]\n",
      " [0.0467 ]\n",
      " [0.0466 ]\n",
      " [0.0437 ]\n",
      " [0.04535]\n",
      " [0.0462 ]\n",
      " [0.04735]\n",
      " [0.0459 ]\n",
      " [0.044  ]]\n"
     ]
    }
   ],
   "source": [
    "df_val = df.values / 1000\n",
    "#for a in range(len(df_val[0])-2):\n",
    "#    df_val[:,a] /= 1000\n",
    "window  =  5\n",
    "X_train, y_train, X_test, y_test = load_data_v2(df_val, window)\n",
    "#print(\"X_train\", X_train.shape)\n",
    "#print(\"y_train\", y_train.shape)\n",
    "#print(\"X_test\", X_test.shape)\n",
    "#print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 317 samples, validate on 36 samples\n",
      "Epoch 1/250\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0017 - acc: 0.0000e+00 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 2/250\n",
      "317/317 [==============================] - 0s 585us/step - loss: 0.0016 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 3/250\n",
      "317/317 [==============================] - 0s 536us/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 4/250\n",
      "317/317 [==============================] - 0s 615us/step - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 5/250\n",
      "317/317 [==============================] - 0s 705us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 6/250\n",
      "317/317 [==============================] - 0s 555us/step - loss: 9.6564e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 7/250\n",
      "317/317 [==============================] - 0s 616us/step - loss: 7.9470e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 8/250\n",
      "317/317 [==============================] - 0s 603us/step - loss: 6.3469e-04 - acc: 0.0000e+00 - val_loss: 7.6479e-04 - val_acc: 0.0000e+00\n",
      "Epoch 9/250\n",
      "317/317 [==============================] - 0s 547us/step - loss: 4.8989e-04 - acc: 0.0000e+00 - val_loss: 5.1990e-04 - val_acc: 0.0000e+00\n",
      "Epoch 10/250\n",
      "317/317 [==============================] - 0s 601us/step - loss: 3.5910e-04 - acc: 0.0000e+00 - val_loss: 3.1374e-04 - val_acc: 0.0000e+00\n",
      "Epoch 11/250\n",
      "317/317 [==============================] - 0s 571us/step - loss: 2.7498e-04 - acc: 0.0000e+00 - val_loss: 1.5884e-04 - val_acc: 0.0000e+00\n",
      "Epoch 12/250\n",
      "317/317 [==============================] - 0s 614us/step - loss: 2.2913e-04 - acc: 0.0000e+00 - val_loss: 6.8582e-05 - val_acc: 0.0000e+00\n",
      "Epoch 13/250\n",
      "317/317 [==============================] - 0s 651us/step - loss: 2.3581e-04 - acc: 0.0000e+00 - val_loss: 4.5639e-05 - val_acc: 0.0000e+00\n",
      "Epoch 14/250\n",
      "317/317 [==============================] - 0s 570us/step - loss: 2.8859e-04 - acc: 0.0000e+00 - val_loss: 5.5139e-05 - val_acc: 0.0000e+00\n",
      "Epoch 15/250\n",
      "317/317 [==============================] - 0s 731us/step - loss: 3.4481e-04 - acc: 0.0000e+00 - val_loss: 6.0682e-05 - val_acc: 0.0000e+00\n",
      "Epoch 16/250\n",
      "317/317 [==============================] - 0s 522us/step - loss: 3.4162e-04 - acc: 0.0000e+00 - val_loss: 5.5939e-05 - val_acc: 0.0000e+00\n",
      "Epoch 17/250\n",
      "317/317 [==============================] - 0s 598us/step - loss: 3.4597e-04 - acc: 0.0000e+00 - val_loss: 4.6698e-05 - val_acc: 0.0000e+00\n",
      "Epoch 18/250\n",
      "317/317 [==============================] - 0s 618us/step - loss: 3.2451e-04 - acc: 0.0000e+00 - val_loss: 4.0548e-05 - val_acc: 0.0000e+00\n",
      "Epoch 19/250\n",
      "317/317 [==============================] - 0s 642us/step - loss: 2.8689e-04 - acc: 0.0000e+00 - val_loss: 4.4184e-05 - val_acc: 0.0000e+00\n",
      "Epoch 20/250\n",
      "317/317 [==============================] - 0s 511us/step - loss: 2.4843e-04 - acc: 0.0000e+00 - val_loss: 6.0245e-05 - val_acc: 0.0000e+00\n",
      "Epoch 21/250\n",
      "317/317 [==============================] - 0s 624us/step - loss: 2.3394e-04 - acc: 0.0000e+00 - val_loss: 8.7493e-05 - val_acc: 0.0000e+00\n",
      "Epoch 22/250\n",
      "317/317 [==============================] - 0s 635us/step - loss: 2.2279e-04 - acc: 0.0000e+00 - val_loss: 1.2214e-04 - val_acc: 0.0000e+00\n",
      "Epoch 23/250\n",
      "317/317 [==============================] - 0s 513us/step - loss: 2.1893e-04 - acc: 0.0000e+00 - val_loss: 1.5962e-04 - val_acc: 0.0000e+00\n",
      "Epoch 24/250\n",
      "317/317 [==============================] - 0s 594us/step - loss: 2.1590e-04 - acc: 0.0000e+00 - val_loss: 1.9543e-04 - val_acc: 0.0000e+00\n",
      "Epoch 25/250\n",
      "317/317 [==============================] - 0s 520us/step - loss: 2.2124e-04 - acc: 0.0000e+00 - val_loss: 2.2593e-04 - val_acc: 0.0000e+00\n",
      "Epoch 26/250\n",
      "317/317 [==============================] - 0s 620us/step - loss: 2.2642e-04 - acc: 0.0000e+00 - val_loss: 2.4748e-04 - val_acc: 0.0000e+00\n",
      "Epoch 27/250\n",
      "317/317 [==============================] - 0s 622us/step - loss: 2.3130e-04 - acc: 0.0000e+00 - val_loss: 2.6137e-04 - val_acc: 0.0000e+00\n",
      "Epoch 28/250\n",
      "317/317 [==============================] - 0s 530us/step - loss: 2.3535e-04 - acc: 0.0000e+00 - val_loss: 2.6757e-04 - val_acc: 0.0000e+00\n",
      "Epoch 29/250\n",
      "317/317 [==============================] - 0s 747us/step - loss: 2.3611e-04 - acc: 0.0000e+00 - val_loss: 2.6613e-04 - val_acc: 0.0000e+00\n",
      "Epoch 30/250\n",
      "317/317 [==============================] - 0s 521us/step - loss: 2.3375e-04 - acc: 0.0000e+00 - val_loss: 2.5823e-04 - val_acc: 0.0000e+00\n",
      "Epoch 31/250\n",
      "317/317 [==============================] - 0s 601us/step - loss: 2.2983e-04 - acc: 0.0000e+00 - val_loss: 2.4482e-04 - val_acc: 0.0000e+00\n",
      "Epoch 32/250\n",
      "317/317 [==============================] - 0s 621us/step - loss: 2.2589e-04 - acc: 0.0000e+00 - val_loss: 2.2741e-04 - val_acc: 0.0000e+00\n",
      "Epoch 33/250\n",
      "317/317 [==============================] - 0s 619us/step - loss: 2.2629e-04 - acc: 0.0000e+00 - val_loss: 2.0763e-04 - val_acc: 0.0000e+00\n",
      "Epoch 34/250\n",
      "317/317 [==============================] - 0s 511us/step - loss: 2.1654e-04 - acc: 0.0000e+00 - val_loss: 1.8698e-04 - val_acc: 0.0000e+00\n",
      "Epoch 35/250\n",
      "317/317 [==============================] - 0s 609us/step - loss: 2.1904e-04 - acc: 0.0000e+00 - val_loss: 1.6652e-04 - val_acc: 0.0000e+00\n",
      "Epoch 36/250\n",
      "317/317 [==============================] - 0s 578us/step - loss: 2.1512e-04 - acc: 0.0000e+00 - val_loss: 1.4757e-04 - val_acc: 0.0000e+00\n",
      "Epoch 37/250\n",
      "317/317 [==============================] - 0s 493us/step - loss: 2.0623e-04 - acc: 0.0000e+00 - val_loss: 1.3096e-04 - val_acc: 0.0000e+00\n",
      "Epoch 38/250\n",
      "317/317 [==============================] - 0s 516us/step - loss: 2.0688e-04 - acc: 0.0000e+00 - val_loss: 1.1690e-04 - val_acc: 0.0000e+00\n",
      "Epoch 39/250\n",
      "317/317 [==============================] - 0s 548us/step - loss: 2.0545e-04 - acc: 0.0000e+00 - val_loss: 1.0564e-04 - val_acc: 0.0000e+00\n",
      "Epoch 40/250\n",
      "317/317 [==============================] - 0s 500us/step - loss: 2.1344e-04 - acc: 0.0000e+00 - val_loss: 9.7092e-05 - val_acc: 0.0000e+00\n",
      "Epoch 41/250\n",
      "317/317 [==============================] - 0s 524us/step - loss: 2.1261e-04 - acc: 0.0000e+00 - val_loss: 9.1426e-05 - val_acc: 0.0000e+00\n",
      "Epoch 42/250\n",
      "317/317 [==============================] - 0s 520us/step - loss: 2.1443e-04 - acc: 0.0000e+00 - val_loss: 8.8664e-05 - val_acc: 0.0000e+00\n",
      "Epoch 43/250\n",
      "317/317 [==============================] - 0s 525us/step - loss: 2.1265e-04 - acc: 0.0000e+00 - val_loss: 8.8279e-05 - val_acc: 0.0000e+00\n",
      "Epoch 44/250\n",
      "317/317 [==============================] - 0s 512us/step - loss: 2.0762e-04 - acc: 0.0000e+00 - val_loss: 8.9853e-05 - val_acc: 0.0000e+00\n",
      "Epoch 45/250\n",
      "317/317 [==============================] - 0s 565us/step - loss: 2.1872e-04 - acc: 0.0000e+00 - val_loss: 9.3547e-05 - val_acc: 0.0000e+00\n",
      "Epoch 46/250\n",
      "317/317 [==============================] - 0s 526us/step - loss: 2.1897e-04 - acc: 0.0000e+00 - val_loss: 9.9043e-05 - val_acc: 0.0000e+00\n",
      "Epoch 47/250\n",
      "317/317 [==============================] - 0s 527us/step - loss: 2.0889e-04 - acc: 0.0000e+00 - val_loss: 1.0597e-04 - val_acc: 0.0000e+00\n",
      "Epoch 48/250\n",
      "317/317 [==============================] - 0s 499us/step - loss: 2.0799e-04 - acc: 0.0000e+00 - val_loss: 1.1381e-04 - val_acc: 0.0000e+00\n",
      "Epoch 49/250\n",
      "317/317 [==============================] - 0s 527us/step - loss: 2.0965e-04 - acc: 0.0000e+00 - val_loss: 1.2242e-04 - val_acc: 0.0000e+00\n",
      "Epoch 50/250\n",
      "317/317 [==============================] - 0s 510us/step - loss: 2.0757e-04 - acc: 0.0000e+00 - val_loss: 1.3111e-04 - val_acc: 0.0000e+00\n",
      "Epoch 51/250\n",
      "317/317 [==============================] - 0s 554us/step - loss: 2.0916e-04 - acc: 0.0000e+00 - val_loss: 1.3962e-04 - val_acc: 0.0000e+00\n",
      "Epoch 52/250\n",
      "317/317 [==============================] - 0s 520us/step - loss: 2.1298e-04 - acc: 0.0000e+00 - val_loss: 1.4811e-04 - val_acc: 0.0000e+00\n",
      "Epoch 53/250\n",
      "317/317 [==============================] - 0s 528us/step - loss: 2.0491e-04 - acc: 0.0000e+00 - val_loss: 1.5542e-04 - val_acc: 0.0000e+00\n",
      "Epoch 54/250\n",
      "317/317 [==============================] - 0s 543us/step - loss: 2.0587e-04 - acc: 0.0000e+00 - val_loss: 1.6142e-04 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/250\n",
      "317/317 [==============================] - 0s 497us/step - loss: 2.0371e-04 - acc: 0.0000e+00 - val_loss: 1.6560e-04 - val_acc: 0.0000e+00\n",
      "Epoch 56/250\n",
      "317/317 [==============================] - 0s 522us/step - loss: 2.1054e-04 - acc: 0.0000e+00 - val_loss: 1.6814e-04 - val_acc: 0.0000e+00\n",
      "Epoch 57/250\n",
      "317/317 [==============================] - 0s 544us/step - loss: 2.0526e-04 - acc: 0.0000e+00 - val_loss: 1.6885e-04 - val_acc: 0.0000e+00\n",
      "Epoch 58/250\n",
      "317/317 [==============================] - 0s 489us/step - loss: 2.0866e-04 - acc: 0.0000e+00 - val_loss: 1.6812e-04 - val_acc: 0.0000e+00\n",
      "Epoch 59/250\n",
      "317/317 [==============================] - 0s 501us/step - loss: 2.0864e-04 - acc: 0.0000e+00 - val_loss: 1.6602e-04 - val_acc: 0.0000e+00\n",
      "Epoch 60/250\n",
      "317/317 [==============================] - 0s 477us/step - loss: 2.1015e-04 - acc: 0.0000e+00 - val_loss: 1.6240e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/250\n",
      "317/317 [==============================] - 0s 489us/step - loss: 2.1185e-04 - acc: 0.0000e+00 - val_loss: 1.5806e-04 - val_acc: 0.0000e+00\n",
      "Epoch 62/250\n",
      "317/317 [==============================] - 0s 481us/step - loss: 2.0448e-04 - acc: 0.0000e+00 - val_loss: 1.5331e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/250\n",
      "317/317 [==============================] - 0s 602us/step - loss: 2.1109e-04 - acc: 0.0000e+00 - val_loss: 1.4810e-04 - val_acc: 0.0000e+00\n",
      "Epoch 64/250\n",
      "317/317 [==============================] - 0s 471us/step - loss: 2.1070e-04 - acc: 0.0000e+00 - val_loss: 1.4326e-04 - val_acc: 0.0000e+00\n",
      "Epoch 65/250\n",
      "317/317 [==============================] - 0s 504us/step - loss: 2.0674e-04 - acc: 0.0000e+00 - val_loss: 1.3887e-04 - val_acc: 0.0000e+00\n",
      "Epoch 66/250\n",
      "317/317 [==============================] - 0s 498us/step - loss: 2.0693e-04 - acc: 0.0000e+00 - val_loss: 1.3478e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/250\n",
      "317/317 [==============================] - 0s 487us/step - loss: 2.0325e-04 - acc: 0.0000e+00 - val_loss: 1.3160e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/250\n",
      "317/317 [==============================] - 0s 505us/step - loss: 2.0804e-04 - acc: 0.0000e+00 - val_loss: 1.2923e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/250\n",
      "317/317 [==============================] - 0s 506us/step - loss: 2.0997e-04 - acc: 0.0000e+00 - val_loss: 1.2763e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/250\n",
      "317/317 [==============================] - 0s 501us/step - loss: 2.0414e-04 - acc: 0.0000e+00 - val_loss: 1.2695e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/250\n",
      "317/317 [==============================] - 0s 483us/step - loss: 2.0927e-04 - acc: 0.0000e+00 - val_loss: 1.2727e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/250\n",
      "317/317 [==============================] - 0s 508us/step - loss: 2.0918e-04 - acc: 0.0000e+00 - val_loss: 1.2836e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 2.1017e-04 - acc: 0.0000e+00 - val_loss: 1.3029e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/250\n",
      "317/317 [==============================] - 0s 527us/step - loss: 2.0368e-04 - acc: 0.0000e+00 - val_loss: 1.3277e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/250\n",
      "317/317 [==============================] - 0s 497us/step - loss: 2.0592e-04 - acc: 0.0000e+00 - val_loss: 1.3547e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/250\n",
      "317/317 [==============================] - 0s 504us/step - loss: 2.0492e-04 - acc: 0.0000e+00 - val_loss: 1.3820e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/250\n",
      "317/317 [==============================] - 0s 507us/step - loss: 2.0275e-04 - acc: 0.0000e+00 - val_loss: 1.4092e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/250\n",
      "317/317 [==============================] - 0s 516us/step - loss: 2.0355e-04 - acc: 0.0000e+00 - val_loss: 1.4342e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/250\n",
      "317/317 [==============================] - 0s 497us/step - loss: 2.0647e-04 - acc: 0.0000e+00 - val_loss: 1.4582e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/250\n",
      "317/317 [==============================] - 0s 474us/step - loss: 2.0048e-04 - acc: 0.0000e+00 - val_loss: 1.4747e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/250\n",
      "317/317 [==============================] - 0s 515us/step - loss: 2.0788e-04 - acc: 0.0000e+00 - val_loss: 1.4859e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/250\n",
      "317/317 [==============================] - 0s 486us/step - loss: 2.0616e-04 - acc: 0.0000e+00 - val_loss: 1.4908e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/250\n",
      "317/317 [==============================] - 0s 499us/step - loss: 2.0015e-04 - acc: 0.0000e+00 - val_loss: 1.4895e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/250\n",
      "317/317 [==============================] - 0s 468us/step - loss: 2.0129e-04 - acc: 0.0000e+00 - val_loss: 1.4828e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/250\n",
      "317/317 [==============================] - 0s 533us/step - loss: 2.0356e-04 - acc: 0.0000e+00 - val_loss: 1.4709e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/250\n",
      "317/317 [==============================] - 0s 480us/step - loss: 2.1035e-04 - acc: 0.0000e+00 - val_loss: 1.4570e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 2.1077e-04 - acc: 0.0000e+00 - val_loss: 1.4408e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/250\n",
      "317/317 [==============================] - 0s 502us/step - loss: 2.0219e-04 - acc: 0.0000e+00 - val_loss: 1.4223e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/250\n",
      "317/317 [==============================] - 0s 483us/step - loss: 2.0065e-04 - acc: 0.0000e+00 - val_loss: 1.4028e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 2.0325e-04 - acc: 0.0000e+00 - val_loss: 1.3876e-04 - val_acc: 0.0000e+00\n",
      "Epoch 91/250\n",
      "317/317 [==============================] - 0s 507us/step - loss: 2.0711e-04 - acc: 0.0000e+00 - val_loss: 1.3728e-04 - val_acc: 0.0000e+00\n",
      "Epoch 92/250\n",
      "317/317 [==============================] - 0s 500us/step - loss: 2.0686e-04 - acc: 0.0000e+00 - val_loss: 1.3617e-04 - val_acc: 0.0000e+00\n",
      "Epoch 93/250\n",
      "317/317 [==============================] - 0s 491us/step - loss: 2.0578e-04 - acc: 0.0000e+00 - val_loss: 1.3571e-04 - val_acc: 0.0000e+00\n",
      "Epoch 94/250\n",
      "317/317 [==============================] - 0s 515us/step - loss: 2.0676e-04 - acc: 0.0000e+00 - val_loss: 1.3568e-04 - val_acc: 0.0000e+00\n",
      "Epoch 95/250\n",
      "317/317 [==============================] - 0s 492us/step - loss: 1.9592e-04 - acc: 0.0000e+00 - val_loss: 1.3548e-04 - val_acc: 0.0000e+00\n",
      "Epoch 96/250\n",
      "317/317 [==============================] - 0s 503us/step - loss: 1.9873e-04 - acc: 0.0000e+00 - val_loss: 1.3560e-04 - val_acc: 0.0000e+00\n",
      "Epoch 97/250\n",
      "317/317 [==============================] - 0s 479us/step - loss: 2.0127e-04 - acc: 0.0000e+00 - val_loss: 1.3590e-04 - val_acc: 0.0000e+00\n",
      "Epoch 98/250\n",
      "317/317 [==============================] - 0s 488us/step - loss: 2.0068e-04 - acc: 0.0000e+00 - val_loss: 1.3667e-04 - val_acc: 0.0000e+00\n",
      "Epoch 99/250\n",
      "317/317 [==============================] - 0s 489us/step - loss: 2.0631e-04 - acc: 0.0000e+00 - val_loss: 1.3795e-04 - val_acc: 0.0000e+00\n",
      "Epoch 100/250\n",
      "317/317 [==============================] - 0s 497us/step - loss: 2.0394e-04 - acc: 0.0000e+00 - val_loss: 1.3948e-04 - val_acc: 0.0000e+00\n",
      "Epoch 101/250\n",
      "317/317 [==============================] - 0s 503us/step - loss: 2.0018e-04 - acc: 0.0000e+00 - val_loss: 1.4098e-04 - val_acc: 0.0000e+00\n",
      "Epoch 102/250\n",
      "317/317 [==============================] - 0s 515us/step - loss: 2.0756e-04 - acc: 0.0000e+00 - val_loss: 1.4224e-04 - val_acc: 0.0000e+00\n",
      "Epoch 103/250\n",
      "317/317 [==============================] - 0s 486us/step - loss: 2.0194e-04 - acc: 0.0000e+00 - val_loss: 1.4353e-04 - val_acc: 0.0000e+00\n",
      "Epoch 104/250\n",
      "317/317 [==============================] - 0s 493us/step - loss: 2.0712e-04 - acc: 0.0000e+00 - val_loss: 1.4487e-04 - val_acc: 0.0000e+00\n",
      "Epoch 105/250\n",
      "317/317 [==============================] - 0s 518us/step - loss: 2.0481e-04 - acc: 0.0000e+00 - val_loss: 1.4602e-04 - val_acc: 0.0000e+00\n",
      "Epoch 106/250\n",
      "317/317 [==============================] - 0s 508us/step - loss: 2.0394e-04 - acc: 0.0000e+00 - val_loss: 1.4685e-04 - val_acc: 0.0000e+00\n",
      "Epoch 107/250\n",
      "317/317 [==============================] - 0s 501us/step - loss: 2.0408e-04 - acc: 0.0000e+00 - val_loss: 1.4712e-04 - val_acc: 0.0000e+00\n",
      "Epoch 108/250\n",
      "317/317 [==============================] - 0s 508us/step - loss: 2.0454e-04 - acc: 0.0000e+00 - val_loss: 1.4714e-04 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/250\n",
      "317/317 [==============================] - 0s 481us/step - loss: 2.0462e-04 - acc: 0.0000e+00 - val_loss: 1.4692e-04 - val_acc: 0.0000e+00\n",
      "Epoch 110/250\n",
      "317/317 [==============================] - 0s 499us/step - loss: 2.0094e-04 - acc: 0.0000e+00 - val_loss: 1.4629e-04 - val_acc: 0.0000e+00\n",
      "Epoch 111/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 2.0482e-04 - acc: 0.0000e+00 - val_loss: 1.4550e-04 - val_acc: 0.0000e+00\n",
      "Epoch 112/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 2.0136e-04 - acc: 0.0000e+00 - val_loss: 1.4472e-04 - val_acc: 0.0000e+00\n",
      "Epoch 113/250\n",
      "317/317 [==============================] - 0s 518us/step - loss: 2.0255e-04 - acc: 0.0000e+00 - val_loss: 1.4396e-04 - val_acc: 0.0000e+00\n",
      "Epoch 114/250\n",
      "317/317 [==============================] - 0s 499us/step - loss: 2.0761e-04 - acc: 0.0000e+00 - val_loss: 1.4329e-04 - val_acc: 0.0000e+00\n",
      "Epoch 115/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 2.0047e-04 - acc: 0.0000e+00 - val_loss: 1.4251e-04 - val_acc: 0.0000e+00\n",
      "Epoch 116/250\n",
      "317/317 [==============================] - 0s 510us/step - loss: 2.0334e-04 - acc: 0.0000e+00 - val_loss: 1.4152e-04 - val_acc: 0.0000e+00\n",
      "Epoch 117/250\n",
      "317/317 [==============================] - 0s 489us/step - loss: 2.0368e-04 - acc: 0.0000e+00 - val_loss: 1.4092e-04 - val_acc: 0.0000e+00\n",
      "Epoch 118/250\n",
      "317/317 [==============================] - 0s 483us/step - loss: 1.9955e-04 - acc: 0.0000e+00 - val_loss: 1.4043e-04 - val_acc: 0.0000e+00\n",
      "Epoch 119/250\n",
      "317/317 [==============================] - 0s 483us/step - loss: 2.0284e-04 - acc: 0.0000e+00 - val_loss: 1.4035e-04 - val_acc: 0.0000e+00\n",
      "Epoch 120/250\n",
      "317/317 [==============================] - 0s 511us/step - loss: 2.0102e-04 - acc: 0.0000e+00 - val_loss: 1.4037e-04 - val_acc: 0.0000e+00\n",
      "Epoch 121/250\n",
      "317/317 [==============================] - 0s 491us/step - loss: 2.0511e-04 - acc: 0.0000e+00 - val_loss: 1.4042e-04 - val_acc: 0.0000e+00\n",
      "Epoch 122/250\n",
      "317/317 [==============================] - 0s 500us/step - loss: 2.0115e-04 - acc: 0.0000e+00 - val_loss: 1.4072e-04 - val_acc: 0.0000e+00\n",
      "Epoch 123/250\n",
      "317/317 [==============================] - 0s 506us/step - loss: 2.0519e-04 - acc: 0.0000e+00 - val_loss: 1.4110e-04 - val_acc: 0.0000e+00\n",
      "Epoch 124/250\n",
      "317/317 [==============================] - 0s 485us/step - loss: 2.0196e-04 - acc: 0.0000e+00 - val_loss: 1.4157e-04 - val_acc: 0.0000e+00\n",
      "Epoch 125/250\n",
      "317/317 [==============================] - 0s 492us/step - loss: 1.9848e-04 - acc: 0.0000e+00 - val_loss: 1.4204e-04 - val_acc: 0.0000e+00\n",
      "Epoch 126/250\n",
      "317/317 [==============================] - 0s 497us/step - loss: 2.0327e-04 - acc: 0.0000e+00 - val_loss: 1.4230e-04 - val_acc: 0.0000e+00\n",
      "Epoch 127/250\n",
      "317/317 [==============================] - 0s 500us/step - loss: 1.9898e-04 - acc: 0.0000e+00 - val_loss: 1.4279e-04 - val_acc: 0.0000e+00\n",
      "Epoch 128/250\n",
      "317/317 [==============================] - 0s 470us/step - loss: 1.9929e-04 - acc: 0.0000e+00 - val_loss: 1.4300e-04 - val_acc: 0.0000e+00\n",
      "Epoch 129/250\n",
      "317/317 [==============================] - 0s 507us/step - loss: 2.0187e-04 - acc: 0.0000e+00 - val_loss: 1.4353e-04 - val_acc: 0.0000e+00\n",
      "Epoch 130/250\n",
      "317/317 [==============================] - 0s 472us/step - loss: 2.0109e-04 - acc: 0.0000e+00 - val_loss: 1.4382e-04 - val_acc: 0.0000e+00\n",
      "Epoch 131/250\n",
      "317/317 [==============================] - 0s 477us/step - loss: 2.0046e-04 - acc: 0.0000e+00 - val_loss: 1.4393e-04 - val_acc: 0.0000e+00\n",
      "Epoch 132/250\n",
      "317/317 [==============================] - 0s 528us/step - loss: 1.9891e-04 - acc: 0.0000e+00 - val_loss: 1.4381e-04 - val_acc: 0.0000e+00\n",
      "Epoch 133/250\n",
      "317/317 [==============================] - 0s 491us/step - loss: 2.0279e-04 - acc: 0.0000e+00 - val_loss: 1.4347e-04 - val_acc: 0.0000e+00\n",
      "Epoch 134/250\n",
      "317/317 [==============================] - 0s 485us/step - loss: 2.0210e-04 - acc: 0.0000e+00 - val_loss: 1.4302e-04 - val_acc: 0.0000e+00\n",
      "Epoch 135/250\n",
      "317/317 [==============================] - 0s 487us/step - loss: 1.9686e-04 - acc: 0.0000e+00 - val_loss: 1.4245e-04 - val_acc: 0.0000e+00\n",
      "Epoch 136/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 1.9838e-04 - acc: 0.0000e+00 - val_loss: 1.4163e-04 - val_acc: 0.0000e+00\n",
      "Epoch 137/250\n",
      "317/317 [==============================] - 0s 582us/step - loss: 2.0095e-04 - acc: 0.0000e+00 - val_loss: 1.4076e-04 - val_acc: 0.0000e+00\n",
      "Epoch 138/250\n",
      "317/317 [==============================] - 0s 587us/step - loss: 1.9969e-04 - acc: 0.0000e+00 - val_loss: 1.4017e-04 - val_acc: 0.0000e+00\n",
      "Epoch 139/250\n",
      "317/317 [==============================] - 0s 504us/step - loss: 2.0363e-04 - acc: 0.0000e+00 - val_loss: 1.3971e-04 - val_acc: 0.0000e+00\n",
      "Epoch 140/250\n",
      "317/317 [==============================] - 0s 491us/step - loss: 2.0497e-04 - acc: 0.0000e+00 - val_loss: 1.3942e-04 - val_acc: 0.0000e+00\n",
      "Epoch 141/250\n",
      "317/317 [==============================] - 0s 500us/step - loss: 1.9912e-04 - acc: 0.0000e+00 - val_loss: 1.3934e-04 - val_acc: 0.0000e+00\n",
      "Epoch 142/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 1.9530e-04 - acc: 0.0000e+00 - val_loss: 1.3958e-04 - val_acc: 0.0000e+00\n",
      "Epoch 143/250\n",
      "317/317 [==============================] - 0s 504us/step - loss: 2.0054e-04 - acc: 0.0000e+00 - val_loss: 1.4027e-04 - val_acc: 0.0000e+00\n",
      "Epoch 144/250\n",
      "317/317 [==============================] - 0s 504us/step - loss: 1.9992e-04 - acc: 0.0000e+00 - val_loss: 1.4065e-04 - val_acc: 0.0000e+00\n",
      "Epoch 145/250\n",
      "317/317 [==============================] - 0s 510us/step - loss: 2.0032e-04 - acc: 0.0000e+00 - val_loss: 1.4096e-04 - val_acc: 0.0000e+00\n",
      "Epoch 146/250\n",
      "317/317 [==============================] - 0s 488us/step - loss: 1.9744e-04 - acc: 0.0000e+00 - val_loss: 1.4140e-04 - val_acc: 0.0000e+00\n",
      "Epoch 147/250\n",
      "317/317 [==============================] - 0s 502us/step - loss: 1.9712e-04 - acc: 0.0000e+00 - val_loss: 1.4178e-04 - val_acc: 0.0000e+00\n",
      "Epoch 148/250\n",
      "317/317 [==============================] - 0s 509us/step - loss: 2.0279e-04 - acc: 0.0000e+00 - val_loss: 1.4236e-04 - val_acc: 0.0000e+00\n",
      "Epoch 149/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 1.9750e-04 - acc: 0.0000e+00 - val_loss: 1.4271e-04 - val_acc: 0.0000e+00\n",
      "Epoch 150/250\n",
      "317/317 [==============================] - 0s 513us/step - loss: 1.9724e-04 - acc: 0.0000e+00 - val_loss: 1.4271e-04 - val_acc: 0.0000e+00\n",
      "Epoch 151/250\n",
      "317/317 [==============================] - 0s 495us/step - loss: 2.0274e-04 - acc: 0.0000e+00 - val_loss: 1.4275e-04 - val_acc: 0.0000e+00\n",
      "Epoch 152/250\n",
      "317/317 [==============================] - 0s 501us/step - loss: 1.9668e-04 - acc: 0.0000e+00 - val_loss: 1.4267e-04 - val_acc: 0.0000e+00\n",
      "Epoch 153/250\n",
      "317/317 [==============================] - 0s 488us/step - loss: 1.9634e-04 - acc: 0.0000e+00 - val_loss: 1.4242e-04 - val_acc: 0.0000e+00\n",
      "Epoch 154/250\n",
      "317/317 [==============================] - 0s 492us/step - loss: 1.9934e-04 - acc: 0.0000e+00 - val_loss: 1.4216e-04 - val_acc: 0.0000e+00\n",
      "Epoch 155/250\n",
      "317/317 [==============================] - 0s 492us/step - loss: 1.9621e-04 - acc: 0.0000e+00 - val_loss: 1.4176e-04 - val_acc: 0.0000e+00\n",
      "Epoch 156/250\n",
      "317/317 [==============================] - 0s 520us/step - loss: 1.9787e-04 - acc: 0.0000e+00 - val_loss: 1.4143e-04 - val_acc: 0.0000e+00\n",
      "Epoch 157/250\n",
      "317/317 [==============================] - 0s 508us/step - loss: 2.0238e-04 - acc: 0.0000e+00 - val_loss: 1.4114e-04 - val_acc: 0.0000e+00\n",
      "Epoch 158/250\n",
      "317/317 [==============================] - 0s 488us/step - loss: 1.9814e-04 - acc: 0.0000e+00 - val_loss: 1.4104e-04 - val_acc: 0.0000e+00\n",
      "Epoch 159/250\n",
      "317/317 [==============================] - 0s 509us/step - loss: 1.9810e-04 - acc: 0.0000e+00 - val_loss: 1.4090e-04 - val_acc: 0.0000e+00\n",
      "Epoch 160/250\n",
      "317/317 [==============================] - 0s 465us/step - loss: 1.9700e-04 - acc: 0.0000e+00 - val_loss: 1.4096e-04 - val_acc: 0.0000e+00\n",
      "Epoch 161/250\n",
      "317/317 [==============================] - 0s 509us/step - loss: 1.9475e-04 - acc: 0.0000e+00 - val_loss: 1.4080e-04 - val_acc: 0.0000e+00\n",
      "Epoch 162/250\n",
      "317/317 [==============================] - 0s 500us/step - loss: 1.9729e-04 - acc: 0.0000e+00 - val_loss: 1.4073e-04 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/250\n",
      "317/317 [==============================] - 0s 513us/step - loss: 1.9584e-04 - acc: 0.0000e+00 - val_loss: 1.4058e-04 - val_acc: 0.0000e+00\n",
      "Epoch 164/250\n",
      "317/317 [==============================] - 0s 505us/step - loss: 1.9498e-04 - acc: 0.0000e+00 - val_loss: 1.4060e-04 - val_acc: 0.0000e+00\n",
      "Epoch 165/250\n",
      "317/317 [==============================] - 0s 477us/step - loss: 1.9736e-04 - acc: 0.0000e+00 - val_loss: 1.4038e-04 - val_acc: 0.0000e+00\n",
      "Epoch 166/250\n",
      "317/317 [==============================] - 0s 504us/step - loss: 1.9960e-04 - acc: 0.0000e+00 - val_loss: 1.4022e-04 - val_acc: 0.0000e+00\n",
      "Epoch 167/250\n",
      "317/317 [==============================] - 0s 511us/step - loss: 1.9244e-04 - acc: 0.0000e+00 - val_loss: 1.3999e-04 - val_acc: 0.0000e+00\n",
      "Epoch 168/250\n",
      "317/317 [==============================] - 0s 507us/step - loss: 1.9141e-04 - acc: 0.0000e+00 - val_loss: 1.3951e-04 - val_acc: 0.0000e+00\n",
      "Epoch 169/250\n",
      "317/317 [==============================] - 0s 511us/step - loss: 2.0073e-04 - acc: 0.0000e+00 - val_loss: 1.3942e-04 - val_acc: 0.0000e+00\n",
      "Epoch 170/250\n",
      "317/317 [==============================] - 0s 523us/step - loss: 1.9538e-04 - acc: 0.0000e+00 - val_loss: 1.3967e-04 - val_acc: 0.0000e+00\n",
      "Epoch 171/250\n",
      "317/317 [==============================] - 0s 469us/step - loss: 1.9300e-04 - acc: 0.0000e+00 - val_loss: 1.3988e-04 - val_acc: 0.0000e+00\n",
      "Epoch 172/250\n",
      "317/317 [==============================] - 0s 496us/step - loss: 1.9334e-04 - acc: 0.0000e+00 - val_loss: 1.3999e-04 - val_acc: 0.0000e+00\n",
      "Epoch 173/250\n",
      "317/317 [==============================] - 0s 495us/step - loss: 1.9886e-04 - acc: 0.0000e+00 - val_loss: 1.4019e-04 - val_acc: 0.0000e+00\n",
      "Epoch 174/250\n",
      "317/317 [==============================] - 0s 474us/step - loss: 1.9466e-04 - acc: 0.0000e+00 - val_loss: 1.4035e-04 - val_acc: 0.0000e+00\n",
      "Epoch 175/250\n",
      "317/317 [==============================] - 0s 575us/step - loss: 1.9725e-04 - acc: 0.0000e+00 - val_loss: 1.4094e-04 - val_acc: 0.0000e+00\n",
      "Epoch 176/250\n",
      "317/317 [==============================] - 0s 472us/step - loss: 1.9428e-04 - acc: 0.0000e+00 - val_loss: 1.4141e-04 - val_acc: 0.0000e+00\n",
      "Epoch 177/250\n",
      "317/317 [==============================] - 0s 502us/step - loss: 1.9747e-04 - acc: 0.0000e+00 - val_loss: 1.4194e-04 - val_acc: 0.0000e+00\n",
      "Epoch 178/250\n",
      "317/317 [==============================] - 0s 495us/step - loss: 1.9568e-04 - acc: 0.0000e+00 - val_loss: 1.4200e-04 - val_acc: 0.0000e+00\n",
      "Epoch 179/250\n",
      "317/317 [==============================] - 0s 514us/step - loss: 1.9601e-04 - acc: 0.0000e+00 - val_loss: 1.4207e-04 - val_acc: 0.0000e+00\n",
      "Epoch 180/250\n",
      "317/317 [==============================] - 0s 477us/step - loss: 1.9674e-04 - acc: 0.0000e+00 - val_loss: 1.4187e-04 - val_acc: 0.0000e+00\n",
      "Epoch 181/250\n",
      "317/317 [==============================] - 0s 499us/step - loss: 1.9178e-04 - acc: 0.0000e+00 - val_loss: 1.4141e-04 - val_acc: 0.0000e+00\n",
      "Epoch 182/250\n",
      "317/317 [==============================] - 0s 525us/step - loss: 1.9401e-04 - acc: 0.0000e+00 - val_loss: 1.4090e-04 - val_acc: 0.0000e+00\n",
      "Epoch 183/250\n",
      "317/317 [==============================] - 0s 495us/step - loss: 1.9681e-04 - acc: 0.0000e+00 - val_loss: 1.4034e-04 - val_acc: 0.0000e+00\n",
      "Epoch 184/250\n",
      "317/317 [==============================] - 0s 506us/step - loss: 1.9406e-04 - acc: 0.0000e+00 - val_loss: 1.3960e-04 - val_acc: 0.0000e+00\n",
      "Epoch 185/250\n",
      "317/317 [==============================] - 0s 477us/step - loss: 1.8935e-04 - acc: 0.0000e+00 - val_loss: 1.3882e-04 - val_acc: 0.0000e+00\n",
      "Epoch 186/250\n",
      "317/317 [==============================] - 0s 546us/step - loss: 1.9468e-04 - acc: 0.0000e+00 - val_loss: 1.3831e-04 - val_acc: 0.0000e+00\n",
      "Epoch 187/250\n",
      "317/317 [==============================] - 0s 517us/step - loss: 1.9290e-04 - acc: 0.0000e+00 - val_loss: 1.3811e-04 - val_acc: 0.0000e+00\n",
      "Epoch 188/250\n",
      "317/317 [==============================] - 0s 522us/step - loss: 1.9354e-04 - acc: 0.0000e+00 - val_loss: 1.3829e-04 - val_acc: 0.0000e+00\n",
      "Epoch 189/250\n",
      "317/317 [==============================] - 0s 505us/step - loss: 1.9532e-04 - acc: 0.0000e+00 - val_loss: 1.3866e-04 - val_acc: 0.0000e+00\n",
      "Epoch 190/250\n",
      "317/317 [==============================] - 0s 506us/step - loss: 1.9204e-04 - acc: 0.0000e+00 - val_loss: 1.3880e-04 - val_acc: 0.0000e+00\n",
      "Epoch 191/250\n",
      "317/317 [==============================] - 0s 486us/step - loss: 1.9078e-04 - acc: 0.0000e+00 - val_loss: 1.3862e-04 - val_acc: 0.0000e+00\n",
      "Epoch 192/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 1.9534e-04 - acc: 0.0000e+00 - val_loss: 1.3866e-04 - val_acc: 0.0000e+00\n",
      "Epoch 193/250\n",
      "317/317 [==============================] - 0s 495us/step - loss: 1.9637e-04 - acc: 0.0000e+00 - val_loss: 1.3857e-04 - val_acc: 0.0000e+00\n",
      "Epoch 194/250\n",
      "317/317 [==============================] - 0s 540us/step - loss: 1.9194e-04 - acc: 0.0000e+00 - val_loss: 1.3856e-04 - val_acc: 0.0000e+00\n",
      "Epoch 195/250\n",
      "317/317 [==============================] - 0s 514us/step - loss: 1.8896e-04 - acc: 0.0000e+00 - val_loss: 1.3847e-04 - val_acc: 0.0000e+00\n",
      "Epoch 196/250\n",
      "317/317 [==============================] - 0s 502us/step - loss: 1.9391e-04 - acc: 0.0000e+00 - val_loss: 1.3836e-04 - val_acc: 0.0000e+00\n",
      "Epoch 197/250\n",
      "317/317 [==============================] - 0s 520us/step - loss: 1.9110e-04 - acc: 0.0000e+00 - val_loss: 1.3783e-04 - val_acc: 0.0000e+00\n",
      "Epoch 198/250\n",
      "317/317 [==============================] - 0s 487us/step - loss: 1.9348e-04 - acc: 0.0000e+00 - val_loss: 1.3711e-04 - val_acc: 0.0000e+00\n",
      "Epoch 199/250\n",
      "317/317 [==============================] - 0s 500us/step - loss: 1.9286e-04 - acc: 0.0000e+00 - val_loss: 1.3649e-04 - val_acc: 0.0000e+00\n",
      "Epoch 200/250\n",
      "317/317 [==============================] - 0s 520us/step - loss: 1.8800e-04 - acc: 0.0000e+00 - val_loss: 1.3587e-04 - val_acc: 0.0000e+00\n",
      "Epoch 201/250\n",
      "317/317 [==============================] - 0s 538us/step - loss: 1.8937e-04 - acc: 0.0000e+00 - val_loss: 1.3534e-04 - val_acc: 0.0000e+00\n",
      "Epoch 202/250\n",
      "317/317 [==============================] - 0s 535us/step - loss: 1.9288e-04 - acc: 0.0000e+00 - val_loss: 1.3520e-04 - val_acc: 0.0000e+00\n",
      "Epoch 203/250\n",
      "317/317 [==============================] - 0s 500us/step - loss: 1.9280e-04 - acc: 0.0000e+00 - val_loss: 1.3517e-04 - val_acc: 0.0000e+00\n",
      "Epoch 204/250\n",
      "317/317 [==============================] - 0s 506us/step - loss: 1.8992e-04 - acc: 0.0000e+00 - val_loss: 1.3498e-04 - val_acc: 0.0000e+00\n",
      "Epoch 205/250\n",
      "317/317 [==============================] - 0s 491us/step - loss: 1.9011e-04 - acc: 0.0000e+00 - val_loss: 1.3510e-04 - val_acc: 0.0000e+00\n",
      "Epoch 206/250\n",
      "317/317 [==============================] - 0s 532us/step - loss: 1.8541e-04 - acc: 0.0000e+00 - val_loss: 1.3562e-04 - val_acc: 0.0000e+00\n",
      "Epoch 207/250\n",
      "317/317 [==============================] - 0s 508us/step - loss: 1.8895e-04 - acc: 0.0000e+00 - val_loss: 1.3557e-04 - val_acc: 0.0000e+00\n",
      "Epoch 208/250\n",
      "317/317 [==============================] - 0s 501us/step - loss: 1.8745e-04 - acc: 0.0000e+00 - val_loss: 1.3560e-04 - val_acc: 0.0000e+00\n",
      "Epoch 209/250\n",
      "317/317 [==============================] - 0s 500us/step - loss: 1.8857e-04 - acc: 0.0000e+00 - val_loss: 1.3574e-04 - val_acc: 0.0000e+00\n",
      "Epoch 210/250\n",
      "317/317 [==============================] - 0s 495us/step - loss: 1.9199e-04 - acc: 0.0000e+00 - val_loss: 1.3636e-04 - val_acc: 0.0000e+00\n",
      "Epoch 211/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 1.8718e-04 - acc: 0.0000e+00 - val_loss: 1.3682e-04 - val_acc: 0.0000e+00\n",
      "Epoch 212/250\n",
      "317/317 [==============================] - 0s 579us/step - loss: 1.8481e-04 - acc: 0.0000e+00 - val_loss: 1.3703e-04 - val_acc: 0.0000e+00\n",
      "Epoch 213/250\n",
      "317/317 [==============================] - 0s 488us/step - loss: 1.8417e-04 - acc: 0.0000e+00 - val_loss: 1.3619e-04 - val_acc: 0.0000e+00\n",
      "Epoch 214/250\n",
      "317/317 [==============================] - 0s 521us/step - loss: 1.8570e-04 - acc: 0.0000e+00 - val_loss: 1.3522e-04 - val_acc: 0.0000e+00\n",
      "Epoch 215/250\n",
      "317/317 [==============================] - 0s 506us/step - loss: 1.8546e-04 - acc: 0.0000e+00 - val_loss: 1.3293e-04 - val_acc: 0.0000e+00\n",
      "Epoch 216/250\n",
      "317/317 [==============================] - 0s 477us/step - loss: 1.8333e-04 - acc: 0.0000e+00 - val_loss: 1.3043e-04 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/250\n",
      "317/317 [==============================] - 0s 520us/step - loss: 1.8166e-04 - acc: 0.0000e+00 - val_loss: 1.2765e-04 - val_acc: 0.0000e+00\n",
      "Epoch 218/250\n",
      "317/317 [==============================] - 0s 528us/step - loss: 1.8242e-04 - acc: 0.0000e+00 - val_loss: 1.2709e-04 - val_acc: 0.0000e+00\n",
      "Epoch 219/250\n",
      "317/317 [==============================] - 0s 514us/step - loss: 1.8373e-04 - acc: 0.0000e+00 - val_loss: 1.2895e-04 - val_acc: 0.0000e+00\n",
      "Epoch 220/250\n",
      "317/317 [==============================] - 0s 540us/step - loss: 1.7895e-04 - acc: 0.0000e+00 - val_loss: 1.3108e-04 - val_acc: 0.0000e+00\n",
      "Epoch 221/250\n",
      "317/317 [==============================] - 0s 495us/step - loss: 1.7967e-04 - acc: 0.0000e+00 - val_loss: 1.3377e-04 - val_acc: 0.0000e+00\n",
      "Epoch 222/250\n",
      "317/317 [==============================] - 0s 502us/step - loss: 1.7919e-04 - acc: 0.0000e+00 - val_loss: 1.3528e-04 - val_acc: 0.0000e+00\n",
      "Epoch 223/250\n",
      "317/317 [==============================] - 0s 471us/step - loss: 1.8085e-04 - acc: 0.0000e+00 - val_loss: 1.3510e-04 - val_acc: 0.0000e+00\n",
      "Epoch 224/250\n",
      "317/317 [==============================] - 0s 525us/step - loss: 1.8208e-04 - acc: 0.0000e+00 - val_loss: 1.3242e-04 - val_acc: 0.0000e+00\n",
      "Epoch 225/250\n",
      "317/317 [==============================] - 0s 488us/step - loss: 1.7557e-04 - acc: 0.0000e+00 - val_loss: 1.2941e-04 - val_acc: 0.0000e+00\n",
      "Epoch 226/250\n",
      "317/317 [==============================] - 0s 475us/step - loss: 1.7289e-04 - acc: 0.0000e+00 - val_loss: 1.2555e-04 - val_acc: 0.0000e+00\n",
      "Epoch 227/250\n",
      "317/317 [==============================] - 0s 484us/step - loss: 1.7693e-04 - acc: 0.0000e+00 - val_loss: 1.2097e-04 - val_acc: 0.0000e+00\n",
      "Epoch 228/250\n",
      "317/317 [==============================] - 0s 478us/step - loss: 1.7599e-04 - acc: 0.0000e+00 - val_loss: 1.1737e-04 - val_acc: 0.0000e+00\n",
      "Epoch 229/250\n",
      "317/317 [==============================] - 0s 490us/step - loss: 1.7193e-04 - acc: 0.0000e+00 - val_loss: 1.1929e-04 - val_acc: 0.0000e+00\n",
      "Epoch 230/250\n",
      "317/317 [==============================] - 0s 486us/step - loss: 1.7101e-04 - acc: 0.0000e+00 - val_loss: 1.2467e-04 - val_acc: 0.0000e+00\n",
      "Epoch 231/250\n",
      "317/317 [==============================] - 0s 512us/step - loss: 1.6601e-04 - acc: 0.0000e+00 - val_loss: 1.3102e-04 - val_acc: 0.0000e+00\n",
      "Epoch 232/250\n",
      "317/317 [==============================] - 0s 502us/step - loss: 1.6535e-04 - acc: 0.0000e+00 - val_loss: 1.3483e-04 - val_acc: 0.0000e+00\n",
      "Epoch 233/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 1.6348e-04 - acc: 0.0000e+00 - val_loss: 1.2929e-04 - val_acc: 0.0000e+00\n",
      "Epoch 234/250\n",
      "317/317 [==============================] - 0s 496us/step - loss: 1.6327e-04 - acc: 0.0000e+00 - val_loss: 1.2098e-04 - val_acc: 0.0000e+00\n",
      "Epoch 235/250\n",
      "317/317 [==============================] - 0s 487us/step - loss: 1.5929e-04 - acc: 0.0000e+00 - val_loss: 1.1398e-04 - val_acc: 0.0000e+00\n",
      "Epoch 236/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 1.5733e-04 - acc: 0.0000e+00 - val_loss: 1.1801e-04 - val_acc: 0.0000e+00\n",
      "Epoch 237/250\n",
      "317/317 [==============================] - 0s 541us/step - loss: 1.5488e-04 - acc: 0.0000e+00 - val_loss: 1.3151e-04 - val_acc: 0.0000e+00\n",
      "Epoch 238/250\n",
      "317/317 [==============================] - 0s 491us/step - loss: 1.4768e-04 - acc: 0.0000e+00 - val_loss: 1.2093e-04 - val_acc: 0.0000e+00\n",
      "Epoch 239/250\n",
      "317/317 [==============================] - 0s 494us/step - loss: 1.4870e-04 - acc: 0.0000e+00 - val_loss: 9.8741e-05 - val_acc: 0.0000e+00\n",
      "Epoch 240/250\n",
      "317/317 [==============================] - 0s 502us/step - loss: 1.4254e-04 - acc: 0.0000e+00 - val_loss: 1.0398e-04 - val_acc: 0.0000e+00\n",
      "Epoch 241/250\n",
      "317/317 [==============================] - 0s 475us/step - loss: 1.3720e-04 - acc: 0.0000e+00 - val_loss: 1.2494e-04 - val_acc: 0.0000e+00\n",
      "Epoch 242/250\n",
      "317/317 [==============================] - 0s 516us/step - loss: 1.3399e-04 - acc: 0.0000e+00 - val_loss: 9.2285e-05 - val_acc: 0.0000e+00\n",
      "Epoch 243/250\n",
      "317/317 [==============================] - 0s 510us/step - loss: 1.2155e-04 - acc: 0.0000e+00 - val_loss: 9.8625e-05 - val_acc: 0.0000e+00\n",
      "Epoch 244/250\n",
      "317/317 [==============================] - 0s 500us/step - loss: 1.1428e-04 - acc: 0.0000e+00 - val_loss: 9.7280e-05 - val_acc: 0.0000e+00\n",
      "Epoch 245/250\n",
      "317/317 [==============================] - 0s 521us/step - loss: 1.1516e-04 - acc: 0.0000e+00 - val_loss: 6.0400e-05 - val_acc: 0.0000e+00\n",
      "Epoch 246/250\n",
      "317/317 [==============================] - 0s 481us/step - loss: 1.0432e-04 - acc: 0.0000e+00 - val_loss: 1.7109e-04 - val_acc: 0.0000e+00\n",
      "Epoch 247/250\n",
      "317/317 [==============================] - 0s 493us/step - loss: 1.1373e-04 - acc: 0.0000e+00 - val_loss: 5.0196e-05 - val_acc: 0.0000e+00\n",
      "Epoch 248/250\n",
      "317/317 [==============================] - 0s 482us/step - loss: 1.0805e-04 - acc: 0.0000e+00 - val_loss: 7.9504e-05 - val_acc: 0.0000e+00\n",
      "Epoch 249/250\n",
      "317/317 [==============================] - 0s 573us/step - loss: 8.2991e-05 - acc: 0.0000e+00 - val_loss: 1.2526e-04 - val_acc: 0.0000e+00\n",
      "Epoch 250/250\n",
      "317/317 [==============================] - 0s 512us/step - loss: 9.7064e-05 - acc: 0.0000e+00 - val_loss: 6.9255e-05 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26202c1748>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = build_model([3,lag,1])\n",
    "#model = build_model2([3,window,1])\n",
    "model = build_model2([len(df_val[0])-1,5,1])\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=576,\n",
    "    epochs=250,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 0s 411us/step\n",
      "Train Score: 0.00 MSE (0.01 RMSE)\n",
      "[[[ 0.05495  0.05     0.053    0.0509   4.80171  0.0509 ]\n",
      "  [ 0.0535   0.0495   0.052    0.0521   2.92069  0.0521 ]\n",
      "  [ 0.0495   0.0455   0.0495   0.0457   9.65772  0.0457 ]\n",
      "  [ 0.0479   0.04545  0.0455   0.0471  13.49893  0.0471 ]\n",
      "  [ 0.05375  0.0501   0.051    0.05345 12.65525  0.05345]]\n",
      "\n",
      " [[ 0.0535   0.0495   0.052    0.0521   2.92069  0.0521 ]\n",
      "  [ 0.0495   0.0455   0.0495   0.0457   9.65772  0.0457 ]\n",
      "  [ 0.0479   0.04545  0.0455   0.0471  13.49893  0.0471 ]\n",
      "  [ 0.05375  0.0501   0.051    0.05345 12.65525  0.05345]\n",
      "  [ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]]\n",
      "\n",
      " [[ 0.0495   0.0455   0.0495   0.0457   9.65772  0.0457 ]\n",
      "  [ 0.0479   0.04545  0.0455   0.0471  13.49893  0.0471 ]\n",
      "  [ 0.05375  0.0501   0.051    0.05345 12.65525  0.05345]\n",
      "  [ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]\n",
      "  [ 0.052    0.049    0.051    0.05185  1.99467  0.05185]]\n",
      "\n",
      " [[ 0.0479   0.04545  0.0455   0.0471  13.49893  0.0471 ]\n",
      "  [ 0.05375  0.0501   0.051    0.05345 12.65525  0.05345]\n",
      "  [ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]\n",
      "  [ 0.052    0.049    0.051    0.05185  1.99467  0.05185]\n",
      "  [ 0.052    0.04975  0.052    0.05015  2.36211  0.05015]]\n",
      "\n",
      " [[ 0.05375  0.0501   0.051    0.05345 12.65525  0.05345]\n",
      "  [ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]\n",
      "  [ 0.052    0.049    0.051    0.05185  1.99467  0.05185]\n",
      "  [ 0.052    0.04975  0.052    0.05015  2.36211  0.05015]\n",
      "  [ 0.05     0.043    0.05     0.0445   7.65132  0.0445 ]]\n",
      "\n",
      " [[ 0.054    0.05175  0.054    0.0522   1.97621  0.0522 ]\n",
      "  [ 0.052    0.049    0.051    0.05185  1.99467  0.05185]\n",
      "  [ 0.052    0.04975  0.052    0.05015  2.36211  0.05015]\n",
      "  [ 0.05     0.043    0.05     0.0445   7.65132  0.0445 ]\n",
      "  [ 0.046    0.04125  0.046    0.045   10.98163  0.045  ]]\n",
      "\n",
      " [[ 0.052    0.049    0.051    0.05185  1.99467  0.05185]\n",
      "  [ 0.052    0.04975  0.052    0.05015  2.36211  0.05015]\n",
      "  [ 0.05     0.043    0.05     0.0445   7.65132  0.0445 ]\n",
      "  [ 0.046    0.04125  0.046    0.045   10.98163  0.045  ]\n",
      "  [ 0.045    0.04     0.045    0.0409   8.67518  0.0409 ]]\n",
      "\n",
      " [[ 0.052    0.04975  0.052    0.05015  2.36211  0.05015]\n",
      "  [ 0.05     0.043    0.05     0.0445   7.65132  0.0445 ]\n",
      "  [ 0.046    0.04125  0.046    0.045   10.98163  0.045  ]\n",
      "  [ 0.045    0.04     0.045    0.0409   8.67518  0.0409 ]\n",
      "  [ 0.0409   0.0367   0.0409   0.03845 13.00883  0.03845]]\n",
      "\n",
      " [[ 0.05     0.043    0.05     0.0445   7.65132  0.0445 ]\n",
      "  [ 0.046    0.04125  0.046    0.045   10.98163  0.045  ]\n",
      "  [ 0.045    0.04     0.045    0.0409   8.67518  0.0409 ]\n",
      "  [ 0.0409   0.0367   0.0409   0.03845 13.00883  0.03845]\n",
      "  [ 0.044    0.0395   0.0395   0.04335 11.21623  0.04335]]\n",
      "\n",
      " [[ 0.046    0.04125  0.046    0.045   10.98163  0.045  ]\n",
      "  [ 0.045    0.04     0.045    0.0409   8.67518  0.0409 ]\n",
      "  [ 0.0409   0.0367   0.0409   0.03845 13.00883  0.03845]\n",
      "  [ 0.044    0.0395   0.0395   0.04335 11.21623  0.04335]\n",
      "  [ 0.0463   0.04275  0.044    0.04585  3.61627  0.04585]]\n",
      "\n",
      " [[ 0.045    0.04     0.045    0.0409   8.67518  0.0409 ]\n",
      "  [ 0.0409   0.0367   0.0409   0.03845 13.00883  0.03845]\n",
      "  [ 0.044    0.0395   0.0395   0.04335 11.21623  0.04335]\n",
      "  [ 0.0463   0.04275  0.044    0.04585  3.61627  0.04585]\n",
      "  [ 0.048    0.0444   0.0459   0.0469   3.15284  0.0469 ]]\n",
      "\n",
      " [[ 0.0409   0.0367   0.0409   0.03845 13.00883  0.03845]\n",
      "  [ 0.044    0.0395   0.0395   0.04335 11.21623  0.04335]\n",
      "  [ 0.0463   0.04275  0.044    0.04585  3.61627  0.04585]\n",
      "  [ 0.048    0.0444   0.0459   0.0469   3.15284  0.0469 ]\n",
      "  [ 0.0474   0.04405  0.0465   0.0469   4.35053  0.0469 ]]\n",
      "\n",
      " [[ 0.044    0.0395   0.0395   0.04335 11.21623  0.04335]\n",
      "  [ 0.0463   0.04275  0.044    0.04585  3.61627  0.04585]\n",
      "  [ 0.048    0.0444   0.0459   0.0469   3.15284  0.0469 ]\n",
      "  [ 0.0474   0.04405  0.0465   0.0469   4.35053  0.0469 ]\n",
      "  [ 0.04905  0.0475   0.048    0.04855  4.0111   0.04855]]\n",
      "\n",
      " [[ 0.0463   0.04275  0.044    0.04585  3.61627  0.04585]\n",
      "  [ 0.048    0.0444   0.0459   0.0469   3.15284  0.0469 ]\n",
      "  [ 0.0474   0.04405  0.0465   0.0469   4.35053  0.0469 ]\n",
      "  [ 0.04905  0.0475   0.048    0.04855  4.0111   0.04855]\n",
      "  [ 0.0486   0.0453   0.0486   0.04655  2.14289  0.04655]]\n",
      "\n",
      " [[ 0.048    0.0444   0.0459   0.0469   3.15284  0.0469 ]\n",
      "  [ 0.0474   0.04405  0.0465   0.0469   4.35053  0.0469 ]\n",
      "  [ 0.04905  0.0475   0.048    0.04855  4.0111   0.04855]\n",
      "  [ 0.0486   0.0453   0.0486   0.04655  2.14289  0.04655]\n",
      "  [ 0.048    0.0435   0.0465   0.0442   2.66491  0.0442 ]]\n",
      "\n",
      " [[ 0.0474   0.04405  0.0465   0.0469   4.35053  0.0469 ]\n",
      "  [ 0.04905  0.0475   0.048    0.04855  4.0111   0.04855]\n",
      "  [ 0.0486   0.0453   0.0486   0.04655  2.14289  0.04655]\n",
      "  [ 0.048    0.0435   0.0465   0.0442   2.66491  0.0442 ]\n",
      "  [ 0.0447   0.04225  0.0442   0.04415  3.42594  0.04415]]\n",
      "\n",
      " [[ 0.04905  0.0475   0.048    0.04855  4.0111   0.04855]\n",
      "  [ 0.0486   0.0453   0.0486   0.04655  2.14289  0.04655]\n",
      "  [ 0.048    0.0435   0.0465   0.0442   2.66491  0.0442 ]\n",
      "  [ 0.0447   0.04225  0.0442   0.04415  3.42594  0.04415]\n",
      "  [ 0.04415  0.04225  0.04415  0.04315  2.15404  0.04315]]\n",
      "\n",
      " [[ 0.0486   0.0453   0.0486   0.04655  2.14289  0.04655]\n",
      "  [ 0.048    0.0435   0.0465   0.0442   2.66491  0.0442 ]\n",
      "  [ 0.0447   0.04225  0.0442   0.04415  3.42594  0.04415]\n",
      "  [ 0.04415  0.04225  0.04415  0.04315  2.15404  0.04315]\n",
      "  [ 0.04555  0.04225  0.04385  0.04515  2.53208  0.04515]]\n",
      "\n",
      " [[ 0.048    0.0435   0.0465   0.0442   2.66491  0.0442 ]\n",
      "  [ 0.0447   0.04225  0.0442   0.04415  3.42594  0.04415]\n",
      "  [ 0.04415  0.04225  0.04415  0.04315  2.15404  0.04315]\n",
      "  [ 0.04555  0.04225  0.04385  0.04515  2.53208  0.04515]\n",
      "  [ 0.0455   0.04415  0.0451   0.04485  1.48535  0.04485]]\n",
      "\n",
      " [[ 0.0447   0.04225  0.0442   0.04415  3.42594  0.04415]\n",
      "  [ 0.04415  0.04225  0.04415  0.04315  2.15404  0.04315]\n",
      "  [ 0.04555  0.04225  0.04385  0.04515  2.53208  0.04515]\n",
      "  [ 0.0455   0.04415  0.0451   0.04485  1.48535  0.04485]\n",
      "  [ 0.0448   0.04375  0.0447   0.04455  0.6867   0.04455]]\n",
      "\n",
      " [[ 0.04415  0.04225  0.04415  0.04315  2.15404  0.04315]\n",
      "  [ 0.04555  0.04225  0.04385  0.04515  2.53208  0.04515]\n",
      "  [ 0.0455   0.04415  0.0451   0.04485  1.48535  0.04485]\n",
      "  [ 0.0448   0.04375  0.0447   0.04455  0.6867   0.04455]\n",
      "  [ 0.0458   0.04455  0.04455  0.04555  2.34566  0.04555]]\n",
      "\n",
      " [[ 0.04555  0.04225  0.04385  0.04515  2.53208  0.04515]\n",
      "  [ 0.0455   0.04415  0.0451   0.04485  1.48535  0.04485]\n",
      "  [ 0.0448   0.04375  0.0447   0.04455  0.6867   0.04455]\n",
      "  [ 0.0458   0.04455  0.04455  0.04555  2.34566  0.04555]\n",
      "  [ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]]\n",
      "\n",
      " [[ 0.0455   0.04415  0.0451   0.04485  1.48535  0.04485]\n",
      "  [ 0.0448   0.04375  0.0447   0.04455  0.6867   0.04455]\n",
      "  [ 0.0458   0.04455  0.04455  0.04555  2.34566  0.04555]\n",
      "  [ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]\n",
      "  [ 0.049    0.0477   0.04795  0.0487   8.7053   0.0487 ]]\n",
      "\n",
      " [[ 0.0448   0.04375  0.0447   0.04455  0.6867   0.04455]\n",
      "  [ 0.0458   0.04455  0.04455  0.04555  2.34566  0.04555]\n",
      "  [ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]\n",
      "  [ 0.049    0.0477   0.04795  0.0487   8.7053   0.0487 ]\n",
      "  [ 0.052    0.0485   0.0485   0.0519   2.06345  0.0519 ]]\n",
      "\n",
      " [[ 0.0458   0.04455  0.04455  0.04555  2.34566  0.04555]\n",
      "  [ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]\n",
      "  [ 0.049    0.0477   0.04795  0.0487   8.7053   0.0487 ]\n",
      "  [ 0.052    0.0485   0.0485   0.0519   2.06345  0.0519 ]\n",
      "  [ 0.0525   0.04985  0.052    0.05035  1.43449  0.05035]]\n",
      "\n",
      " [[ 0.0475   0.045    0.045    0.0467   4.94062  0.0467 ]\n",
      "  [ 0.049    0.0477   0.04795  0.0487   8.7053   0.0487 ]\n",
      "  [ 0.052    0.0485   0.0485   0.0519   2.06345  0.0519 ]\n",
      "  [ 0.0525   0.04985  0.052    0.05035  1.43449  0.05035]\n",
      "  [ 0.0515   0.0493   0.05075  0.0509   3.4663   0.0509 ]]\n",
      "\n",
      " [[ 0.049    0.0477   0.04795  0.0487   8.7053   0.0487 ]\n",
      "  [ 0.052    0.0485   0.0485   0.0519   2.06345  0.0519 ]\n",
      "  [ 0.0525   0.04985  0.052    0.05035  1.43449  0.05035]\n",
      "  [ 0.0515   0.0493   0.05075  0.0509   3.4663   0.0509 ]\n",
      "  [ 0.05335  0.0496   0.05175  0.05295  3.48324  0.05295]]\n",
      "\n",
      " [[ 0.052    0.0485   0.0485   0.0519   2.06345  0.0519 ]\n",
      "  [ 0.0525   0.04985  0.052    0.05035  1.43449  0.05035]\n",
      "  [ 0.0515   0.0493   0.05075  0.0509   3.4663   0.0509 ]\n",
      "  [ 0.05335  0.0496   0.05175  0.05295  3.48324  0.05295]\n",
      "  [ 0.053    0.052    0.052    0.0524   8.60601  0.0524 ]]\n",
      "\n",
      " [[ 0.0525   0.04985  0.052    0.05035  1.43449  0.05035]\n",
      "  [ 0.0515   0.0493   0.05075  0.0509   3.4663   0.0509 ]\n",
      "  [ 0.05335  0.0496   0.05175  0.05295  3.48324  0.05295]\n",
      "  [ 0.053    0.052    0.052    0.0524   8.60601  0.0524 ]\n",
      "  [ 0.053    0.0515   0.0528   0.0522   1.02804  0.0522 ]]\n",
      "\n",
      " [[ 0.0515   0.0493   0.05075  0.0509   3.4663   0.0509 ]\n",
      "  [ 0.05335  0.0496   0.05175  0.05295  3.48324  0.05295]\n",
      "  [ 0.053    0.052    0.052    0.0524   8.60601  0.0524 ]\n",
      "  [ 0.053    0.0515   0.0528   0.0522   1.02804  0.0522 ]\n",
      "  [ 0.053    0.0511   0.0521   0.0516   1.20903  0.0516 ]]\n",
      "\n",
      " [[ 0.05335  0.0496   0.05175  0.05295  3.48324  0.05295]\n",
      "  [ 0.053    0.052    0.052    0.0524   8.60601  0.0524 ]\n",
      "  [ 0.053    0.0515   0.0528   0.0522   1.02804  0.0522 ]\n",
      "  [ 0.053    0.0511   0.0521   0.0516   1.20903  0.0516 ]\n",
      "  [ 0.0533   0.0516   0.0516   0.05315  1.85586  0.05315]]]\n",
      "31/31 [==============================] - 0s 540us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.00 MSE (0.01 RMSE)\n"
     ]
    }
   ],
   "source": [
    "trainScore = model.evaluate(X_train, y_train, verbose=1)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "print (X_test)\n",
    "\n",
    "testScore = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05335 0.0496  0.05175 0.05295 3.48324 0.05295]\n",
      " [0.053   0.052   0.052   0.0524  8.60601 0.0524 ]\n",
      " [0.053   0.0515  0.0528  0.0522  1.02804 0.0522 ]\n",
      " [0.053   0.0511  0.0521  0.0516  1.20903 0.0516 ]\n",
      " [0.0533  0.0516  0.0516  0.05315 1.85586 0.05315]]\n",
      "pred [[0.06337436]\n",
      " [0.06466032]\n",
      " [0.06200078]\n",
      " [0.06100525]\n",
      " [0.05750837]\n",
      " [0.05727755]\n",
      " [0.05895183]\n",
      " [0.0607985 ]\n",
      " [0.0587818 ]\n",
      " [0.05848598]\n",
      " [0.05434909]\n",
      " [0.05449411]\n",
      " [0.04840267]\n",
      " [0.04570159]\n",
      " [0.04588942]\n",
      " [0.04546753]\n",
      " [0.04626008]\n",
      " [0.04709461]\n",
      " [0.04559841]\n",
      " [0.04515074]\n",
      " [0.04703283]\n",
      " [0.04886582]\n",
      " [0.05333194]\n",
      " [0.05328475]\n",
      " [0.04935743]\n",
      " [0.04790749]\n",
      " [0.04973287]\n",
      " [0.05401506]\n",
      " [0.05354649]\n",
      " [0.05069534]\n",
      " [0.05080152]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[-1])\n",
    "diff=[]\n",
    "ratio=[]\n",
    "p = model.predict(X_test)\n",
    "print (\"pred\",p)\n",
    "for u in range(len(y_test)):\n",
    "    pr = p[u][0]\n",
    "    ratio.append((y_test[u]/pr)-1)\n",
    "    diff.append(abs(y_test[u]- pr))\n",
    "    #print(u, y_test[u], pr, (y_test[u]/pr)-1, abs(y_test[u]- pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "plt2.plot((p * 1000 ) ,color='red', label='prediction')\n",
    "plt2.plot(y_test * 1000,color='blue', label='y_test')\n",
    "plt2.legend(loc='upper left')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_last(stock_name ):\n",
    "    from pandas_datareader import data\n",
    "\n",
    "    # Only get the adjusted close.\n",
    "    df = data.DataReader(stock_name,\n",
    "                       start='2017-01-01',\n",
    "                       end='2018-09-22',\n",
    "                       data_source='yahoo')\n",
    "\n",
    "    df['Volume'] /= 100\n",
    "\n",
    "    df = df[-50:-1]\n",
    "    print (df.tail(5))\n",
    "    \n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_prediction(stock, seq_len):\n",
    "\n",
    "    data_x = stock\n",
    "    data_y = stock\n",
    "    #print(\"DATA Y 0 :\",data_y)\n",
    "    print (\"Amount of features TOT :\",len(data_x[0]) )\n",
    "    #data_x = np.delete(data_x,np.s_[len(stock[0])-1],axis=1)\n",
    "    data_y = np.delete(data_y,np.s_[0:len(stock[0])-1],axis=1)\n",
    "    #print(\"DATA Y :\",data_y)\n",
    "    amount_of_features = len(data_x[0]) \n",
    "    \n",
    "    print (\"Amount of features found:\",amount_of_features)\n",
    "    \n",
    "    sequence_length = seq_len \n",
    "    result_x = []\n",
    "    result_y = []\n",
    "    for index in range(len(data_x) - sequence_length ):\n",
    "        result_x.append(data_x[index: index + sequence_length + 1])\n",
    "        result_y.append(data_y[index: index + sequence_length + 1])\n",
    "\n",
    "    result_x = np.array(result_x)\n",
    "    result_y = np.array(result_y)\n",
    "    \n",
    "    #x_train = train_x[:, :-1]\n",
    "    x_test = result_x[:, :-1]\n",
    "    #x_test = result_x[:, :-1]\n",
    "   \n",
    "    #y_train = train_y[:, -1]\n",
    "    y_test = result_y[:, -1]\n",
    "    \n",
    "    #print (\"x_test before:\",x_test)\n",
    "    #print (\"y_test before:\",y_test)\n",
    "    \n",
    "    #x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "    print (\"x_test:\",x_test)\n",
    "    print (\"y_test:\",y_test)\n",
    "\n",
    "    return [x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 High        Low       Open      Close    Volume  Adj Close\n",
      "Date                                                                       \n",
      "2018-09-14  48.250000  46.250000  47.700001  47.400002   1592.08  47.400002\n",
      "2018-09-17  49.500000  47.049999  47.400002  49.000000   4224.01  49.000000\n",
      "2018-09-18  51.900002  48.750000  49.000000  50.900002   5624.78  50.900002\n",
      "2018-09-19  52.349998  50.500000  51.750000  50.900002   9421.33  50.900002\n",
      "2018-09-20  54.000000  51.750000  52.500000  52.900002  10983.66  52.900002\n"
     ]
    }
   ],
   "source": [
    "df_new2 = load_data_last ('TRAN.BA' )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features TOT : 6\n",
      "Amount of features found: 6\n",
      "x_test: [[[0.0447  0.04225 0.0442  0.04415 3.42594 0.04415]\n",
      "  [0.04415 0.04225 0.04415 0.04315 2.15404 0.04315]\n",
      "  [0.04555 0.04225 0.04385 0.04515 2.53208 0.04515]\n",
      "  [0.0455  0.04415 0.0451  0.04485 1.48535 0.04485]\n",
      "  [0.0448  0.04375 0.0447  0.04455 0.6867  0.04455]]\n",
      "\n",
      " [[0.04415 0.04225 0.04415 0.04315 2.15404 0.04315]\n",
      "  [0.04555 0.04225 0.04385 0.04515 2.53208 0.04515]\n",
      "  [0.0455  0.04415 0.0451  0.04485 1.48535 0.04485]\n",
      "  [0.0448  0.04375 0.0447  0.04455 0.6867  0.04455]\n",
      "  [0.0458  0.04455 0.04455 0.04555 2.34566 0.04555]]\n",
      "\n",
      " [[0.04555 0.04225 0.04385 0.04515 2.53208 0.04515]\n",
      "  [0.0455  0.04415 0.0451  0.04485 1.48535 0.04485]\n",
      "  [0.0448  0.04375 0.0447  0.04455 0.6867  0.04455]\n",
      "  [0.0458  0.04455 0.04455 0.04555 2.34566 0.04555]\n",
      "  [0.0475  0.045   0.045   0.0467  4.94062 0.0467 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.0485  0.046   0.047   0.0478  3.15377 0.0478 ]\n",
      "  [0.0497  0.0472  0.04795 0.049   4.76569 0.049  ]\n",
      "  [0.05095 0.0472  0.05    0.0478  6.41655 0.0478 ]\n",
      "  [0.04825 0.04625 0.0477  0.0474  1.59208 0.0474 ]\n",
      "  [0.0495  0.04705 0.0474  0.049   4.22401 0.049  ]]\n",
      "\n",
      " [[0.0497  0.0472  0.04795 0.049   4.76569 0.049  ]\n",
      "  [0.05095 0.0472  0.05    0.0478  6.41655 0.0478 ]\n",
      "  [0.04825 0.04625 0.0477  0.0474  1.59208 0.0474 ]\n",
      "  [0.0495  0.04705 0.0474  0.049   4.22401 0.049  ]\n",
      "  [0.0519  0.04875 0.049   0.0509  5.62478 0.0509 ]]\n",
      "\n",
      " [[0.05095 0.0472  0.05    0.0478  6.41655 0.0478 ]\n",
      "  [0.04825 0.04625 0.0477  0.0474  1.59208 0.0474 ]\n",
      "  [0.0495  0.04705 0.0474  0.049   4.22401 0.049  ]\n",
      "  [0.0519  0.04875 0.049   0.0509  5.62478 0.0509 ]\n",
      "  [0.05235 0.0505  0.05175 0.0509  9.42133 0.0509 ]]]\n",
      "y_test: [[0.04555]\n",
      " [0.0467 ]\n",
      " [0.0487 ]\n",
      " [0.0519 ]\n",
      " [0.05035]\n",
      " [0.0509 ]\n",
      " [0.05295]\n",
      " [0.0524 ]\n",
      " [0.0522 ]\n",
      " [0.0516 ]\n",
      " [0.05315]\n",
      " [0.053  ]\n",
      " [0.05015]\n",
      " [0.04635]\n",
      " [0.0467 ]\n",
      " [0.0466 ]\n",
      " [0.0437 ]\n",
      " [0.04535]\n",
      " [0.0462 ]\n",
      " [0.04735]\n",
      " [0.0459 ]\n",
      " [0.044  ]\n",
      " [0.04565]\n",
      " [0.0459 ]\n",
      " [0.04205]\n",
      " [0.0434 ]\n",
      " [0.042  ]\n",
      " [0.04055]\n",
      " [0.04025]\n",
      " [0.0442 ]\n",
      " [0.04235]\n",
      " [0.04055]\n",
      " [0.0437 ]\n",
      " [0.04555]\n",
      " [0.04795]\n",
      " [0.04755]\n",
      " [0.0478 ]\n",
      " [0.049  ]\n",
      " [0.0478 ]\n",
      " [0.0474 ]\n",
      " [0.049  ]\n",
      " [0.0509 ]\n",
      " [0.0509 ]\n",
      " [0.0529 ]]\n",
      "X_test (44, 5, 6)\n",
      "y_test (44, 1)\n"
     ]
    }
   ],
   "source": [
    "X_test_future, y_test_future = load_data_prediction(df_new2 / 1000, 5)\n",
    "print(\"X_test\", X_test_future.shape)\n",
    "print(\"y_test\", y_test_future.shape)\n",
    "\n",
    "#print(df_new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05095 0.0472  0.05    0.0478  6.41655 0.0478 ]\n",
      " [0.04825 0.04625 0.0477  0.0474  1.59208 0.0474 ]\n",
      " [0.0495  0.04705 0.0474  0.049   4.22401 0.049  ]\n",
      " [0.0519  0.04875 0.049   0.0509  5.62478 0.0509 ]\n",
      " [0.05235 0.0505  0.05175 0.0509  9.42133 0.0509 ]]\n",
      "[[0.0485  0.046   0.047   0.0478  3.15377 0.0478 ]\n",
      " [0.0497  0.0472  0.04795 0.049   4.76569 0.049  ]\n",
      " [0.05095 0.0472  0.05    0.0478  6.41655 0.0478 ]\n",
      " [0.04825 0.04625 0.0477  0.0474  1.59208 0.0474 ]\n",
      " [0.0495  0.04705 0.0474  0.049   4.22401 0.049  ]]\n",
      "[[ 0.0485   0.045    0.04555  0.04795 10.81823  0.04795]\n",
      " [ 0.0491   0.04665  0.048    0.04755  4.02354  0.04755]\n",
      " [ 0.0485   0.046    0.047    0.0478   3.15377  0.0478 ]\n",
      " [ 0.0497   0.0472   0.04795  0.049    4.76569  0.049  ]\n",
      " [ 0.05095  0.0472   0.05     0.0478   6.41655  0.0478 ]]\n",
      "[[45.15074 ]\n",
      " [47.032825]\n",
      " [48.865818]\n",
      " [53.33194 ]\n",
      " [53.284748]\n",
      " [49.357426]\n",
      " [47.907494]\n",
      " [49.732872]\n",
      " [54.015064]\n",
      " [53.54649 ]\n",
      " [50.695343]\n",
      " [50.80152 ]\n",
      " [51.23991 ]\n",
      " [51.905983]\n",
      " [51.92078 ]\n",
      " [51.10427 ]\n",
      " [49.696163]\n",
      " [48.08538 ]\n",
      " [45.695774]\n",
      " [44.61284 ]\n",
      " [44.224243]\n",
      " [44.986298]\n",
      " [48.102253]\n",
      " [49.101814]\n",
      " [49.151466]\n",
      " [49.57451 ]\n",
      " [45.53303 ]\n",
      " [46.309025]\n",
      " [42.72654 ]\n",
      " [44.531776]\n",
      " [57.338795]\n",
      " [61.547585]\n",
      " [66.48027 ]\n",
      " [66.77935 ]\n",
      " [68.349266]\n",
      " [47.34943 ]\n",
      " [48.520233]\n",
      " [50.632088]\n",
      " [49.968323]\n",
      " [50.469772]\n",
      " [47.182327]\n",
      " [47.69624 ]\n",
      " [47.500214]\n",
      " [51.387123]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_future[-1])\n",
    "print(X_test_future[-3])\n",
    "print(X_test_future[-5])\n",
    "diff=[]\n",
    "ratio=[]\n",
    "pred = model.predict(X_test_future) \n",
    "print(pred * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "plt2.plot((pred * 1000 ) ,color='red', label='prediction')\n",
    "plt2.plot(y_test_future * 1000,color='blue', label='real data Adj Close')\n",
    "plt2.legend(loc='upper left')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
